{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"End-To-End Example with Tensorflow Keras - MNIST Classification\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-To-End Example with Tensorflow Keras and Hops - MNIST Classification\n",
    "---\n",
    "*DATA PREPARATION --> MODEL TRAINING --> MODEL SERVING --> MODEL MONITORING*\n",
    "\n",
    "<font color='red'> <h3>Tested with TensorFlow 2.9.1</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../images/hops.png)\n",
    "\n",
    "## The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>.\n",
    "\n",
    "### Using the `experiment` module\n",
    "\n",
    "To be able to run your Machine Learning code in Hopsworks, the code for the whole program needs to be provided and put inside a wrapper function. Everything, from importing libraries to reading data and defining the model and running the program needs to be put inside a wrapper function.\n",
    "\n",
    "The `experiment` module provides an api to Python programs such as TensorFlow, Keras and PyTorch on a Hopsworks on any number of machines and GPUs.\n",
    "\n",
    "An Experiment could be a single Python program, which we refer to as an **Experiment**. \n",
    "\n",
    "Grid search or genetic hyperparameter optimization such as differential evolution which runs several Experiments in parallel, which we refer to as **Parallel Experiment**. \n",
    "\n",
    "ParameterServerStrategy, CollectiveAllReduceStrategy and MultiworkerMirroredStrategy making multi-machine/multi-gpu training as simple as invoking a function for orchestration. This mode is referred to as **Distributed Training**.\n",
    "\n",
    "### Using the `tensorboard` module\n",
    "The `tensorboard` module allow us to get the log directory for summaries and checkpoints to be written to the TensorBoard we will see in a bit. The only function that we currently need to call is `tensorboard.logdir()`, which returns the path to the TensorBoard log directory. Furthermore, the content of this directory will be put in as a Dataset in your project's Experiments folder.\n",
    "\n",
    "The directory could in practice be used to store other data that should be accessible after the experiment is finished.\n",
    "```python\n",
    "# Use this module to get the TensorBoard logdir\n",
    "from hops import tensorboard\n",
    "tensorboard_logdir = tensorboard.logdir()\n",
    "```\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a mnist folder in your Resources Dataset, the path to the mnist data would be `hdfs.project_path() + 'Resources/mnist'`\n",
    "\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "```python\n",
    "# Downloading the mnist dataset to the current working directory\n",
    "from hops import hdfs\n",
    "mnist_hdfs_path = hdfs.project_path() + \"Resources/mnist\"\n",
    "local_mnist_path = hdfs.copy_to_local(mnist_hdfs_path)\n",
    "```\n",
    "\n",
    "### Documentation\n",
    "See the following links to learn more about running experiments in Hopsworks\n",
    "\n",
    "- <a href=\"https://hopsworks.readthedocs.io/en/latest/hopsml/experiment.html\">Learn more about experiments</a>\n",
    "<br>\n",
    "- <a href=\"https://hopsworks.readthedocs.io/en/latest/hopsml/hopsML.html\">Building End-To-End pipelines</a>\n",
    "<br>\n",
    "- Give us a star, create an issue or a feature request on  <a href=\"https://github.com/logicalclocks/hopsworks\">Hopsworks github</a>\n",
    "\n",
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>\n",
    "![Image7-Monitor.png](../../images/experiments.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Mnist Classifier running a Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mniste2e\"\n",
    "EXPERIMENT_NAME = \"Mnist classifier for e2e example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 140ms/step - loss: 1.0067 - accuracy: 0.5625\n",
      "Saving trained model to: /srv/hops/staging/private_dirs/19b0fb823ca769217d995f45563455c4363cf74c556b80afe5762fe813fd9713/model-2959f4bb-a422-4525-b76d-3c975969678d\n",
      "2022-07-09 17:52:11,313 INFO: Assets written to: /srv/hops/staging/private_dirs/19b0fb823ca769217d995f45563455c4363cf74c556b80afe5762fe813fd9713/model-2959f4bb-a422-4525-b76d-3c975969678d/assets\n",
      "Done saving!\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9bf45fc8f247aaac444eb79f712390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://hopsworks0.logicalclocks.com/p/119/models/mniste2e/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'mniste2e', version: 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import math\n",
    "from hops import tensorboard\n",
    "\n",
    "import hsml\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "from hops import hdfs\n",
    "\n",
    "import pydoop.hdfs as pydoop\n",
    "\n",
    "batch_size=32\n",
    "num_classes = 10\n",
    "\n",
    "# Provide path to train and validation datasets\n",
    "train_filenames = [hdfs.project_path() + \"TourData/mnist/train/train.tfrecords\"]\n",
    "validation_filenames = [hdfs.project_path() + \"TourData/mnist/validation/validation.tfrecords\"]\n",
    "\n",
    "# Define input function\n",
    "def data_input(filenames, batch_size=128, num_classes = 10, shuffle=False, repeat=None):\n",
    "\n",
    "    def parser(serialized_example):\n",
    "        \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "        features = tf.io.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "                'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "            })\n",
    "        image = tf.io.decode_raw(features['image_raw'], tf.uint8)\n",
    "        image.set_shape([28 * 28])\n",
    "\n",
    "        # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "        image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "        label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "        # Create a one hot array for your labels\n",
    "        label = tf.one_hot(label, num_classes)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Import MNIST data\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "    # Map the parser over dataset, and batch results by up to batch_size\n",
    "    dataset = dataset.map(parser)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=128)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "    return dataset\n",
    "\n",
    "# Define a Keras Model.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer= tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.fit(data_input(train_filenames, batch_size), \n",
    "    verbose=0,\n",
    "    epochs=3, \n",
    "    steps_per_epoch=5,\n",
    "    validation_data=data_input(validation_filenames, batch_size),\n",
    "    validation_steps=1\n",
    ")\n",
    "\n",
    "score = model.evaluate(data_input(validation_filenames, batch_size), steps=1)\n",
    "\n",
    "# Export model\n",
    "# WARNING(break-tutorial-inline-code): The following code snippet is\n",
    "# in-lined in tutorials, please update tutorial documents accordingly\n",
    "# whenever code changes.\n",
    "\n",
    "export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "print('Saving trained model to: {}'.format(export_path))\n",
    "\n",
    "tf.saved_model.save(model, export_path)\n",
    "\n",
    "print('Done saving!')\n",
    "\n",
    "metrics = {'accuracy': score[1]}\n",
    "\n",
    "# export trained model\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "input_example = np.random.randint(0, high=256, size=784, dtype=np.uint8)\n",
    "\n",
    "input_schema = Schema(input_example)\n",
    "output_schema = Schema([{'type': 'float32', 'shape': [10], 'description': 'Predictions per image category'}])\n",
    "\n",
    "tf_model = mr.tensorflow.create_model(MODEL_NAME,\n",
    "                                      metrics=metrics,\n",
    "                                      input_example=input_example,\n",
    "                                      model_schema=ModelSchema(input_schema=input_schema, output_schema=output_schema))\n",
    "\n",
    "tf_model.save(export_path)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a connection to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "# Connect with Hopsworks\n",
    "conn = hsml.connection()\n",
    "\n",
    "# Retrieve the model registry handle\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best Mnist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mniste2e\n",
      "Model version: 1\n",
      "{'accuracy': '0.5625'}\n"
     ]
    }
   ],
   "source": [
    "# Get best version of the mnist model\n",
    "best_model = mr.get_best_model(MODEL_NAME, \"accuracy\", \"max\")\n",
    "\n",
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Deployment for the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting number of replicas to default value '1'\n",
      "Setting number of partitions to default value '1'\n",
      "Setting up a Tensorflow Serving server to run the model\n",
      "Deployment created, explore it at https://hopsworks0.logicalclocks.com/p/119/deployments/4\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.inference_logger import InferenceLogger\n",
    "from hsml.kafka_topic import KafkaTopic\n",
    "\n",
    "kafka_topic = KafkaTopic()\n",
    "my_logger = InferenceLogger(kafka_topic=kafka_topic, mode=\"ALL\")\n",
    "\n",
    "# Deploy the best version of the model\n",
    "mnistclassifier = best_model.deploy(inference_logger=my_logger) # To deploy the model using KServe, set serving_tool to KSERVE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the serving have been created, you can find it in the Hopsworks UI by going to the \"Deployments\" tab. You can also use the class attributes or the `.describe()` method of a deployment object to describe access its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of mniste2e\n",
      "{\n",
      "    \"artifact_version\": null,\n",
      "    \"batching_configuration\": {\n",
      "        \"batching_enabled\": false\n",
      "    },\n",
      "    \"created\": \"2022-07-09T17:52:44Z\",\n",
      "    \"creator\": \"Admin Admin\",\n",
      "    \"id\": 4,\n",
      "    \"inference_logging\": \"ALL\",\n",
      "    \"kafka_topic_dto\": {\n",
      "        \"name\": \"mniste2e-inf6486\",\n",
      "        \"num_of_partitions\": null,\n",
      "        \"num_of_replicas\": null\n",
      "    },\n",
      "    \"model_name\": \"mniste2e\",\n",
      "    \"model_path\": \"/Projects/demo_ml_meb10000/Models/mniste2e\",\n",
      "    \"model_server\": \"TENSORFLOW_SERVING\",\n",
      "    \"model_version\": 1,\n",
      "    \"name\": \"mniste2e\",\n",
      "    \"predictor\": null,\n",
      "    \"predictor_resources\": {\n",
      "        \"limits\": {\n",
      "            \"cores\": 1,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        },\n",
      "        \"requests\": {\n",
      "            \"cores\": 1,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        }\n",
      "    },\n",
      "    \"requested_instances\": 1,\n",
      "    \"serving_tool\": \"DEFAULT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Description of \" + mnistclassifier.name)\n",
    "mnistclassifier.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Mnist samples using the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6e4ae4b2794a59a3e1cb2b85082952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "if mnistclassifier.get_state().status == \"Stopped\":\n",
    "    mnistclassifier.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while mnistclassifier.get_state().status != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send prediction requests to the Mnist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n",
      "{'predictions': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    data = { \"signature_name\": \"serving_default\", \"instances\": [best_model.input_example] }\n",
    "    predictions = mnistclassifier.predict(data)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Logs\n",
    "\n",
    "### Consume Prediction Requests and Responses using Kafka\n",
    "\n",
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = mnistclassifier.inference_logger.kafka_topic.name\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving: mniste2e, version: 1, timestamp: 1657389186893,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389187493,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389188033,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389188631,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389189191,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389189728,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389190236,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389190822,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389191396,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1657389191993,\n",
      "         http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 10):\n",
    "    msg = consumer.poll(timeout=5.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"         http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                print(\"predictions: {}\\n\".format(json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]))\n",
    "                      \n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}