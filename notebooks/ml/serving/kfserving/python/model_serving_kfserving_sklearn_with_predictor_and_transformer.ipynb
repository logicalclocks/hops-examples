{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with KFServing, Scikit-learn, Predictors and Transformers - Iris Flower Classification\"\n",
    "date: 2021-01-10\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with KFServing, Scikit-learn, Predictors and Transformers - Iris Flower Classification\n",
    "---\n",
    "*INPUT --> TRANSFORMER --> ENRICHED INPUT --> PREDICTOR (MODEL) --> PREDICTION*\n",
    "\n",
    "<font color='red'> <h3>This notebook requires KFServing to be installed</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *irisflowerclassifier* is already available in Hopsworks. An example of training a model for the *Iris flower classification problem* is available in `Jupyter/end_to_end_pipelines/sklearn/end_to_end_sklearn.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "## The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>.\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a mnist folder in your Resources Dataset, the path to the mnist data would be `hdfs.project_path() + 'Resources/mnist'`\n",
    "\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "```python\n",
    "from hops import hdfs\n",
    "# Uploading the traied model to hdfs\n",
    "hdfs.copy_to_hdfs(\"iris_flower_knn.pkl\", \"Resources/iris_flower\", overwrite=True)\n",
    "\n",
    "# Downloading the iris flower model to the current working directory\n",
    "iris_flower_model_hdfs_path = hdfs.project_path() + \"Resources/iris_flower/iris_flower_knn.pkl\"\n",
    "local_iris_flower_model_path = hdfs.copy_to_local(iris_flower_model_hdfs_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the Iris Flower classifier\n",
    "\n",
    "#### Predictor script\n",
    "\n",
    "To serve a Scikit-Learn Model, write a predictor script in Python that downloads the HDFS model in the constructor and saves it as a class variable and then implements the `Predict` class and the methods `predict`, `classify` and `regress`, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.externals import joblib\n",
    "from hops import hdfs\n",
    "import os\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/irisflowerclassifier/1/iris_flower_knn.pkl\"\n",
    "        print(\"Copying SKLearn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local SkLearn model for serving\")\n",
    "        self.model = joblib.load(\"./iris_flower_knn.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are note JSON serializable\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        \"\"\" Serves a classification request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "\n",
    "    def regress(self, inputs):\n",
    "        \"\"\" Serves a regression request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "```\n",
    "\n",
    "Then, if you are using Jupyter, keep track of the path where you created the predictor script. Otherweise, upload the script to some folder in your project using the UI or the _hops_ Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer script\n",
    "\n",
    "To serve a model with a transformer, write a python script that implements the `Transformer` class and the methods `preprocess` and `postprocess`, like this:\n",
    "\n",
    "```python\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        print(\"[Transformer] Initializing...\")\n",
    "        # Initialization code goes here\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        # Transform the request inputs here. The object returned by this method will be used as model input.\n",
    "        return inputs\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        # Transform the predictions computed by the model before returning a response.\n",
    "        return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best IrisFlowerClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: irisflowerclassifier\n",
      "Model version: 1\n",
      "{'accuracy': '0.98'}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"irisflowerclassifier\"\n",
    "EVALUATION_METRIC=\"accuracy\"\n",
    "\n",
    "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"max\")\n",
    "\n",
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Serving of Exported Model\n",
    "\n",
    "Once all the files are in the file system, we can create a serving instance that points to the model files using `serving.create_or_update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 16:11:41,169 INFO: Serving irisflowerclassifier successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving\n",
    "SERVING_NAME = MODEL_NAME\n",
    "\n",
    "PREDICTOR_SCRIPT_PATH = hdfs.project_path() + \"/Jupyter/serving/kfserving/python/predictor.py\" # or .ipynb\n",
    "TRANSFORMER_SCRIPT_PATH = hdfs.project_path() + \"/Jupyter/serving/kfserving/python/transformer.py\" # or .ipynb\n",
    "\n",
    "serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                         model_path=best_model.model_path, # set the path of the model to be deployed\n",
    "                         model_server=\"PYTHON\", # set the model server to run the model\n",
    "                         kfserving=True, # enable KFServing\n",
    "                         predictor=PREDICTOR_SCRIPT_PATH, # set the predictor to load the model and make predictions\n",
    "                         transformer=TRANSFORMER_SCRIPT_PATH, # set the path to the transformer script\n",
    "                         transformer_instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                         # optional arguments\n",
    "                         model_version=best_model.version, # set the version of the model to be deployed\n",
    "                         topic_name=\"CREATE\", # topic name or CREATE to create a new topic for inference logging, otherwise NONE\n",
    "                         instances=1, # number of replicas\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the serving instance is created, it will be shown in the \"Model Serving\" tab in the Hopsworks UI. You can view detailed information like server-logs and which Kafka Topic it is logging inference requests to.\n",
    "\n",
    "![kfserving_python_pred_trans_details](./../../../images/kfserving_python_pred_trans_details.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the Python module to query the Hopsworks REST API about information on the existing servings using methods like: \n",
    "\n",
    "- `get_all()`\n",
    "- `get_id(serving_name)`\n",
    "- `get_model_path(serving_name)`\n",
    "- `get_model_version(serving_name)`\n",
    "- `get_artifact_version(serving_name)`\n",
    "- `get_kafka_topic(serving_name)`\n",
    "- `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: \tid: 1057,\n",
      "        model_path: /Projects/demo_ml_meb10000/Models/irisflowerclassifier,\n",
      "        model_version: 1,\n",
      "        artifact_version: 4,\n",
      "        predictor: predictor.py,\n",
      "        transformer: transformer.py,\n",
      "        model_server: PYTHON,\n",
      "        serving_tool: KFSERVING\n"
     ]
    }
   ],
   "source": [
    "print(\"Info: \\tid: {},\\n \\\n",
    "       model_path: {},\\n \\\n",
    "       model_version: {},\\n \\\n",
    "       artifact_version: {},\\n \\\n",
    "       predictor: {},\\n \\\n",
    "       transformer: {},\\n \\\n",
    "       model_server: {},\\n \\\n",
    "       serving_tool: {}\".format(\n",
    "    serving.get_id(SERVING_NAME),\n",
    "    serving.get_model_path(SERVING_NAME),\n",
    "    serving.get_model_version(SERVING_NAME),\n",
    "    serving.get_artifact_version(SERVING_NAME),\n",
    "    serving.get_predictor(SERVING_NAME),\n",
    "    serving.get_transformer(SERVING_NAME),\n",
    "    serving.get_model_server(SERVING_NAME),\n",
    "    serving.get_serving_tool(SERVING_NAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irisflowerclassifier\n"
     ]
    }
   ],
   "source": [
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify flowers with the Iris Flower classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start/stop the serving instance either from the Hopsworks UI or from the python/REST API as demonstrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down currently running serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if serving.get_status(SERVING_NAME) == \"Running\":\n",
    "    serving.stop(SERVING_NAME)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start new serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-12 13:40:31,332 INFO: Serving with name: irisflowerclassifier successfully started\n"
     ]
    }
   ],
   "source": [
    "serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until serving is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Server Logs\n",
    "\n",
    "You can access the server logs using Kibana by clicking on the 'Show logs' button in the action bar, and filter them using fields such as serving component (i.e., predictor or transformer) or container name among other things.\n",
    "\n",
    "![kfserving_python_pred_trans_logs](./../../../images/kfserving_python_pred_trans_logs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making inference requests you can use the utility method `serving.make_inference_request`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "NUM_FEATURES = 4\n",
    "\n",
    "for i in range(20):\n",
    "    data = {\"inputs\" : [[random.uniform(1, 8) for i in range(NUM_FEATURES)]]}\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Logs\n",
    "\n",
    "### Consume Prediction Requests and Responses using Kafka\n",
    "\n",
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO -> servingId: 3, modelName: irisflowerclassifier, modelVersion: 1,requestTimestamp: 1641994886, inferenceId:0e9dc5b9-6e34-4b58-b5d2-40f91653a764, messageType:response\n",
      "Predictions -> [2]\n",
      "\n",
      "INFO -> servingId: 3, modelName: irisflowerclassifier, modelVersion: 1,requestTimestamp: 1641994886, inferenceId:074e585a-9e7d-4b9a-8862-6974a7fe9358, messageType:response\n",
      "Predictions -> [1]\n",
      "\n",
      "INFO -> servingId: 3, modelName: irisflowerclassifier, modelVersion: 1,requestTimestamp: 1641994886, inferenceId:dff9797f-1987-4849-b076-2cf7ba8fd833, messageType:response\n",
      "Predictions -> [2]\n",
      "\n",
      "INFO -> servingId: 3, modelName: irisflowerclassifier, modelVersion: 1,requestTimestamp: 1641994886, inferenceId:851a394f-f884-4008-88c6-11c7cd5d926b, messageType:response\n",
      "Predictions -> [1]\n",
      "\n",
      "INFO -> servingId: 3, modelName: irisflowerclassifier, modelVersion: 1,requestTimestamp: 1641994887, inferenceId:cd416ea2-9f56-45a6-bca6-e5ce5bcb996c, messageType:response\n",
      "Predictions -> [2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 10):\n",
    "    msg = consumer.poll(timeout=5.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)  \n",
    "            payload = json.loads(event_dict[\"payload\"])\n",
    "            \n",
    "            if (event_dict['messageType'] == \"request\" and not PRINT_INSTANCES) or \\\n",
    "                (event_dict['messageType'] == \"response\" and not PRINT_PREDICTIONS):\n",
    "                continue\n",
    "            \n",
    "            print(\"INFO -> servingId: {}, modelName: {}, modelVersion: {},\"\\\n",
    "                  \"requestTimestamp: {}, inferenceId:{}, messageType:{}\".format(\n",
    "                       event_dict[\"servingId\"],\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"inferenceId\"],\n",
    "                       event_dict[\"messageType\"]))\n",
    "\n",
    "            if event_dict['messageType'] == \"request\":\n",
    "                print(\"Instances -> {}\\n\".format(payload['instances']))\n",
    "                \n",
    "            if event_dict['messageType'] == \"response\":\n",
    "                print(\"Predictions -> {}\\n\".format(payload['predictions']))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}