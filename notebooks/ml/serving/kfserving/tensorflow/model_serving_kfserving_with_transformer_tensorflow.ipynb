{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with KFServing, Transformers and Tensorflow - MNIST Classification\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with KFServing, Transformers and Tensorflow - MNIST Classification\n",
    "---\n",
    "\n",
    "*INPUT --> TRANSFORMER --> ENRICHED INPUT --> MODEL --> PREDICTION*\n",
    "\n",
    "<font color='red'><h3>This notebook requires KFServing</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *mnist* is already available in Hopsworks. An example of training a model for the *MNIST handwritten digit classification problem* is available in `Jupyter/experiment/Tensorflow/mnist.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "### The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Repository for best model based on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/models.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"mnist\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mnist\n",
      "Model version: 1\n",
      "{'accuracy': '0.75'}\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve the Trained Model with a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve a model with a transformer, write a python script that implements the `Transformer` class and the methods `preprocess` and `postprocess`, like this:\n",
    "\n",
    "```python\n",
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        print(\"[Transformer] Initializing...\")\n",
    "        # Initialization code goes here\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        # Transform the request inputs here. The object returned by this method will be used as model input.\n",
    "        return inputs\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        # Transform the predictions computed by the model before returning a response.\n",
    "        return outputs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring model server from artifact files: TENSORFLOW_SERVING\n",
      "Creating serving mnist for artifact /Projects/demo_ml_meb10000//Models/mnist ...\n",
      "Serving mnist successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = MODEL_NAME\n",
    "MODEL_PATH=\"/Models/\" + best_model['name']\n",
    "TRANSFORMER_PATH=hdfs.project_path() + \"/Jupyter/serving/kfserving/tensorflow/transformer.py\" # or .ipynb\n",
    "\n",
    "response = serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                                    MODEL_PATH, model_version=best_model['version'], # set the path and version of the model to be deployed\n",
    "                                    kfserving=True, # whether to serve the model using KFServing or the default tool in the current Hopsworks version\n",
    "                                    topic_name=\"CREATE\", # (optional) set the topic name or CREATE to create a new topic for inference logging\n",
    "                                    inference_logging=\"ALL\", # with KFServing, select the type of inference data to log into Kafka, e.g MODEL_INPUTS, PREDICTIONS or ALL\n",
    "                                    transformer=TRANSFORMER_PATH, \n",
    "                                    instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                                    transformer_instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n"
     ]
    }
   ],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopped'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get serving status\n",
    "serving.get_status(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify digits with the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: mnist...\n",
      "Serving with name: mnist successfully started\n"
     ]
    }
   ],
   "source": [
    "if serving.get_status(SERVING_NAME) == 'Stopped':\n",
    "    serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active servings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/servings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.0613542125, 0.101323538, 0.0399091654, 0.0345890522, 0.302067667, 0.142577931, 0.0791310892, 0.0789082348, 0.0758777037, 0.0842615142]]}\n",
      "{'predictions': [[0.0460102223, 0.0554421134, 0.0376880467, 0.0415558703, 0.214382008, 0.254266232, 0.062533088, 0.0978983194, 0.109497815, 0.0807262808]]}\n",
      "{'predictions': [[0.0668974221, 0.0909847245, 0.0988900438, 0.0637232, 0.17731753, 0.122183181, 0.0818561539, 0.0969023556, 0.118541665, 0.0827036947]]}\n",
      "{'predictions': [[0.0559709519, 0.0646299496, 0.0344434641, 0.0303664282, 0.306105494, 0.147630468, 0.0688919, 0.109457299, 0.0753086582, 0.107195415]]}\n",
      "{'predictions': [[0.0380319357, 0.0628150403, 0.0754821, 0.026162535, 0.422493875, 0.0817232728, 0.0735279322, 0.0677653, 0.0746347308, 0.0773632899]]}\n",
      "{'predictions': [[0.0665044934, 0.0668867528, 0.040783342, 0.0294370688, 0.338596195, 0.154240429, 0.0706104711, 0.0722219348, 0.107167564, 0.0535517447]]}\n",
      "{'predictions': [[0.0825293362, 0.0559448414, 0.0419535786, 0.0181798562, 0.370016396, 0.154312477, 0.0558506288, 0.111585282, 0.0492350422, 0.0603924952]]}\n",
      "{'predictions': [[0.0444687, 0.0443344265, 0.0746727288, 0.0247968901, 0.416514188, 0.0837049931, 0.102883548, 0.0640649423, 0.0908682868, 0.0536912456]]}\n",
      "{'predictions': [[0.0763259754, 0.101309389, 0.0394318104, 0.0272058602, 0.320164829, 0.117243476, 0.0681534633, 0.0669988394, 0.11084564, 0.0723207369]]}\n",
      "{'predictions': [[0.0394673645, 0.0884961, 0.0372174904, 0.0370060354, 0.335928798, 0.159753889, 0.087445952, 0.0784435645, 0.0646848083, 0.0715560839]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "NUM_FEATURES=784\n",
    "\n",
    "for i in range(10):\n",
    "    data = {\n",
    "                \"signature_name\": \"serving_default\", \"instances\": [np.random.rand(NUM_FEATURES).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Requests and Responses using Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO -> servingId: 18, modelName: mnist, modelVersion: 1,requestTimestamp: 1623766542, inferenceId:741bdbc9-4846-469a-a2fc-2427e6a22679, messageType:response\n",
      "Predictions -> [[0.0668974221, 0.0909847245, 0.0988900438, 0.0637232, 0.17731753, 0.122183181, 0.0818561539, 0.0969023556, 0.118541665, 0.0827036947]]\n",
      "\n",
      "INFO -> servingId: 18, modelName: mnist, modelVersion: 1,requestTimestamp: 1623766542, inferenceId:c0fcae6d-1106-4922-aacc-5bc201d51393, messageType:response\n",
      "Predictions -> [[0.0559709519, 0.0646299496, 0.0344434641, 0.0303664282, 0.306105494, 0.147630468, 0.0688919, 0.109457299, 0.0753086582, 0.107195415]]\n",
      "\n",
      "INFO -> servingId: 18, modelName: mnist, modelVersion: 1,requestTimestamp: 1623766543, inferenceId:78412c97-5b28-47eb-bc7f-4689f044a5fc, messageType:response\n",
      "Predictions -> [[0.0380319357, 0.0628150403, 0.0754821, 0.026162535, 0.422493875, 0.0817232728, 0.0735279322, 0.0677653, 0.0746347308, 0.0773632899]]\n",
      "\n",
      "INFO -> servingId: 18, modelName: mnist, modelVersion: 1,requestTimestamp: 1623766543, inferenceId:dbb0163a-bd86-4edb-a6c9-24e607d0b7ee, messageType:response\n",
      "Predictions -> [[0.0665044934, 0.0668867528, 0.040783342, 0.0294370688, 0.338596195, 0.154240429, 0.0706104711, 0.0722219348, 0.107167564, 0.0535517447]]\n",
      "\n",
      "INFO -> servingId: 18, modelName: mnist, modelVersion: 1,requestTimestamp: 1623766543, inferenceId:8a586324-0b77-4d9c-9f1e-bb17a9cd4195, messageType:response\n",
      "Predictions -> [[0.0825293362, 0.0559448414, 0.0419535786, 0.0181798562, 0.370016396, 0.154312477, 0.0558506288, 0.111585282, 0.0492350422, 0.0603924952]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 10):\n",
    "    msg = consumer.poll(timeout=1)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)  \n",
    "            payload = json.loads(event_dict[\"payload\"])\n",
    "            \n",
    "            if (event_dict['messageType'] == \"request\" and not PRINT_INSTANCES) or \\\n",
    "                (event_dict['messageType'] == \"response\" and not PRINT_PREDICTIONS):\n",
    "                continue\n",
    "            \n",
    "            print(\"INFO -> servingId: {}, modelName: {}, modelVersion: {},\"\\\n",
    "                  \"requestTimestamp: {}, inferenceId:{}, messageType:{}\".format(\n",
    "                       event_dict[\"servingId\"],\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"inferenceId\"],\n",
    "                       event_dict[\"messageType\"]))\n",
    "\n",
    "            if event_dict['messageType'] == \"request\":\n",
    "                print(\"Instances -> {}\\n\".format(payload['instances']))\n",
    "                \n",
    "            if event_dict['messageType'] == \"response\":\n",
    "                print(\"Predictions -> {}\\n\".format(payload['predictions']))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}