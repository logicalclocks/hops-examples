{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with Docker/Kubernetes and Scikit-Learn - Iris Flower Classification\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Docker/Kubernetes and Scikit-Learn - Iris Flower Classification\n",
    "---\n",
    "*INPUT --> MODEL --> PREDICTION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *irisflowerclassifier* is already available in Hopsworks. An example of training a model for the *Iris flower classification problem* is available in `Jupyter/end_to_end_pipelines/sklearn/end_to_end_sklearn.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "## The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>.\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a mnist folder in your Resources Dataset, the path to the mnist data would be `hdfs.project_path() + 'Resources/mnist'`\n",
    "\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "```python\n",
    "from hops import hdfs\n",
    "# Uploading the traied model to hdfs\n",
    "hdfs.copy_to_hdfs(\"iris_flower_knn.pkl\", \"Resources/iris_flower\", overwrite=True)\n",
    "\n",
    "# Downloading the iris flower model to the current working directory\n",
    "iris_flower_model_hdfs_path = hdfs.project_path() + \"Resources/iris_flower/iris_flower_knn.pkl\"\n",
    "local_iris_flower_model_path = hdfs.copy_to_local(iris_flower_model_hdfs_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the Iris Flower classifier\n",
    "\n",
    "To serve a SkLearn Model, write a python script that downloads the HDFS model in the constructor and saves it as a class variable and then implements the `Predict` class and the methods `predict`, `classify` and `regress`, like this:\n",
    "\n",
    "```python\n",
    "from sklearn.externals import joblib\n",
    "from hops import hdfs\n",
    "import os\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/irisflowerclassifier/1/iris_flower_knn.pkl\"\n",
    "        print(\"Copying SKLearn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local SkLearn model for serving\")\n",
    "        self.model = joblib.load(\"./iris_flower_knn.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are note JSON serializable\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        \"\"\" Serves a classification request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "\n",
    "    def regress(self, inputs):\n",
    "        \"\"\" Serves a regression request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "```\n",
    "\n",
    "Then upload this python script to some folder in your project and go to the \"Model Serving\" service in Hopsworks:\n",
    "\n",
    "![sklearn_serving1.png](./../../../images/sklearn_serving1.png)\n",
    "\n",
    "Then click on \"create serving\" and configure your serving:\n",
    "\n",
    "![sklearn_serving2.png](./../../../images/sklearn_serving2.png)\n",
    "\n",
    "Once the serving is created, you can start it and view information like server-logs and which kafka topic it is logging inference requests to.\n",
    "\n",
    "![sklearn_serving3.png](./../../../images/sklearn_serving3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best IrisFlowerClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: irisflowerclassifier\n",
      "Model version: 1\n",
      "{'accuracy': '0.98'}\n"
     ]
    }
   ],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "\n",
    "MODEL_NAME = \"irisflowerclassifier\"\n",
    "EVALUATION_METRIC=\"accuracy\"\n",
    "IRIS_FLOWER_CLASSIFIER_SCRIPT = \"iris_flower_classifier.py\"\n",
    "\n",
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)\n",
    "\n",
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Serving of Exported Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the files have been exported to the model directory, we can create a serving instance that points to the model files using `serving.create_or_update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring model server from artifact files: FLASK\n",
      "Creating serving irisflowerclassifier for artifact /Projects/demo_ml_meb10000/Models/irisflowerclassifier/1/iris_flower_classifier.py ...\n",
      "Serving irisflowerclassifier successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving\n",
    "SERVING_NAME = MODEL_NAME\n",
    "SCRIPT_PATH = \"Models/\" + MODEL_NAME + \"/\" + str(best_model['version']) + \"/\" + IRIS_FLOWER_CLASSIFIER_SCRIPT\n",
    "\n",
    "serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                         SCRIPT_PATH, model_version=best_model['version'], # set the path and version of the model to be deployed\n",
    "                         topic_name=\"CREATE\", # topic name or CREATE to create a new topic for inference logging, otherwise NONE\n",
    "                         instances=1 # number of replicas\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irisflowerclassifier\n"
     ]
    }
   ],
   "source": [
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the serving have been created, you can find it in the Hopsworks UI by going to the \"Model Serving\" tab. You can also use the python module to query the Hopsworks REST API about information on the existing servings using methods like: \n",
    "\n",
    "- `get_servings()`\n",
    "- `get_serving_id(serving_name)`\n",
    "- `get_serving_model_path(serving_name)`\n",
    "- `get_serving_type(serving_name)`\n",
    "- `get_serving_version(serving_name)`\n",
    "- `get_serving_kafka_topic(serving_name)`\n",
    "- `get_serving_status(serving_name)`\n",
    "- `exist(serving_name)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: \tid: 789,\n",
      "        model_path: /Projects/demo_ml_meb10000/Models/irisflowerclassifier/1/iris_flower_classifier.py,\n",
      "        model_version: 1,\n",
      "        artifact_version: 0,\n",
      "        model_server: FLASK,\n",
      "        serving_tool: DEFAULT\n"
     ]
    }
   ],
   "source": [
    "print(\"Info: \\tid: {},\\n \\\n",
    "       model_path: {},\\n \\\n",
    "       model_version: {},\\n \\\n",
    "       artifact_version: {},\\n \\\n",
    "       model_server: {},\\n \\\n",
    "       serving_tool: {}\".format(\n",
    "    serving.get_id(SERVING_NAME),\n",
    "    serving.get_model_path(SERVING_NAME),\n",
    "    serving.get_model_version(SERVING_NAME),\n",
    "    serving.get_artifact_version(SERVING_NAME),\n",
    "    serving.get_model_server(SERVING_NAME),\n",
    "    serving.get_serving_tool(SERVING_NAME)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or describe a serving using\n",
    "- `describe(serving_name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 789, name: irisflowerclassifier, model_path: /Projects/demo_ml_meb10000/Models/irisflowerclassifier/1/iris_flower_classifier.py, model_version: 1, artifact_version: 0, transformer: None, model_server: FLASK, serving_tool: DEFAULT, requested_instances: 1, available_instances: 0, available_transformer_instances: 0, predictor_resource_config: {'cores': 1, 'memory': 1024, 'gpus': 0}, creator: Admin Admin, created: 2021-07-07T09:44:24Z, status: Stopped, kafka_topic_dto: <hops.kafka.KafkaTopicDTO object at 0x7f5a10ff13d0>\n"
     ]
    }
   ],
   "source": [
    "serving.describe(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify flowers with the Iris Flower classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start/stop the serving instance either from the Hopsworks UI or from the python/REST API as demonstrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down currently running serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if serving.get_status(SERVING_NAME) == \"Running\":\n",
    "    serving.stop(SERVING_NAME)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start new serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: irisflowerclassifier...\n",
      "Serving with name: irisflowerclassifier successfully started\n"
     ]
    }
   ],
   "source": [
    "serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until serving is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making inference requests you can use the utility method `serving.make_inference_request`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [2]}\n",
      "{'predictions': [1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "NUM_FEATURES = 4\n",
    "\n",
    "for i in range(20):\n",
    "    data = {\"inputs\" : [[random.uniform(1, 8) for i in range(NUM_FEATURES)]]}\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Logs\n",
    "\n",
    "### Consume Prediction Requests and Responses using Kafka\n",
    "\n",
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving: irisflowerclassifier, version: 1, timestamp: 1625651538595,\n",
      "        http_response_code: 200, model_server: FLASK, serving_tool: DEFAULT\n",
      "predictions: 2\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1625651538772,\n",
      "        http_response_code: 200, model_server: FLASK, serving_tool: DEFAULT\n",
      "predictions: 2\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1625651538928,\n",
      "        http_response_code: 200, model_server: FLASK, serving_tool: DEFAULT\n",
      "predictions: 2\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1625651539069,\n",
      "        http_response_code: 200, model_server: FLASK, serving_tool: DEFAULT\n",
      "predictions: 2\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1625651539230,\n",
      "        http_response_code: 200, model_server: FLASK, serving_tool: DEFAULT\n",
      "predictions: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 5):\n",
    "    msg = consumer.poll(timeout=1.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"        http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                prediction = json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]\n",
    "                print(\"predictions: {}\\n\".format(prediction))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}