{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with Docker/Kubernetes and Scikit-learn - Iris Flower Classification\"\n",
    "date: 2021-01-10\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Docker/Kubernetes and Scikit-learn - Iris Flower Classification\n",
    "---\n",
    "*INPUT --> PREDICTOR (MODEL) --> PREDICTION*\n",
    "\n",
    "Serving scikit-learn models with Docker or Kubernetes requires the implementation of a **predictor** script (i.e., python script) that implements the *Predict* class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *irisflowerclassifier* is already available in Hopsworks. An example of training a model for the *Iris flower classification problem* is available in `Jupyter/end_to_end_pipelines/sklearn/end_to_end_sklearn.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks) \n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "### Hopsworks Machine Learning (HSML) library\n",
    "\n",
    "`hsml` is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models. To learn more about `hsml`, see the <a href=\"https://docs.hopsworks.ai/machine-learning-api/latest\">Hopsworks Model Management</a> docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the Iris Flower classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor script\n",
    "\n",
    "To serve a Scikit-Learn Model, write a predictor script in Python that downloads the HDFS model in the constructor and saves it as a class variable and then implements the `Predict` class and the methods `predict`, `classify` and `regress`, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import joblib\n",
    "from hops import hdfs\n",
    "import os\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/irisflowerclassifier/1/iris_knn.pkl\"\n",
    "        print(\"Copying SKLearn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local SkLearn model for serving\")\n",
    "        self.model = joblib.load(\"./iris_knn.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are note JSON serializable\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        \"\"\" Serves a classification request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "\n",
    "    def regress(self, inputs):\n",
    "        \"\"\" Serves a regression request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a connection to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "# Connect with Hopsworks\n",
    "conn = hsml.connection()\n",
    "\n",
    "# Retrieve the model registry handle\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Registry for best Iris Flower Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: irisflowerclassifier\n",
      "Model version: 1\n",
      "Model metrics: {'accuracy': '0.98'}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"irisflowerclassifier\"\n",
    "\n",
    "best_model = mr.get_best_model(MODEL_NAME, \"accuracy\", \"max\")\n",
    "\n",
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print('Model metrics: ' + str(best_model.training_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Deployment for the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the deployment has been created, you can find it in the Hopsworks UI by going to the \"Deployments\" tab. You can also use the class attributes or the `.describe()` method of a deployment object to access its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_SCRIPT_PATH = mr.project_path + \"/Jupyter/serving/docker_and_kubernetes/python/predictor.py\" # or .ipynb\n",
    "\n",
    "# Deploy the trained model\n",
    "irisclassifier = best_model.deploy(script_file=PREDICTOR_SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the serving have been created, you can find it in the Hopsworks UI by going to the \"Deployments\" tab. You can also use the class attributes or the `.describe()` method of a deployment object to describe access its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment: irisflowerclassifier\n",
      "{\n",
      "    \"artifact_version\": 2,\n",
      "    \"batching_enabled\": false,\n",
      "    \"created\": \"2022-05-18T13:18:37.853Z\",\n",
      "    \"creator\": \"Admin Admin\",\n",
      "    \"id\": 4,\n",
      "    \"inference_logging\": \"ALL\",\n",
      "    \"kafka_topic_dto\": {\n",
      "        \"name\": \"CREATE\",\n",
      "        \"num_of_partitions\": 1,\n",
      "        \"num_of_replicas\": 1\n",
      "    },\n",
      "    \"model_name\": \"irisflowerclassifier\",\n",
      "    \"model_path\": \"/Projects/demo_ml_meb10000/Models/irisflowerclassifier\",\n",
      "    \"model_server\": \"PYTHON\",\n",
      "    \"model_version\": 1,\n",
      "    \"name\": \"irisflowerclassifier\",\n",
      "    \"predictor\": \"predictor.py\",\n",
      "    \"predictor_resource_config\": {\n",
      "        \"cores\": 1,\n",
      "        \"gpus\": 0,\n",
      "        \"memory\": 1024\n",
      "    },\n",
      "    \"requested_instances\": 1,\n",
      "    \"serving_tool\": \"DEFAULT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Deployment: \" + irisclassifier.name)\n",
    "irisclassifier.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify flowers with the Iris Flower classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b480f0babdba44e5a428991a4d4aeade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if irisclassifier.get_state().status == \"Stopped\":\n",
    "    irisclassifier.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Model Server Logs\n",
    "\n",
    "You can access the server logs using Kibana by clicking on the 'Show logs' button in the action bar, and filter them using fields such as serving component (i.e., predictor or transformer) or container name among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![docker_sklearn_predictor_logs.gif](./../../../images/docker_sklearn_predictor_logs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making inference requests you can use the `.predict()` method of the deployment metadata object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n",
      "{'predictions': [0]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    data = {\"instances\" : [best_model.input_example]}\n",
    "    predictions = irisclassifier.predict(data)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Logs\n",
    "\n",
    "### Consume Prediction Requests and Responses using Kafka\n",
    "\n",
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = irisclassifier.inference_logger.kafka_topic.name\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving: irisflowerclassifier, version: 1, timestamp: 1652883610404,\n",
      "        http_response_code: 200, model_server: PYTHON, serving_tool: DEFAULT\n",
      "predictions: 0\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1652883611088,\n",
      "        http_response_code: 200, model_server: PYTHON, serving_tool: DEFAULT\n",
      "predictions: 0\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1652883611898,\n",
      "        http_response_code: 200, model_server: PYTHON, serving_tool: DEFAULT\n",
      "predictions: 0\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1652883612895,\n",
      "        http_response_code: 200, model_server: PYTHON, serving_tool: DEFAULT\n",
      "predictions: 0\n",
      "\n",
      "serving: irisflowerclassifier, version: 1, timestamp: 1652883613715,\n",
      "        http_response_code: 200, model_server: PYTHON, serving_tool: DEFAULT\n",
      "predictions: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 5):\n",
    "    msg = consumer.poll(timeout=5.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"        http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                prediction = json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]\n",
    "                print(\"predictions: {}\\n\".format(prediction))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}