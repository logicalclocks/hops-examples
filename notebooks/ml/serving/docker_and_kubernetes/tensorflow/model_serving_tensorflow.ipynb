{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\n",
    "---\n",
    "*INPUT --> MODEL --> PREDICTION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *mnist* is already available in Hopsworks. An example of training a model for the *MNIST handwritten digit classification problem* is available in `Jupyter/end_to_end_pipelines/tensorflow/end_to_end_tensorflow.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "### The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Repository for best model based on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/models.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"mnist\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mnist\n",
      "Model version: 1\n",
      "{'accuracy': '0.75'}\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Serving of Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring model server from artifact files: TENSORFLOW_SERVING\n",
      "Creating serving mnist for artifact /Projects/demo_ml_meb10000//Models/mnist ...\n",
      "Serving mnist successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = MODEL_NAME\n",
    "MODEL_PATH=\"/Models/\" + best_model['name']\n",
    "\n",
    "response = serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                                    MODEL_PATH, model_version=best_model['version'], # set the path and version of the model to be deployed\n",
    "                                    kfserving=False, # the model will be served either with Docker or Kubernetes depending on the Hopsworks version\n",
    "                                    topic_name=\"CREATE\", # (optional) set the topic name or CREATE to create a new topic for inference logging\n",
    "                                    instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n"
     ]
    }
   ],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopped'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get serving status\n",
    "serving.get_status(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify digits with the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: mnist...\n",
      "Serving with name: mnist successfully started\n"
     ]
    }
   ],
   "source": [
    "if serving.get_status(SERVING_NAME) == 'Stopped':\n",
    "    serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active servings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/servings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.0570081174, 0.0655516908, 0.0384815671, 0.0236714445, 0.326048434, 0.208096072, 0.0829753354, 0.081687, 0.0640862733, 0.0523940176]]}\n",
      "{'predictions': [[0.0410101265, 0.0716211, 0.0464931, 0.0277518351, 0.239403486, 0.129260719, 0.0967375934, 0.106829077, 0.165632, 0.075260967]]}\n",
      "{'predictions': [[0.0452777408, 0.0637608, 0.0491008312, 0.0400233492, 0.346460611, 0.0908036157, 0.0619194657, 0.0982931, 0.114815064, 0.0895455331]]}\n",
      "{'predictions': [[0.0314507075, 0.0585816801, 0.0445142835, 0.0438556336, 0.273136318, 0.215605736, 0.0794836283, 0.130447, 0.0517816357, 0.0711433962]]}\n",
      "{'predictions': [[0.0698710307, 0.0869943276, 0.0301038921, 0.0247928668, 0.286450505, 0.167296484, 0.094396539, 0.0698259771, 0.11602433, 0.0542440601]]}\n",
      "{'predictions': [[0.0860077, 0.0403077304, 0.0244597942, 0.0264858678, 0.27012223, 0.163445517, 0.0495519452, 0.056668926, 0.170305803, 0.112644464]]}\n",
      "{'predictions': [[0.0458639339, 0.0367544517, 0.0322426595, 0.0237141084, 0.434011459, 0.172226384, 0.0522794165, 0.0790228918, 0.0574153252, 0.0664693117]]}\n",
      "{'predictions': [[0.0509353839, 0.0694219097, 0.0321636237, 0.0302796885, 0.377082, 0.0967385247, 0.0627277941, 0.0596957766, 0.129347622, 0.0916076154]]}\n",
      "{'predictions': [[0.0929256454, 0.084371306, 0.0401998349, 0.0336654373, 0.23777315, 0.168973163, 0.0494310856, 0.10867174, 0.0850041956, 0.0989845097]]}\n",
      "{'predictions': [[0.102271199, 0.0851190835, 0.0694739893, 0.0348733179, 0.261094034, 0.118886627, 0.0565741286, 0.0851207152, 0.0804453865, 0.106141426]]}\n",
      "{'predictions': [[0.0288008358, 0.0665152222, 0.0653517917, 0.0326284356, 0.26474762, 0.172006473, 0.0978971422, 0.0685486645, 0.0634336, 0.14007026]]}\n",
      "{'predictions': [[0.0499485396, 0.0927426, 0.0480168052, 0.0398117565, 0.300983, 0.18416892, 0.0596857667, 0.0824098736, 0.0793504193, 0.0628822893]]}\n",
      "{'predictions': [[0.0330961198, 0.038298443, 0.0398440063, 0.0286740121, 0.463378936, 0.182802454, 0.0649509579, 0.0510277376, 0.0580670796, 0.0398602225]]}\n",
      "{'predictions': [[0.0428033546, 0.0566293746, 0.0597526729, 0.027359141, 0.361109078, 0.130646974, 0.0565278865, 0.0533226, 0.139279976, 0.0725689903]]}\n",
      "{'predictions': [[0.0815023333, 0.0703946352, 0.0303218532, 0.0270094294, 0.399567336, 0.0814294592, 0.0649439767, 0.0777247623, 0.0781110227, 0.0889950916]]}\n",
      "{'predictions': [[0.0542477667, 0.0503175519, 0.0552789271, 0.033593718, 0.34673205, 0.120051116, 0.084772341, 0.10054718, 0.0648765489, 0.0895828158]]}\n",
      "{'predictions': [[0.0621216558, 0.0424978137, 0.0395628363, 0.0377483591, 0.28446573, 0.200037196, 0.0966890827, 0.0949611291, 0.069457829, 0.0724583194]]}\n",
      "{'predictions': [[0.0257849973, 0.0820573047, 0.0741208941, 0.0209595, 0.270556331, 0.115839921, 0.0773180723, 0.133814871, 0.0846348703, 0.114913322]]}\n",
      "{'predictions': [[0.0392508022, 0.0422034599, 0.0526005216, 0.0182473138, 0.429069, 0.172948882, 0.0755125433, 0.0646678358, 0.0515763573, 0.0539232716]]}\n",
      "{'predictions': [[0.0543204099, 0.0507360362, 0.0692701191, 0.0304225981, 0.417896062, 0.114129879, 0.0645712093, 0.0764147639, 0.0615315959, 0.0607073642]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "NUM_FEATURES=784\n",
    "\n",
    "for i in range(20):\n",
    "    data = {\n",
    "                \"signature_name\": \"serving_default\", \"instances\": [np.random.rand(NUM_FEATURES).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Requests and Responses using Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving: mnist, version: 1, timestamp: 1623765611867,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0570081174, 0.0655516908, 0.0384815671, 0.0236714445, 0.326048434, 0.208096072, 0.0829753354, 0.081687, 0.0640862733, 0.0523940176]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1623765612038,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0410101265, 0.0716211, 0.0464931, 0.0277518351, 0.239403486, 0.129260719, 0.0967375934, 0.106829077, 0.165632, 0.075260967]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1623765612232,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0452777408, 0.0637608, 0.0491008312, 0.0400233492, 0.346460611, 0.0908036157, 0.0619194657, 0.0982931, 0.114815064, 0.0895455331]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1623765612459,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0314507075, 0.0585816801, 0.0445142835, 0.0438556336, 0.273136318, 0.215605736, 0.0794836283, 0.130447, 0.0517816357, 0.0711433962]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1623765612678,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0698710307, 0.0869943276, 0.0301038921, 0.0247928668, 0.286450505, 0.167296484, 0.094396539, 0.0698259771, 0.11602433, 0.0542440601]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 5):\n",
    "    msg = consumer.poll(timeout=1.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"        http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                print(\"predictions: {}\\n\".format(json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]))\n",
    "                      \n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}