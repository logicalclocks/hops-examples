{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\"\n",
    "date: 2022-01-10\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\n",
    "---\n",
    "*INPUT --> MODEL --> PREDICTION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *mnist* is already available in Hopsworks. An example of training a model for the *MNIST handwritten digit classification problem* is available in `Jupyter/end_to_end_pipelines/tensorflow/end_to_end_tensorflow.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "### The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Registry for best Mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mnist_e2e\n",
      "Model version: 1\n",
      "{'accuracy': '0.625'}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME=\"mnist_e2e\"\n",
    "EVALUATION_METRIC=\"accuracy\"\n",
    "\n",
    "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"max\")\n",
    "\n",
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Serving of Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 11:45:02,168 INFO: Serving mniste2e successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = \"mniste2e\"\n",
    "\n",
    "response = serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                                    model_path=best_model.model_path, # set the path of the model to be deployed\n",
    "                                    model_server=\"TENSORFLOW_SERVING\", # set the model server to run the model\n",
    "                                    # optional arguments\n",
    "                                    model_version=best_model.version, # set the version of the model to be deployed\n",
    "                                    kfserving=False, # the model will be served either with Docker or Kubernetes depending on the Hopsworks version\n",
    "                                    topic_name=\"CREATE\", # (optional) set the topic name or CREATE to create a new topic for inference logging\n",
    "                                    instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the serving instance is created, it will be shown in the \"Model Serving\" tab in the Hopsworks UI. You can view detailed information like server-logs and which Kafka Topic it is logging inference requests to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the Python module to query the Hopsworks REST API about information on the existing servings using methods like: \n",
    "\n",
    "- `get_all()`\n",
    "- `get_id(serving_name)`\n",
    "- `get_model_path(serving_name)`\n",
    "- `get_model_version(serving_name)`\n",
    "- `get_artifact_version(serving_name)`\n",
    "- `get_kafka_topic(serving_name)`\n",
    "- `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: \tid: 31,\n",
      "        model_path: /Projects/demo_ml_meb10000/Models/mnist_e2e,\n",
      "        model_version: 1,\n",
      "        artifact_version: 0,\n",
      "        model_server: TENSORFLOW_SERVING,\n",
      "        serving_tool: DEFAULT\n"
     ]
    }
   ],
   "source": [
    "print(\"Info: \\tid: {},\\n \\\n",
    "       model_path: {},\\n \\\n",
    "       model_version: {},\\n \\\n",
    "       artifact_version: {},\\n \\\n",
    "       model_server: {},\\n \\\n",
    "       serving_tool: {}\".format(\n",
    "    serving.get_id(SERVING_NAME),\n",
    "    serving.get_model_path(SERVING_NAME),\n",
    "    serving.get_model_version(SERVING_NAME),\n",
    "    serving.get_artifact_version(SERVING_NAME),\n",
    "    serving.get_model_server(SERVING_NAME),\n",
    "    serving.get_serving_tool(SERVING_NAME)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mniste2e\n"
     ]
    }
   ],
   "source": [
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Digits with the MNIST Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-28 11:45:03,776 INFO: Serving with name: mniste2e successfully started\n"
     ]
    }
   ],
   "source": [
    "if serving.get_status(SERVING_NAME) == 'Stopped':\n",
    "    serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active servings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/servings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.00572621496, 0.108208649, 0.27227217, 0.0510373078, 0.299194, 0.0186527651, 0.0198455323, 0.0278544612, 0.152300492, 0.0449083634]]}\n",
      "{'predictions': [[0.0046633645, 0.0414372683, 0.291441888, 0.0312344842, 0.444997281, 0.0112954751, 0.014659116, 0.0170055088, 0.113880813, 0.0293848235]]}\n",
      "{'predictions': [[0.0076998882, 0.0302102603, 0.235633671, 0.0153572187, 0.287438601, 0.0136197172, 0.0237444546, 0.0156203927, 0.322000772, 0.0486749858]]}\n",
      "{'predictions': [[0.0108826989, 0.0823879838, 0.210901648, 0.0343613736, 0.331943542, 0.0128106838, 0.0406103022, 0.0279905479, 0.179413438, 0.0686978]]}\n",
      "{'predictions': [[0.0109353121, 0.0673055351, 0.39981252, 0.0546917692, 0.294156522, 0.0124330763, 0.0245132856, 0.0148018971, 0.0674069598, 0.0539430939]]}\n",
      "{'predictions': [[0.0114273727, 0.0790175945, 0.13507475, 0.0827436745, 0.370159179, 0.00988207199, 0.0314267166, 0.0309457481, 0.195226014, 0.0540968478]]}\n",
      "{'predictions': [[0.0133531187, 0.0599063262, 0.247828186, 0.0464959219, 0.427820921, 0.0191995669, 0.0206612721, 0.0281933304, 0.0847786739, 0.0517626703]]}\n",
      "{'predictions': [[0.00723468931, 0.111925527, 0.263983548, 0.0296120755, 0.357140779, 0.0177987386, 0.0169458222, 0.0362103, 0.104185537, 0.0549630672]]}\n",
      "{'predictions': [[0.00825782213, 0.0608470626, 0.249044895, 0.0373915546, 0.354562044, 0.0104568275, 0.0274981633, 0.0231450014, 0.158275828, 0.0705208555]]}\n",
      "{'predictions': [[0.0113548581, 0.0370724536, 0.236185282, 0.0362716, 0.245105356, 0.0161779951, 0.0189345255, 0.0334743597, 0.306493491, 0.0589301251]]}\n",
      "{'predictions': [[0.00681277085, 0.11164733, 0.289175451, 0.0336382575, 0.261108965, 0.0185933951, 0.0239503831, 0.0405027345, 0.17715925, 0.0374115221]]}\n",
      "{'predictions': [[0.0111160725, 0.0805897564, 0.239300177, 0.0385839716, 0.3445, 0.0188862346, 0.0326525047, 0.0327585265, 0.153218359, 0.0483945422]]}\n",
      "{'predictions': [[0.00481388066, 0.0545748733, 0.238435447, 0.034287069, 0.353871942, 0.0114829037, 0.0271637905, 0.0178126022, 0.156495899, 0.101061471]]}\n",
      "{'predictions': [[0.00633331668, 0.0470374823, 0.339952618, 0.0310638566, 0.306065679, 0.013985374, 0.0280421339, 0.0124483872, 0.154721156, 0.0603500269]]}\n",
      "{'predictions': [[0.0117167076, 0.0657213852, 0.251739472, 0.0289249122, 0.410816878, 0.011261181, 0.040211603, 0.011705935, 0.13377139, 0.0341305584]]}\n",
      "{'predictions': [[0.00604272867, 0.0362226181, 0.258314073, 0.0260963496, 0.358961761, 0.00796514843, 0.0122928983, 0.0202498324, 0.238217086, 0.0356375128]]}\n",
      "{'predictions': [[0.00509981066, 0.0535564, 0.321242362, 0.0374574, 0.262698799, 0.0172182638, 0.0105276722, 0.0232814737, 0.194753274, 0.0741645545]]}\n",
      "{'predictions': [[0.00960638188, 0.108674034, 0.268778, 0.0699506328, 0.185300842, 0.0157204196, 0.0365220755, 0.0281580407, 0.212751329, 0.0645382553]]}\n",
      "{'predictions': [[0.00637084339, 0.0884655416, 0.20060347, 0.04854545, 0.203479424, 0.0286499448, 0.0381827056, 0.0213748738, 0.325430036, 0.0388977155]]}\n",
      "{'predictions': [[0.00527361361, 0.0295326672, 0.279165745, 0.0256425422, 0.448510081, 0.0165301226, 0.0195797198, 0.0204584897, 0.122249223, 0.0330577828]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "NUM_FEATURES=784\n",
    "\n",
    "for i in range(20):\n",
    "    data = {\n",
    "                \"signature_name\": \"serving_default\", \"instances\": [np.random.rand(NUM_FEATURES).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Requests and Responses using Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving: mniste2e, version: 1, timestamp: 1643370334068,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.00531054195, 0.0404060595, 0.290843785, 0.0300603732, 0.406977743, 0.012764995, 0.0235719737, 0.0228966139, 0.123488344, 0.0436795913]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1643370334060,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.00717737712, 0.0555244572, 0.316040367, 0.0621430837, 0.293955833, 0.0159867015, 0.0470166355, 0.0125214364, 0.149252892, 0.0403812565]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1643370334059,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.00496881455, 0.0392289087, 0.340284616, 0.0283909775, 0.31322825, 0.0148269441, 0.0102977483, 0.0242117718, 0.158610448, 0.0659515]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1643370334060,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.011219847, 0.0522725135, 0.23986721, 0.0267774705, 0.435368598, 0.00980452728, 0.0200000331, 0.022984853, 0.139044553, 0.0426604077]\n",
      "\n",
      "serving: mniste2e, version: 1, timestamp: 1643370334411,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.00758659374, 0.076115191, 0.349409163, 0.0237191711, 0.286886334, 0.0122001776, 0.00834187213, 0.029796049, 0.148333102, 0.0576123595]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 5):\n",
    "    msg = consumer.poll(timeout=5.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"        http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                print(\"predictions: {}\\n\".format(json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]))\n",
    "                      \n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}