{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Docker/Kubernetes and Tensorflow - MNIST Classification\n",
    "---\n",
    "*INPUT --> MODEL --> PREDICTION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** It is assumed that a model called *mnist* is already available in Hopsworks. An example of training a model for the *MNIST handwritten digit classification problem* is available in `Jupyter/end_to_end_pipelines/tensorflow/end_to_end_tensorflow.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)\n",
    "\n",
    "![hops.png](../../../images/hops.png)\n",
    "\n",
    "### The `hops` python library\n",
    "\n",
    "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
    "\n",
    "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Repository for best model based on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/models.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Model Repository for best mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsml\n",
    "MODEL_NAME=\"mnist\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()\n",
    "\n",
    "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: mnist\n",
      "Model version: 1\n",
      "{'accuracy': '0.71875'}\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Serving of Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring model server from artifact files: TENSORFLOW_SERVING\n",
      "Creating serving mnist for artifact /Projects/demo_ml_meb10000//Models/mnist ...\n",
      "Serving mnist successfully created\n"
     ]
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = MODEL_NAME\n",
    "\n",
    "response = serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
    "                                    best_model.model_path, model_version=best_model.version, # set the path and version of the model to be deployed\n",
    "                                    kfserving=False, # the model will be served either with Docker or Kubernetes depending on the Hopsworks version\n",
    "                                    topic_name=\"CREATE\", # (optional) set the topic name or CREATE to create a new topic for inference logging\n",
    "                                    instances=1, # with KFServing, set 0 instances to leverage scale-to-zero capabilities\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist\n"
     ]
    }
   ],
   "source": [
    "# List all available servings in the project\n",
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopped'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get serving status\n",
    "serving.get_status(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify digits with the MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Model Serving Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting serving with name: mnist...\n",
      "Serving with name: mnist successfully started\n"
     ]
    }
   ],
   "source": [
    "if serving.get_status(SERVING_NAME) == 'Stopped':\n",
    "    serving.start(SERVING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while serving.get_status(SERVING_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active servings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../../../images/servings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[0.0171465632, 0.048792474, 0.0487240963, 0.125329539, 0.154285505, 0.0121596335, 0.405637234, 0.116209693, 0.0209982134, 0.0507171303]]}\n",
      "{'predictions': [[0.00984500162, 0.0813344792, 0.0794480368, 0.229178905, 0.146066204, 0.0101964734, 0.356174409, 0.037145935, 0.0207445342, 0.0298659969]]}\n",
      "{'predictions': [[0.0314147845, 0.0672961771, 0.0496617593, 0.137096703, 0.171351343, 0.00814313535, 0.397425473, 0.0644465834, 0.0328251459, 0.0403388739]]}\n",
      "{'predictions': [[0.0181549136, 0.038787093, 0.0792503878, 0.264465064, 0.136571646, 0.00646544108, 0.297726512, 0.0910884291, 0.0375934504, 0.0298969876]]}\n",
      "{'predictions': [[0.0168738756, 0.0996855795, 0.0380544215, 0.117498912, 0.1118147, 0.0165658947, 0.379053235, 0.102008328, 0.0545530841, 0.0638920367]]}\n",
      "{'predictions': [[0.0176697783, 0.0455756187, 0.0470717624, 0.0988537148, 0.0915572941, 0.0109392768, 0.496714264, 0.101835445, 0.0549756736, 0.0348071]]}\n",
      "{'predictions': [[0.0205914453, 0.0654138848, 0.0507608578, 0.109451666, 0.14756158, 0.00987146609, 0.431472629, 0.090302147, 0.0381189436, 0.0364553928]]}\n",
      "{'predictions': [[0.0319844857, 0.0610748753, 0.0706905946, 0.14923127, 0.150066555, 0.011391649, 0.384640843, 0.0522877201, 0.065342471, 0.0232895259]]}\n",
      "{'predictions': [[0.019955175, 0.0508767068, 0.102297261, 0.0950483, 0.26953721, 0.0109694609, 0.299288541, 0.05249957, 0.0426387936, 0.056888964]]}\n",
      "{'predictions': [[0.0268841349, 0.0481890179, 0.0405104123, 0.223767355, 0.0753803477, 0.012642215, 0.435542524, 0.0377673581, 0.0656719, 0.0336447731]]}\n",
      "{'predictions': [[0.0316545069, 0.0524385, 0.0268935878, 0.14233245, 0.0910583362, 0.010664613, 0.481864959, 0.0971154124, 0.0333014913, 0.0326760858]]}\n",
      "{'predictions': [[0.0146529395, 0.0730243102, 0.0754668266, 0.0760785118, 0.158810198, 0.00608280115, 0.481211245, 0.0451714061, 0.0347146802, 0.0347870588]]}\n",
      "{'predictions': [[0.0298648961, 0.0861661658, 0.0382564589, 0.114841245, 0.122694105, 0.0161959752, 0.422252536, 0.0983899385, 0.0416136682, 0.029725058]]}\n",
      "{'predictions': [[0.0280318856, 0.0634325817, 0.0437133424, 0.17799063, 0.286054105, 0.0121226059, 0.221906558, 0.0796185061, 0.0358458944, 0.0512838848]]}\n",
      "{'predictions': [[0.035763368, 0.0686579943, 0.106324136, 0.192474946, 0.259408325, 0.00920116, 0.226656079, 0.0474149771, 0.0272313487, 0.0268676374]]}\n",
      "{'predictions': [[0.0138048641, 0.0592474192, 0.0382033624, 0.154716209, 0.165679768, 0.0121336132, 0.366904467, 0.0977301672, 0.0177424084, 0.0738376752]]}\n",
      "{'predictions': [[0.0275360607, 0.0372710526, 0.0691219494, 0.151717484, 0.195877165, 0.0133077614, 0.28821364, 0.125464097, 0.0411424451, 0.0503483824]]}\n",
      "{'predictions': [[0.0236973837, 0.0719966814, 0.0619147, 0.18794328, 0.185334459, 0.0170248579, 0.305889249, 0.0743705, 0.0384167694, 0.033412084]]}\n",
      "{'predictions': [[0.0275418516, 0.0509299189, 0.0435518362, 0.206807852, 0.144844249, 0.0171949361, 0.304691344, 0.0956295207, 0.0572692901, 0.0515392]]}\n",
      "{'predictions': [[0.0305586215, 0.0941101536, 0.0405069329, 0.167417407, 0.0907427, 0.0269653387, 0.356301039, 0.104508244, 0.0603947118, 0.0284948815]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "NUM_FEATURES=784\n",
    "\n",
    "for i in range(20):\n",
    "    data = {\n",
    "                \"signature_name\": \"serving_default\", \"instances\": [np.random.rand(NUM_FEATURES).tolist()]\n",
    "            }\n",
    "    response = serving.make_inference_request(SERVING_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Prediction Requests and Responses using Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Kafka consumer and subscribe to the topic containing the prediction logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
    "\n",
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeout.. no more messages to read from topic\n",
      "serving: mnist, version: 1, timestamp: 1634307452661,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0171465632, 0.048792474, 0.0487240963, 0.125329539, 0.154285505, 0.0121596335, 0.405637234, 0.116209693, 0.0209982134, 0.0507171303]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1634307452775,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.00984500162, 0.0813344792, 0.0794480368, 0.229178905, 0.146066204, 0.0101964734, 0.356174409, 0.037145935, 0.0207445342, 0.0298659969]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1634307452864,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0314147845, 0.0672961771, 0.0496617593, 0.137096703, 0.171351343, 0.00814313535, 0.397425473, 0.0644465834, 0.0328251459, 0.0403388739]\n",
      "\n",
      "serving: mnist, version: 1, timestamp: 1634307452968,\n",
      "        http_response_code: 200, model_server: TENSORFLOW_SERVING, serving_tool: DEFAULT\n",
      "predictions: [0.0181549136, 0.038787093, 0.0792503878, 0.264465064, 0.136571646, 0.00646544108, 0.297726512, 0.0910884291, 0.0375934504, 0.0298969876]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_INSTANCES=False\n",
    "PRINT_PREDICTIONS=True\n",
    "\n",
    "for i in range(0, 5):\n",
    "    msg = consumer.poll(timeout=5.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            \n",
    "            print(\"serving: {}, version: {}, timestamp: {},\\n\"\\\n",
    "                  \"        http_response_code: {}, model_server: {}, serving_tool: {}\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"modelServer\"],\n",
    "                       event_dict[\"servingTool\"]))\n",
    "            \n",
    "            if PRINT_INSTANCES:\n",
    "                print(\"instances: {}\\n\".format(event_dict[\"inferenceRequest\"]))\n",
    "            if PRINT_PREDICTIONS:\n",
    "                print(\"predictions: {}\\n\".format(json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]))\n",
    "                      \n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}