{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Model Serving with KServe, Tensorflow and Transformers - MNIST Classification\"\n",
        "date: 2021-02-24\n",
        "type: technical_note\n",
        "draft: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Serving with KServe, Tensorflow and Transformers - MNIST Classification\n",
        "---\n",
        "\n",
        "*INPUT --> TRANSFORMER --> ENRICHED INPUT --> MODEL --> PREDICTION*\n",
        "\n",
        "<font color='red'><h3>This notebook requires KServe to be installed</h3></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NOTE:** It is assumed that a model called *mnist* is already available in Hopsworks. An example of training a model for the *MNIST handwritten digit classification problem* is available in `Jupyter/experiment/Tensorflow/mnist.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Serving on [Hopsworks](https://github.com/logicalclocks/hopsworks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![hops.png](../../../images/hops.png)\n",
        "\n",
        "### The `hops` python library\n",
        "\n",
        "`hops` is a helper library for Hops that facilitates development by hiding the complexity of running applications and iteracting with services.\n",
        "\n",
        "Have a feature request or encountered an issue? Please let us know on <a href=\"https://github.com/logicalclocks/hops-util-py\">github</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serve the MNIST classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Model Repository for best model based on accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image7-Monitor.png](../../../images/models.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query Model Repository for best mnist Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected. Call `.close()` to terminate connection gracefully.\n"
          ]
        }
      ],
      "source": [
        "import hsml\n",
        "\n",
        "conn = hsml.connection()\n",
        "mr = conn.get_model_registry()\n",
        "\n",
        "MODEL_NAME=\"mnist_e2e\"\n",
        "EVALUATION_METRIC=\"accuracy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"max\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: mnist_e2e\n",
            "Model version: 1\n",
            "{'accuracy': '0.625'}\n"
          ]
        }
      ],
      "source": [
        "print('Model name: ' + best_model.name)\n",
        "print('Model version: ' + str(best_model.version))\n",
        "print(best_model.training_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serve the Trained Model with a Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To serve a model with a transformer, write a python script that implements the `Transformer` class and the methods `preprocess` and `postprocess`, like this:\n",
        "\n",
        "```python\n",
        "class Transformer(object):\n",
        "    def __init__(self):\n",
        "        print(\"[Transformer] Initializing...\")\n",
        "        # Initialization code goes here\n",
        "\n",
        "    def preprocess(self, inputs):\n",
        "        # Transform the request inputs here. The object returned by this method will be used as model input.\n",
        "        return inputs\n",
        "\n",
        "    def postprocess(self, outputs):\n",
        "        # Transform the predictions computed by the model before returning a response.\n",
        "        return outputs\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from hops import serving\n",
        "from hops import hdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-28 11:59:32,893 INFO: Serving mniste2ekftransformer successfully created\n"
          ]
        }
      ],
      "source": [
        "# Create serving instance\n",
        "SERVING_NAME = \"mniste2ekftransformer\"\n",
        "\n",
        "TRANSFORMER_PATH=hdfs.project_path() + \"/Jupyter/serving/kserve/tensorflow/transformer.py\" # or .ipynb\n",
        "\n",
        "response = serving.create_or_update(SERVING_NAME, # define a name for the serving instance\n",
        "                                    model_path=best_model.model_path, # set the path of the model to be deployed\n",
        "                                    model_server=\"TENSORFLOW_SERVING\", # set the model server to run the model\n",
        "                                    kserve=True, # whether to serve the model using KServe or the default tool in the current Hopsworks version\n",
        "                                    transformer=TRANSFORMER_PATH,\n",
        "                                    transformer_instances=0, # set 0 instances to leverage scale-to-zero capabilities\n",
        "                                    # optional arguments\n",
        "                                    model_version=best_model.version, # set the version of the model to be deployed\n",
        "                                    topic_name=\"CREATE\", # (optional) set the topic name or CREATE to create a new topic for inference logging\n",
        "                                    inference_logging=\"ALL\", # with KServe, select the type of inference data to log into Kafka, e.g MODEL_INPUTS, PREDICTIONS or ALL\n",
        "                                    instances=1, # with KServe, set 0 instances to leverage scale-to-zero capabilities\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mniste2ekftransformer\n"
          ]
        }
      ],
      "source": [
        "# List all available servings in the project\n",
        "for s in serving.get_all():\n",
        "    print(s.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Stopped'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get serving status\n",
        "serving.get_status(SERVING_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classify digits with the MNIST classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start Model Serving Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-28 11:59:36,844 INFO: Serving with name: mniste2ekftransformer successfully started\n"
          ]
        }
      ],
      "source": [
        "if serving.get_status(SERVING_NAME) == 'Stopped':\n",
        "    serving.start(SERVING_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "while serving.get_status(SERVING_NAME) != \"Running\":\n",
        "    time.sleep(5) # Let the serving startup correctly\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Model Serving for active servings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image7-Monitor.png](../../../images/servings.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Send Prediction Requests to the Served Model using Hopsworks REST API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'predictions': [[0.00716477772, 0.0495215505, 0.164436236, 0.0216756575, 0.532064497, 0.00794952642, 0.0278893802, 0.0100125261, 0.142405227, 0.0368806198]]}\n",
            "{'predictions': [[0.00994806644, 0.0678183883, 0.183071345, 0.0769148469, 0.298602432, 0.0145412814, 0.0232542269, 0.0508962944, 0.199954733, 0.0749983266]]}\n",
            "{'predictions': [[0.00678317, 0.042503044, 0.271116346, 0.0244265571, 0.424625099, 0.0157555342, 0.0164663401, 0.0211527571, 0.138280511, 0.0388906188]]}\n",
            "{'predictions': [[0.00414211443, 0.0596462786, 0.30761528, 0.0209895894, 0.361549497, 0.0080305282, 0.0214627199, 0.0147881005, 0.147630453, 0.0541454516]]}\n",
            "{'predictions': [[0.00497094495, 0.0577611215, 0.266127735, 0.0304065645, 0.381860167, 0.0151010472, 0.0085090138, 0.0440005139, 0.166975722, 0.0242872462]]}\n",
            "{'predictions': [[0.0074628829, 0.0546550192, 0.365902811, 0.0349127688, 0.336112082, 0.0129383784, 0.0148731982, 0.0186928585, 0.115760811, 0.0386891328]]}\n",
            "{'predictions': [[0.00787949283, 0.0604840368, 0.221175715, 0.0385403596, 0.30112344, 0.0180348102, 0.0290495325, 0.0352735482, 0.195198312, 0.093240656]]}\n",
            "{'predictions': [[0.00616118405, 0.0766904354, 0.261844963, 0.0246380847, 0.177770182, 0.0271635056, 0.0157324187, 0.0496929437, 0.300589889, 0.0597163513]]}\n",
            "{'predictions': [[0.00731297769, 0.0740799457, 0.158399507, 0.0463014878, 0.373041362, 0.0104803853, 0.0281247012, 0.0214988757, 0.194553837, 0.086206913]]}\n",
            "{'predictions': [[0.0101395193, 0.0717072636, 0.188310683, 0.0325433388, 0.4157314, 0.00923475809, 0.0339613594, 0.0183466114, 0.188901231, 0.0311238244]]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "NUM_FEATURES=784\n",
        "\n",
        "for i in range(10):\n",
        "    data = {\n",
        "                \"signature_name\": \"serving_default\", \"instances\": [np.random.rand(NUM_FEATURES).tolist()]\n",
        "            }\n",
        "    response = serving.make_inference_request(SERVING_NAME, data)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Prediction Requests and Responses using Kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All prediction requestst are automatically logged to Kafka which means that you can keep track for your model's performance and its predictions in a scalable manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from hops import kafka\n",
        "from confluent_kafka import Producer, Consumer, KafkaError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup Kafka consumer and subscribe to the topic containing the prediction logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOPIC_NAME = serving.get_kafka_topic(SERVING_NAME)\n",
        "\n",
        "config = kafka.get_kafka_default_config()\n",
        "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
        "consumer = Consumer(config)\n",
        "topics = [TOPIC_NAME]\n",
        "consumer.subscribe(topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the Kafka Avro schema from Hopsworks and setup an Avro reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_schema = kafka.get_schema(TOPIC_NAME)\n",
        "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read messages from the Kafka topic, parse them with the Avro schema and print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO -> servingId: 35, modelName: mnist_e2e, modelVersion: 1,requestTimestamp: 1643371225, inferenceId:b442de40-9aab-41f5-8a6c-c4d772f75969, messageType:response\n",
            "Predictions -> [[0.00873534381, 0.116867647, 0.129798576, 0.0748127252, 0.333430201, 0.0154738724, 0.0273870789, 0.0286567844, 0.192712128, 0.0721256658]]\n",
            "\n",
            "INFO -> servingId: 35, modelName: mnist_e2e, modelVersion: 1,requestTimestamp: 1643371225, inferenceId:ead02abc-0279-418b-9b20-8d2ae3c7d67b, messageType:response\n",
            "Predictions -> [[0.00662171189, 0.0716219693, 0.147491157, 0.0376859158, 0.398301035, 0.00908162631, 0.0212855339, 0.0298583135, 0.233680561, 0.0443721078]]\n",
            "\n",
            "INFO -> servingId: 35, modelName: mnist_e2e, modelVersion: 1,requestTimestamp: 1643371225, inferenceId:1c27d104-dbc8-4d35-a9af-7a182116a34a, messageType:response\n",
            "Predictions -> [[0.00517375534, 0.105344124, 0.197647452, 0.038458731, 0.244261652, 0.0108315526, 0.0158249736, 0.0569163188, 0.269892216, 0.0556492358]]\n",
            "\n",
            "INFO -> servingId: 35, modelName: mnist_e2e, modelVersion: 1,requestTimestamp: 1643371225, inferenceId:e8f4db55-7d8d-45b9-8307-0a17cf910a6d, messageType:response\n",
            "Predictions -> [[0.00795154274, 0.0799642131, 0.369177878, 0.0543756932, 0.286108434, 0.00967404712, 0.012239052, 0.034396667, 0.104219526, 0.0418928973]]\n",
            "\n",
            "INFO -> servingId: 35, modelName: mnist_e2e, modelVersion: 1,requestTimestamp: 1643371226, inferenceId:a912e712-affb-4dff-9455-ae8f5198e94a, messageType:response\n",
            "Predictions -> [[0.00681294175, 0.0445752069, 0.343478769, 0.0334866494, 0.379422158, 0.0143673914, 0.0213937927, 0.0121406969, 0.102342986, 0.0419792905]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PRINT_INSTANCES=False\n",
        "PRINT_PREDICTIONS=True\n",
        "\n",
        "for i in range(0, 10):\n",
        "    msg = consumer.poll(timeout=5.0)\n",
        "    if msg is not None:\n",
        "        value = msg.value()\n",
        "        try:\n",
        "            event_dict = kafka.parse_avro_msg(value, avro_schema)  \n",
        "            payload = json.loads(event_dict[\"payload\"])\n",
        "            \n",
        "            if (event_dict['messageType'] == \"request\" and not PRINT_INSTANCES) or \\\n",
        "                (event_dict['messageType'] == \"response\" and not PRINT_PREDICTIONS):\n",
        "                continue\n",
        "            \n",
        "            print(\"INFO -> servingId: {}, modelName: {}, modelVersion: {},\"\\\n",
        "                  \"requestTimestamp: {}, inferenceId:{}, messageType:{}\".format(\n",
        "                       event_dict[\"servingId\"],\n",
        "                       event_dict[\"modelName\"],\n",
        "                       event_dict[\"modelVersion\"],\n",
        "                       event_dict[\"requestTimestamp\"],\n",
        "                       event_dict[\"inferenceId\"],\n",
        "                       event_dict[\"messageType\"]))\n",
        "\n",
        "            if event_dict['messageType'] == \"request\":\n",
        "                print(\"Instances -> {}\\n\".format(payload['instances']))\n",
        "                \n",
        "            if event_dict['messageType'] == \"response\":\n",
        "                print(\"Predictions -> {}\\n\".format(payload['predictions']))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"A message was read but there was an error parsing it\")\n",
        "            print(e)\n",
        "    else:\n",
        "        print(\"timeout.. no more messages to read from topic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
