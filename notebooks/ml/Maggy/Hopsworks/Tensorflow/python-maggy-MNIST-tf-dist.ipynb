{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "title: \"Maggy Distributed Training on MNIST Dataset using Python Kernel\"\n",
    "date: 2022-02-16\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maggy enables you to train with Tensorflow distributed optimizers.\n",
    "Using Maggy, you have to make minimal changes in train your model in a distributed fashion. \n",
    "\n",
    "Make sure you have the latest Maggy installation and that you are running this notebook with a Python kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from maggy.experiment import experiment\n",
    "from maggy.experiment_config.tf_distributed import TfDistributedConfig\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model definition\n",
    "\n",
    "Let's define the model we want to train. The layers of the model have to be defined in the \\_\\_init__ function.\n",
    "\n",
    "Do not instantiate the class, otherwise you won't be able to use Maggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, nlayers):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(28, 2, activation='relu')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.d1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.d2 = keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "model = NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset creation\n",
    "\n",
    "You can create the dataset here and pass it to the TfDistributedConfig, or creating it in the training function.\n",
    "\n",
    "In this example, we are downloading the dataset from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.reshape(x_train, (60000, 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the training function\n",
    "\n",
    "The programming model is that you wrap the code containing the model training inside a wrapper function. Inside that wrapper function provide all imports and parts that make up your experiment.\n",
    "\n",
    "The function should return the metric that you want to optimize for. This should coincide with the metric being reported in the Keras callback (see next point).\n",
    "You can return the metric list, in this case only the loss element will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_function(model, train_set, test_set, hparams):\n",
    "    \n",
    "    # Define training parameters\n",
    "    num_epochs = 10\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    criterion = keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate,momentum=0.9,decay=1e-5)\n",
    "    \n",
    "    model = model(nlayers = hparams['nlayers'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=criterion, metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(train_set[0],\n",
    "              train_set[1],\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              )\n",
    "\n",
    "    print(\"Testing\")\n",
    "    \n",
    "    loss = model.evaluate(\n",
    "        test_set[0],\n",
    "        test_set[1],\n",
    "        batch_size=32)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configuring the experiment\n",
    "\n",
    "In order to use maggy distributed training, we have to configure the training model, we can pass it to TfDistributedConfig.\n",
    "the model class has to be an implementation of __tf.keras.Model__.\n",
    "We can also define __train_set__, __test_set__ and eventually the __model_parameters__. __model_parameters__ is a dictionary\n",
    "containing the parameters to be used in the \\_\\_init__ function of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the constructor parameters of your model\n",
    "model_parameters = {\n",
    "    'nlayers': 2\n",
    "}\n",
    "\n",
    "#pass the model parameters in the last \n",
    "config = TfDistributedConfig(name=\"tf_test\", \n",
    "                             model=model, \n",
    "                             train_set=(x_train, y_train), \n",
    "                             test_set=(x_test, y_test),\n",
    "                             hparams=model_parameters\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Run distributed training\n",
    "\n",
    "Finally, we are ready to launch the maggy experiment. You just need to pass 2 parameters: the training function and the configuration variable we defined in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-16 10:52:47,768 INFO: Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "2022-02-16 10:52:47,769 INFO: Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n",
      "2022-02-16T10:52:47.771477 (0/0): train_set or test_set is null, data has not been sharded. \n",
      "\n",
      "2022-02-16T10:52:47.771691 (0/0): index of slice 0 \n",
      "\n",
      "2022-02-16T10:52:47.771738 (0/0): Starting distributed training. \n",
      "\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 13s 43ms/step - loss: 0.7543 - accuracy: 0.7819\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.1528 - accuracy: 0.9553\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 10s 41ms/step - loss: 0.0815 - accuracy: 0.9762\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0488 - accuracy: 0.9855\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0378 - accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0263 - accuracy: 0.9918\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0177 - accuracy: 0.9946\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 10s 43ms/step - loss: 0.0147 - accuracy: 0.9953\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 10s 42ms/step - loss: 0.0052 - accuracy: 0.9989\n",
      "Testing\n",
      "313/313 [==============================] - 3s 6ms/step - loss: 0.0926 - accuracy: 0.9772\n",
      "2022-02-16T10:54:35.439119 (0/0): Finished distributed training. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(training_function, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.09255693107843399\n"
     ]
    }
   ],
   "source": [
    "print('loss: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
