{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"Maggy Distributed Hyper Parameter Optimization with Tensorflow example\"\n",
    "date: 2021-05-03\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "99cf3ed8-9d48-4c11-9692-0ccb78e4aa75"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## maggy on Databricks - MNIST Example\n",
    "---\n",
    "Created: 01/02/2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook illustrates the usage of the maggy framework for asynchronous hyperparameter optimization on the famous MNIST dataset.  \n",
    "\n",
    "In this specific example we are using random search over three parameters and we are deploying the median early stopping rule in order to make use of the asynchrony of the framework. The Median Stopping Rule implements the simple strategy of stopping a trial if its performance falls below the median of other trials at similar points in time.\n",
    "\n",
    "We are using Keras for this example. This notebook works with any Spark cluster given that you are using maggy 0.5.\n",
    "\n",
    "This notebook has been tested with the Databricks Runtime Version 7.6 ML, TensorFlow 2.3.1 and Spark 3.0.1.  \n",
    "Requires Python 3.6 or higher."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3bb5751f-c175-4716-b4dd-fd476e6e1093"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Spark Session"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "86f94a8f-69bf-4f6a-a8ea-82a5842234c2"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you have a running Spark Session/Context available."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "0919e983-5b33-4f64-975c-ac6da66c7a63"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark\n",
    "pyspark.sql.SparkSession.getActiveSession()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "e3747740-5699-4ea0-8bd6-25fb05613d92"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\">Out[11]: </div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: </div>"
      ]
     }
    },
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3548663097321679#setting/sparkui/0121-095636-gaudy326/driver-1745804670923416892\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.5:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "textData": null,
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "htmlSandbox",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3548663097321679#setting/sparkui/0121-095636-gaudy326/driver-1745804670923416892\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.5:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you have the right tensorflow version."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "90ae9f68-dabd-43e3-afc8-a803d7fd237b"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__ != '2.3.1':\n",
    "  !pip install tensorflow==2.3.1"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3a0d1640-3a83-46b1-8f19-6aadb12a55e1"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Searchspace definition"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "e9be5e0c-3bf7-4091-bfac-ba6f6fb3f57d"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to conduct random search for the MNIST example on three hyperparameters: Kernel size, pooling size and dropout rate. Hence, we have two continuous integer valued parameters and one double valued parameter."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "2fa0c2a3-50ac-49b2-aca6-5b5de6ad0508"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from maggy import Searchspace\n",
    "\n",
    "# The searchspace can be instantiated with parameters\n",
    "sp = Searchspace(kernel=('INTEGER', [2, 8]), pool=('INTEGER', [2, 8]))\n",
    "\n",
    "# Or additional parameters can be added one by one\n",
    "sp.add('dropout', ('DOUBLE', [0.01, 0.99]))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "8d38c862-928d-436d-8518-b11baabc274d"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\">Hyperparameter added: kernel\nHyperparameter added: pool\nHyperparameter added: dropout\n</div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hyperparameter added: kernel\nHyperparameter added: pool\nHyperparameter added: dropout\n</div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Model training definition\n",
    "\n",
    "The programming model is that you wrap the code containing the model training inside a wrapper function. Inside that wrapper function provide all imports and parts that make up your experiment.\n",
    "\n",
    "There are several requirements for this wrapper function:\n",
    "\n",
    "1. The function should take the hyperparameters as arguments, plus one additional parameter `reporter` which is needed for reporting the current metric to the experiment driver.\n",
    "2. The function should return the metric that you want to optimize for. This should coincide with the metric being reported in the Keras callback (see next point).\n",
    "3. In order to leverage on the early stopping capabilities of maggy, you need to make use of the maggy reporter API. By including the reporter in your training loop, you are telling maggy which metric to report back to the experiment driver for optimization and to check for early stopping. It is as easy as adding `reporter.broadcast(metric=YOUR_METRIC)` for example at the end of your epoch or batch training step and adding a `reporter` argument to your function signature. If you are not writing your own training loop you can use the pre-written Keras callbacks:\n",
    "    - KerasBatchEnd\n",
    "    - KerasEpochEnd  \n",
    "(Please see documentation for a detailed explanation.)\n",
    "\n",
    "We are going to use the `KerasBatchEnd` callback to report back the accuracy after each batch. However, note that in the BatchEnd callback we have only access to training accuracy since validation after each batch would be too expensive."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "f669eae9-bbfc-4f44-ab54-6ed9d003fc21"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from maggy import experiment\n",
    "from maggy.callbacks import KerasBatchEnd"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "7731489c-52ea-476b-82f0-65a587f9fe2e"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the training wrapper function:\n",
    "(maggy specific parts are highlighted with comments and correspond to the three points described above.)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "c05baa3d-1b80-4fbd-9c9a-a4df5d2d4cf0"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#########\n",
    "### maggy: hyperparameters as arguments and including the reporter\n",
    "#########\n",
    "def training_function(kernel, pool, dropout, reporter):\n",
    "    import numpy as np\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.python.keras.datasets import mnist\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    \n",
    "    from maggy import tensorboard\n",
    "    \n",
    "\n",
    "    log_dir = tensorboard.logdir()\n",
    "    batch_size = 512\n",
    "    num_classes = 10\n",
    "    epochs = 1\n",
    "\n",
    "    # Input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    \n",
    "    train_filenames = [\"TourData/mnist/train/train.tfrecords\"]\n",
    "    validation_filenames = [\"TourData/mnist/validation/validation.tfrecords\"]\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    print(\"x_train shape:\", x_train.shape)\n",
    "    print(x_train.shape[0], \"train samples\")\n",
    "    print(x_test.shape[0], \"test samples\")\n",
    "    num_train = x_train.shape[0]\n",
    "    num_val = x_test.shape[0]\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # Create an iterator over the dataset\n",
    "    def data_input(filenames, batch_size=128, shuffle=False, repeat=None):\n",
    "\n",
    "        def parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.io.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.io.decode_raw(features['image_raw'], tf.uint8)\n",
    "            image.set_shape([28 * 28])\n",
    "\n",
    "            # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "            image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "            # Reshape the tensor\n",
    "            image = tf.reshape(image, [img_rows, img_cols, 1])\n",
    "    \n",
    "            # Create a one hot array for your labels\n",
    "            label = tf.one_hot(label, num_classes)\n",
    "            \n",
    "            return image, label\n",
    "\n",
    "        # Import MNIST data\n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        num_samples = sum(1 for _ in dataset)\n",
    "\n",
    "        # Map the parser over dataset, and batch results by up to batch_size\n",
    "        dataset = dataset.map(parser)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=128)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.repeat(repeat)\n",
    "        return dataset, num_samples\n",
    "    \n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(kernel, kernel),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (kernel, kernel), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(pool, pool)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(1.0)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Setup TensorBoard\n",
    "    tb_callback = TensorBoard(        \n",
    "        log_dir,\n",
    "        update_freq='batch',\n",
    "        profile_batch=0,  # workaround for issue #2084\n",
    "    )\n",
    "    \n",
    "    #########\n",
    "    ### maggy: REPORTER API through keras callback\n",
    "    #########\n",
    "    callbacks = [KerasBatchEnd(reporter, metric='accuracy'), tb_callback]\n",
    "    \n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              steps_per_epoch = num_train//batch_size,\n",
    "              callbacks=callbacks, # add callback\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test,y_test),\n",
    "              validation_steps=num_val//batch_size)\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, steps=num_val//batch_size, verbose=1)\n",
    "    \n",
    "    # Using print in the wrapper function will print underneath the Jupyter Cell with a \n",
    "    # prefix to indicate which prints come from the same executor\n",
    "    \n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    #########\n",
    "    ### maggy: return the metric to be optimized, test accuracy in this case\n",
    "    #########\n",
    "    return score[1]"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b4e4498b-31fd-46ef-9260-03f709c72ba2"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\"></div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Launching the experiment\n",
    "\n",
    "Finally, we are ready to launch the maggy experiment.\n",
    "There are a variety of parameters to specify, some of which are optional:\n",
    "1. `train_fn`: your previously specified training wrapper function\n",
    "2. `searchspace`: the searchspace object\n",
    "3. `optimizer`: the optimization algorithm to be used (only 'randomsearch' available at the moment)\n",
    "4. `direction`: maximize or minimize the specified metric\n",
    "5. `num_trials`: number of different parameter combinations to be evaluated\n",
    "6. `name`: an experiment name\n",
    "7. `hb_interval`: Time in seconds between the heartbeat messages with the metric to the experiment driver. A sensible value is not much smaller than the frequency in which your training loop updates the metric. So using the KerasBatchEnd reporter callback, it does not make sense having a much smaller interval than the amount of time a batch takes to be processed.\n",
    "8. `es_interval`: Interval in seconds, specifying how often the currently running trials should be checked for early stopping. Should be bigger than the `hb_interval`.\n",
    "9. `es_min`: Minimum number of trials to be finished before starting to check for early stopping. For example, the median stopping rule implements the simple strategy of stopping a trial if its performance falls below the median of finished trials at similar points in time. We only want to start comparing to the median once there are several trials finished."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "998ff93e-b819-4666-b771-2dd71c7f97ed"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from maggy.experiment_config import OptimizationConfig\n",
    "\n",
    "config = OptimizationConfig(num_trials=4,\n",
    "                            optimizer=\"randomsearch\",\n",
    "                            searchspace=sp,\n",
    "                            direction=\"max\",\n",
    "                            es_interval=1,\n",
    "                            es_min=5,\n",
    "                            name=\"hp_tuning_test\"\n",
    "                            )\n",
    "\n",
    "result = experiment.lagom(train_fn=training_function, config=config)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "f507ee6e-2505-4486-b96a-47e69cc64a71"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\">You are running Maggy on Databricks.\nStarted Maggy Experiment: MNIST, app-20210225103041-0000, run 1\n\n------ RandomSearch Results ------ direction(max) \nBEST combination {&#34;kernel&#34;: 7, &#34;pool&#34;: 4, &#34;dropout&#34;: 0.07650065874317517} -- metric 0.9725000262260437\nWORST combination {&#34;kernel&#34;: 8, &#34;pool&#34;: 7, &#34;dropout&#34;: 0.033706837495202924} -- metric 0.9567999839782715\nAVERAGE metric -- 0.9657133340835571\nEARLY STOPPED Trials -- 0\nTotal job time 0 hours, 10 minutes, 9 seconds\n\nFinished Experiment\n</div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">You are running Maggy on Databricks.\nStarted Maggy Experiment: MNIST, app-20210225103041-0000, run 1\n\n------ RandomSearch Results ------ direction(max) \nBEST combination {&#34;kernel&#34;: 7, &#34;pool&#34;: 4, &#34;dropout&#34;: 0.07650065874317517} -- metric 0.9725000262260437\nWORST combination {&#34;kernel&#34;: 8, &#34;pool&#34;: 7, &#34;dropout&#34;: 0.033706837495202924} -- metric 0.9567999839782715\nAVERAGE metric -- 0.9657133340835571\nEARLY STOPPED Trials -- 0\nTotal job time 0 hours, 10 minutes, 9 seconds\n\nFinished Experiment\n</div>"
      ]
     }
    }
   ],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "To observe the progress, you can check the sterr of the spark executors."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5f8d2411-c5a7-4877-ab74-7863b1c77754"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each experiment will create a folder at the path '/dbfs/maggy_log/'."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "34ae1276-4154-492c-8f64-49a0b4094e63"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os \nos.listdir('/dbfs/maggy_log')"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "60c74082-bbe8-4cda-a368-9f7c1b5d22f5"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "datasetInfos": [],
       "data": "<div class=\"ansiout\">Out[13]: [&#39;app-20210223105721-0000&#39;,\n &#39;app-20210224090409-0000&#39;,\n &#39;app-20210224090834-0000&#39;,\n &#39;app-20210224112520-0000&#39;,\n &#39;app-20210225103041-0000&#39;]</div>",
       "removedWidgets": [],
       "addedWidgets": {},
       "type": "html",
       "arguments": {}
      }
     },
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [&#39;app-20210223105721-0000&#39;,\n &#39;app-20210224090409-0000&#39;,\n &#39;app-20210224090834-0000&#39;,\n &#39;app-20210224112520-0000&#39;,\n &#39;app-20210225103041-0000&#39;]</div>"
      ]
     }
    }
   ],
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "maggy-mnist-example (1)",
   "dashboards": [],
   "language": "python",
   "widgets": {},
   "notebookOrigID": 1910943470755582
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}