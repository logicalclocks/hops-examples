{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Databricks Feature AWS Store Quickstart\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "73d411fe-e2ad-4cf6-a949-8b1408087f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks AWS Feature Store Quick Start\n",
    "\n",
    "This notebook gives you a quick overview of how you can intergrate the Feature Store on Hopsworks with Databricks and S3. We'll go over four steps:\n",
    "\n",
    "1. Generate some sample data and store it on S3\n",
    "2. Do some feature engineering with Databricks and the data from S3\n",
    "3. **Save the engineered features to the Feature Store**\n",
    "4. **Select a group of the features from the Feature Store and create a training dataset of tf records stored on S3**\n",
    "\n",
    "**This requries configuring the Databricks cluster to be able to interact with Hopsworks Feature Store, see [Databricks Quick Start](https://docs.hopsworks.ai/feature-store-api/latest/integrations/databricks/configuration/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a81aaf2-11ef-4045-bc9d-334f8364eb8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports\n",
    "\n",
    "We'll use numpy and pandas for data generation, pyspark for feature engineering, tensorflow and keras for model training, and the `hsfs` library to interact with the Hopsworks Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1fd8b152-3fa8-4fc5-9c9e-58f33234ec03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hsfs\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c738692-4859-4a9e-92ad-f5e5b86ee7ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connecting to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4447e6c9-1f34-493c-9b56-935b0d931cd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to the feature store, see https://docs.hopsworks.ai/feature-store-api/latest/generated/api/connection_api/#connection_1 for more information\n",
    "# On AWS secrets can be stored also on the Secrets Manager or Parameter Store.\n",
    "connection = hsfs.connection(\n",
    "  host=\"10.0.0.247\",\n",
    "  project=\"dataai\",\n",
    "  port=\"443\",\n",
    "  api_key_value=\"pduNksE2VMuSYdCY.K4AsDoxMBX5luisgb3pB7FimpEO7iyOuYvLPqWQcnXUs51RlBrLyAYXpKDoWH9Cm\",\n",
    "  hostname_verification=False\n",
    ")\n",
    "\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00b67492-299b-4a8e-bc4e-5caa453e460d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mounting an S3 bucket to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f212fc99-c4ce-46ff-b825-164690407187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount a bucket so that we can simulate a Datalake based on S3\n",
    "# This requires IAM roles to be set up for Databricks, see https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#access-s3-buckets-directly\n",
    "AWS_BUCKET_NAME = \"steffendatabricks\" # Ensure to replace with your bucket\n",
    "MOUNT_NAME = \"/mnt/demo_db_hsfs\"\n",
    "dbutils.fs.mount(\"s3a://%s\" % AWS_BUCKET_NAME, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe1db1c6-0dc7-4c0c-9af0-1fb2ef9fd6e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "Lets generate two sample datasets and store them on S3:\n",
    "\n",
    "1. `houses_for_sale_data`:\n",
    "\n",
    "```bash\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "|area_id|house_id|       house_worth|         house_age|        house_size|\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "|      1|       0| 11678.15482418699|133.88670106643886|366.80067322738535|\n",
    "|      1|       1| 2290.436167500643|15994.969706808222|195.84014889823976|\n",
    "|      1|       2| 8380.774578431328|1994.8576926471007|1544.5164614303735|\n",
    "|      1|       3|11641.224696102923|23104.501275562343|1673.7222604337876|\n",
    "|      1|       4| 5382.089422436954| 13903.43637058141| 274.2912104765028|\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- house_id: long (nullable = true)\n",
    " |-- house_worth: double (nullable = true)\n",
    " |-- house_age: double (nullable = true)\n",
    " |-- house_size: double (nullable = true)\n",
    "```\n",
    "2. `houses_sold_data``\n",
    "```bash\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "|      1|                0|                0| 70073.06059070028|\n",
    "|      1|                1|               15| 146.9198329740602|\n",
    "|      1|                2|                6|  594.802165433149|\n",
    "|      1|                3|               10| 77187.84123130841|\n",
    "|      1|                4|                1|110627.48922722359|\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- house_purchase_id: long (nullable = true)\n",
    " |-- number_of_bidders: long (nullable = true)\n",
    " |-- sold_for_amount: double (nullable = true)\n",
    "```\n",
    "\n",
    "We'll use this data for predicting what a house is sold for based on features about the **area** where the house is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a6c83c3-eaa6-4d95-bc58-4f40db4ee42a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generation of `houses_for_sale_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f17b70c-f9ff-4830-a142-6ac2071abdfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "area_ids = list(range(1,51))\n",
    "house_sizes = []\n",
    "house_worths = []\n",
    "house_ages = []\n",
    "house_area_ids = []\n",
    "for i in area_ids:\n",
    "    for j in list(range(1,100)):\n",
    "        house_sizes.append(abs(np.random.normal()*1000)/i)\n",
    "        house_worths.append(abs(np.random.normal()*10000)/i)\n",
    "        house_ages.append(abs(np.random.normal()*10000)/i)\n",
    "        house_area_ids.append(i)\n",
    "house_ids = list(range(len(house_area_ids)))\n",
    "houses_for_sale_data  = pd.DataFrame({\n",
    "        'area_id':house_area_ids,\n",
    "        'house_id':house_ids,\n",
    "        'house_worth': house_worths,\n",
    "        'house_age': house_ages,\n",
    "        'house_size': house_sizes\n",
    "    })\n",
    "houses_for_sale_data_spark_df = sqlContext.createDataFrame(houses_for_sale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "252379e7-d04b-4656-b678-0be33845dddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_for_sale_data_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61bb85ab-bcb0-44e7-837c-fab37f54a9af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_for_sale_data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8218315-20d2-45c0-bc3e-948164e264e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_for_sale_data_spark_df.write.format(\"parquet\").save(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2e5b048-0cab-4878-b795-4c3f726d03a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generation of `houses_sold_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa9037dc-cf72-4e19-bea3-b0a1c8c2590e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "house_purchased_amounts = []\n",
    "house_purchases_bidders = []\n",
    "house_purchases_area_ids = []\n",
    "for i in area_ids:\n",
    "    for j in list(range(1,1000)):\n",
    "        house_purchased_amounts.append(abs(np.random.exponential()*100000)/i)\n",
    "        house_purchases_bidders.append(int(abs(np.random.exponential()*10)/i))\n",
    "        house_purchases_area_ids.append(i)\n",
    "house_purchase_ids = list(range(len(house_purchases_bidders)))\n",
    "houses_sold_data  = pd.DataFrame({\n",
    "        'area_id':house_purchases_area_ids,\n",
    "        'house_purchase_id':house_purchase_ids,\n",
    "        'number_of_bidders': house_purchases_bidders,\n",
    "        'sold_for_amount': house_purchased_amounts\n",
    "    })\n",
    "houses_sold_data_spark_df = sqlContext.createDataFrame(houses_sold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "783a88b9-a1cd-46e7-a248-9ce7e6b20374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_sold_data_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a59a4790-6d7f-4f17-8e54-6585b1dcde5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_sold_data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66ceeac4-4a83-42b8-8f90-d3e14b5abc38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_sold_data_spark_df.write.format(\"parquet\").save(\"%s/houses_sold.parquet\" % MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4cddb161-444f-4d8a-a87f-18227353f128",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Lets generate some aggregate features such as sum and averages from our datasets on S3. \n",
    "\n",
    "1. `houses_for_sale_features`:\n",
    "\n",
    "```bash\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- avg_house_age: double (nullable = true)\n",
    " |-- avg_house_size: double (nullable = true)\n",
    " |-- avg_house_worth: double (nullable = true)\n",
    " |-- sum_house_age: double (nullable = true)\n",
    " |-- sum_house_size: double (nullable = true)\n",
    " |-- sum_house_worth: double (nullable = true)\n",
    "```\n",
    "\n",
    "2. `houses_sold_features`\n",
    "\n",
    "```bash\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- avg_num_bidders: double (nullable = true)\n",
    " |-- avg_sold_for: double (nullable = true)\n",
    " |-- sum_number_of_bidders: long (nullable = true)\n",
    " |-- sum_sold_for_amount: double (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "720d1cbe-f285-4aa5-af5d-68a7c861455e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(MOUNT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77dc56fd-9acc-407d-938b-e7f91f723770",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Features From `houses_for_sale_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d54c292-d5d1-49b7-8f75-c765c289cced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_for_sale_data_spark_df = spark.read.parquet(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)\n",
    "sum_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").sum()\n",
    "count_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").count()\n",
    "sum_count_houses_for_sale_df = sum_houses_for_sale_df.join(count_houses_for_sale_df, \"area_id\")\n",
    "sum_count_houses_for_sale_df = sum_count_houses_for_sale_df \\\n",
    "    .withColumnRenamed(\"sum(house_age)\", \"sum_house_age\") \\\n",
    "    .withColumnRenamed(\"sum(house_worth)\", \"sum_house_worth\") \\\n",
    "    .withColumnRenamed(\"sum(house_size)\", \"sum_house_size\") \\\n",
    "    .withColumnRenamed(\"count\", \"num_rows\")\n",
    "def compute_average_features_house_for_sale(row):\n",
    "    avg_house_worth = row.sum_house_worth/float(row.num_rows)\n",
    "    avg_house_size = row.sum_house_size/float(row.num_rows)\n",
    "    avg_house_age = row.sum_house_age/float(row.num_rows)\n",
    "    return Row(\n",
    "        sum_house_worth=row.sum_house_worth, \n",
    "        sum_house_age=row.sum_house_age,\n",
    "        sum_house_size=row.sum_house_size,\n",
    "        area_id = row.area_id,\n",
    "        avg_house_worth = avg_house_worth,\n",
    "        avg_house_size = avg_house_size,\n",
    "        avg_house_age = avg_house_age\n",
    "       )\n",
    "houses_for_sale_features_df = sum_count_houses_for_sale_df.rdd.map(\n",
    "    lambda row: compute_average_features_house_for_sale(row)\n",
    ").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21d1ff78-dddf-4609-a4dd-162b366cb8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_for_sale_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b6011cd-de2a-4767-a607-000a9bd3cfa0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Features from `houses_sold_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61488a5a-b97a-43dd-bdac-2ba51b71fa77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_sold_data_spark_df = spark.read.parquet(\"%s/houses_sold.parquet\" % MOUNT_NAME)\n",
    "sum_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").sum()\n",
    "count_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").count()\n",
    "sum_count_houses_sold_df = sum_houses_sold_df.join(count_houses_sold_df, \"area_id\")\n",
    "sum_count_houses_sold_df = sum_count_houses_sold_df \\\n",
    "    .withColumnRenamed(\"sum(number_of_bidders)\", \"sum_number_of_bidders\") \\\n",
    "    .withColumnRenamed(\"sum(sold_for_amount)\", \"sum_sold_for_amount\") \\\n",
    "    .withColumnRenamed(\"count\", \"num_rows\")\n",
    "def compute_average_features_houses_sold(row):\n",
    "    avg_num_bidders = row.sum_number_of_bidders/float(row.num_rows)\n",
    "    avg_sold_for = row.sum_sold_for_amount/float(row.num_rows)\n",
    "    return Row(\n",
    "        sum_number_of_bidders=row.sum_number_of_bidders, \n",
    "        sum_sold_for_amount=row.sum_sold_for_amount,\n",
    "        area_id = row.area_id,\n",
    "        avg_num_bidders = avg_num_bidders,\n",
    "        avg_sold_for = avg_sold_for\n",
    "       )\n",
    "houses_sold_features_df = sum_count_houses_sold_df.rdd.map(\n",
    "    lambda row: compute_average_features_houses_sold(row)\n",
    ").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01f05441-7ae6-48ce-a6dc-717e7209969b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "houses_sold_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20c108b3-d7a4-4c1d-8a20-27ddd3ccb9ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save Features to the Feature Store\n",
    "\n",
    "The Featue store has an abstraction of a **feature group** which is a set of features that naturally belong together that are computed using the same feature engineering job.\n",
    "\n",
    "Lets create two feature groups:\n",
    "\n",
    "1. `houses_for_sale_featuregroup`\n",
    "\n",
    "2. `houses_sold_featuregroup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "376d4287-78fa-44c3-bb11-49218919670b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refer to https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group for the different parameters for creating a feature group\n",
    "house_sale_fg = fs.create_feature_group(\"houses_for_sale_featuregroup\",\n",
    "                       version=1,\n",
    "                       description=\"aggregate features of houses for sale per area\",\n",
    "                       primary_key=['area_id'],\n",
    "                       online_enabled=False,\n",
    "                       time_travel_format=None,\n",
    "                       statistics_config={\"histograms\": True, \"correlations\": True, \"exact_uniqueness\": True})\n",
    "\n",
    "house_sale_fg.save(houses_for_sale_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1bd059a-5427-45b6-95cf-a113c79e3d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "house_sold_fg = fs.create_feature_group(\"houses_sold_featuregroup\",\n",
    "                       version=1,\n",
    "                       description=\"aggregate features of sold houses per area\",\n",
    "                       primary_key=['area_id'],\n",
    "                       online_enabled=False,\n",
    "                       time_travel_format=None,\n",
    "                       statistics_config={\"histograms\": True, \"correlations\": True, \"exact_uniqueness\": True})\n",
    "\n",
    "house_sold_fg.save(houses_sold_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92c94a2a-34b9-4dc1-8397-125a60d3a07e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a Training Dataset\n",
    "\n",
    "The feature store has an abstraction of a **training dataset**, which is a dataset with a set of features (potentially from many different feature groups) and labels (in case of supervised learning) stored in a ML Framework friendly format (CSV, Tfrecords, ...)\n",
    "\n",
    "Let's create a training dataset called *predict_house_sold_for_dataset* using the following features:\n",
    "\n",
    "- `avg_house_age`\n",
    "- `avg_house_size`\n",
    "- `avg_house_worth`\n",
    "- `avg_num_bidders`\n",
    "\n",
    "and the target variable is:\n",
    "\n",
    "- `avg_sold_for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1642693c-acaf-410e-a3e6-91eb3ce40f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the metadata for the storage connector from the Feature store.\n",
    "s3_bucket = fs.get_storage_connector(\"house_model_data\", \"S3\")\n",
    "\n",
    "# Join features and feature groups to create the training dataset\n",
    "feature_query = house_sale_fg.select([\"avg_house_age\", \"avg_house_size\", \"avg_house_worth\"])\\\n",
    "                            .join(house_sold_fg.select([\"avg_num_bidders\", \"avg_sold_for\"]))\n",
    "  \n",
    "# Create the training dataset metadata\n",
    "td = fs.create_training_dataset(name=\"predict_house_sold_for_dataset_two\",\n",
    "                           version=2,\n",
    "                           data_format=\"csv\",\n",
    "                           label=['avg_sold_for'],\n",
    "                           storage_connector=s3_bucket,\n",
    "                           statistics_config={\"histograms\": True, \"correlations\": True, \"exact_uniqueness\": True})\n",
    "\n",
    "# Save the training dataset\n",
    "td.save(feature_query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "FeatureStoreQuickStartDatabricks",
   "notebookOrigID": 37084615428578,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}