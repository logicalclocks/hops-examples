{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Feature Ingestion from S3\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Telecom Churn data from a S3 bucket to the Feature Store\n",
    "\n",
    "First, download this sample data from [here](./data/telco_customer_churn.csv) - and upload it into a S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You first need an IAM Role\n",
    "You will need an IAM role to be able to read data from a S3 bucket. In Hopsworks, there are two ways of assuming an IAM role for the notebooks/jobs that you run in Hopsworks:\n",
    "1. you can assign an *Instance Profile* to the Hopsworks cluster when you create it and all users share its IAM Role, and\n",
    "2. you can assign multiple IAM Roles to a Hopsworks Cluster, and then decide which Projects and its users can assume which IAM Role.\n",
    "\n",
    "\n",
    "## Cluster-wide IAM Role\n",
    "On hopsworks.ai, when you are configuring your Hopsworks cluster, you can select an Instance profile for Hopsworks - see the screenshot below.. All jobs run on Hopsworks can use the IAM Role for this Instance profile (the Instance profile is an IAM Role for this instance). That is, all Hopsworks users share the Instance Profile role and the resource access policies attached to that role.\n",
    "\n",
    "<img src=\"images/iam-profile.png\" alt=\"Cluster-wide IAM Profile\" style=\"margin: auto; height: 450px; width:550px;\"/>\n",
    "\n",
    "## Federated IAM Roles (Role Chaining)\n",
    "\n",
    "You can restrict a IAM Roles to be only usable within a specified project. Within the specified project, you can furuther retrict which *role* a user must have to be able to use the IAM Role - e.g., only *Data Owners* in the project called *Noc-list* can use this assume IAM role. See details on [how to setup multiple IAM Roles (Role Chaining) in our documentation](https://hopsworks.readthedocs.io/en/latest/admin_guide/cloud_role_mapping.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create S3 Storage Connector to your Bucket\n",
    "\n",
    "You should also create a S3 storage connector pointing to the bucket where you uploaded the data. You can follow the [Storage Connectors documentation](https://docs.hopsworks.ai/latest/setup/) to see how you can create the storage connector from the feature store UI. If you have assigned a *cluster-wide IAM Role*, you will not need to specify the IAM role to be used. If you are using Federated IAM Roles, and you have permissions to assume one or IAM Roles in the current project, then you select the IAM Role to use to connect to the S3 bucket from the drop-down list (\"No IAM role defined\"), as shown in the screenshot below. It is also possible to create a S3 Storage Connector using an Access Key and Secret Key, although IAM Roles are the preferred authentication method.\n",
    "\\\n",
    "\n",
    "<img src=\"images/s3-connector.png\" alt=\"S3 connector\" style=\"margin: auto; height: 450px; width:750px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "connection = hsfs.connection()\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct Spark to read from S3 we build the path to the file in the bucket. \\\n",
    "Note the file system - `s3a://` \\\n",
    "Note, PySpark reads all columns as a string (StringType) by default - `inferSchema=True` tries to infer the column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : sacramento_houses_raw, version: 1\n",
      "computing feature correlation for: sacramento_houses_raw, version: 1\n",
      "computing feature histograms for: sacramento_houses_raw, version: 1\n",
      "computing cluster analysis for: sacramento_houses_raw, version: 1\n",
      "Registering feature metadata...\n",
      "Registering feature metadata... [COMPLETE]\n",
      "Writing feature data to offline feature group (Hive)...\n",
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "Writing feature data to offline feature group (Hive)... [COMPLETE]\n",
      "Feature group created successfully\n",
      "Feature group imported successfully"
     ]
    }
   ],
   "source": [
    "sc = fs.get_storage_connector(\"telco_delta\")\n",
    "\n",
    "df = spark.read.csv(\"s3a://\" + sc.bucket + \"/telco-delta\", header=True, inferSchema=True) \n",
    "\n",
    "telco_fg = fs.create_feature_group(name=\"telco_fg\",\n",
    "                                   version=1,\n",
    "                                   description=\"On-demand FG with telecom data\",\n",
    "                                   primary_key=[\"customer_id\"],\n",
    "                                   time_travel_format=None,\n",
    "                                   statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "telco_fg.save(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature store UI you should now be able to see that the feature group has been created, browse its schema and statistics. You can now use it to build training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also try and add a schema to the dataframe.\n",
    "# schema = StructType() \\\n",
    "#       .add(\"customer_id\",StringType(),True) \\\n",
    "#       .add(\"gender\",StringType(),True) \\  # BooleanType\n",
    "#       .add(\"senior_citizen\",StringType(),True) \\\n",
    "#       .add(\"partner\",StringType(),True) \\\n",
    "#       .add(\"dependents\",StringType(),True) \\\n",
    "#       .add(\"tenure\",IntegerType(),True) \\\n",
    "#       .add(\"phone_service\",StringType(),True) \\\n",
    "#       .add(\"multiple_lines\",StringType(),True) \\\n",
    "#       .add(\"internet_service\",StringType(),True) \\\n",
    "#       .add(\"online_security\",StringType(),True) \\\n",
    "#       .add(\"online_backup\",StringType(),True) \\\n",
    "#       .add(\"device_protection\",StringType(),True) \\\n",
    "#       .add(\"tech_support\",StringType(),True) \\\n",
    "#       .add(\"streaming_tv\",StringType(),True) \\\n",
    "#       .add(\"streaming_movies\",StringType(),True) \\\n",
    "#       .add(\"contract\",StringType(),True) \\\n",
    "#       .add(\"paperless_billing\",StringType(),True) \\\n",
    "#       .add(\"payment_method\",DoubleType(),True) \\\n",
    "#       .add(\"monthly_charges\",StringType(),True) \\\n",
    "#       .add(\"total_charges\",DoubleType(),True) \\\n",
    "#       .add(\"churn\",DoubleType(),True) \n",
    "# Add: spark.read.csv(...., schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
