{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c7cb4d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Colab Hopsworks Feature Store Tour\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-deviation",
   "metadata": {},
   "source": [
    "## Prerequistes\n",
    "\n",
    "### Step 1: Register an account on [hopsworks.ai](https://hopsworks.ai)\n",
    "Click on the \"Demo\" button to access a demo cluster. \n",
    "Copy the URL to the cluster in the form \"[UUID].cloud.hopsworks.ai\". You will need it to connect to Hopsworks later.\n",
    "\n",
    "### Step 2.  Open the Demo Cluster and run the \"Feature Store Tour\"\n",
    "Note the \"project-name\" that is created when you run the Feature Store Tour. You will need it to connect to Hopsworks later.\n",
    "\n",
    "### Step 3: Configure a Hopsworks API Key\n",
    "You need to set up a Feature Store API key for authentication.\n",
    "In Hopsworks, click on your username in the top-right corner \n",
    "(1) and select Settings to open the user settings. Select API keys. \n",
    "(2) Give the key a name and select the job, featurestore, dataset.create, kafka and project scopes before \n",
    "(3) creating the key. \n",
    "\n",
    "Copy the key into your clipboard for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling hsfs-2.2.15:\n",
      "  Successfully uninstalled hsfs-2.2.15\n",
      "Collecting hsfs[hive]\n",
      "Collecting avro==1.10.1 (from hsfs[hive])\n",
      "Collecting sqlalchemy (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/72/0c/abd3bd19298cd3fc0a6f2f0ac05c369e7272472f578397043929ed743c79/SQLAlchemy-1.4.17-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting boto3 (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/11/20/4294e37c3c6936c905f1e9da958c776d7fee54a4512bdb7706d69c8720e6/boto3-1.17.84-py2.py3-none-any.whl\n",
      "Collecting furl (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/12/18/b29367947b32b510cbbbfa86164929ceed069ff020f84a6dc780df5d6ba1/furl-2.1.2-py2.py3-none-any.whl\n",
      "Collecting pyhumps==1.6.1 (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/8b/5e/d075fb7d93d757da5601d55188bde9869a9a6b59b1fc8d7fb0fdce7714a2/pyhumps-1.6.1-py3-none-any.whl\n",
      "Collecting mock (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
      "Collecting pandas (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting requests (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\n",
      "Collecting pyjks (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/28/46/e1f2c679d9c6c76a5fda4eb99922a51141803089bf58e77c74a1fe6f9033/pyjks-20.0.0-py2.py3-none-any.whl\n",
      "Collecting PyMySQL (from hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/52/a115fe175028b058df353c5a3d5290b71514a83f67078a6482cff24d6137/PyMySQL-1.0.2-py3-none-any.whl\n",
      "Collecting pyhopshive[thrift]; extra == \"hive\" (from hsfs[hive])\n",
      "Collecting greenlet!=0.4.17; python_version >= \"3\" (from sqlalchemy->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/41/1e/2f070528abefc41bf11b9b4f6ec44924f3f623951813c513e8155d97f5aa/greenlet-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting importlib-metadata; python_version < \"3.8\" (from sqlalchemy->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/51/3af0db7c4caa1a5a43d506a848f63b56926fdf4f585d227a8a85a0671bbb/importlib_metadata-4.3.1-py3-none-any.whl\n",
      "Collecting botocore<1.21.0,>=1.20.84 (from boto3->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/22/72c81d754bbcb128cba2ad88670c3c320e4594e6ddd8cca6512c3967108c/botocore-1.20.84-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.5.0,>=0.4.0 (from boto3->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting six>=1.8.0 (from furl->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting orderedmultidict>=1.0.1 (from furl->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/04/16/5e95c70bda8fe6ea715005c0db8e602400bdba50ae3c72cb380eba551289/orderedmultidict-1.0.1-py2.py3-none-any.whl\n",
      "Collecting pytz>=2017.2 (from pandas->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.7.3 (from pandas->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting chardet<5,>=3.0.2 (from requests->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules (from pyjks->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\n",
      "Collecting javaobj-py3 (from pyjks->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/ab/756d5965665633b3c7dc252397e4c111da30235eaca68af204fe53f36d1b/javaobj_py3-0.4.3-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.3.5 (from pyjks->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Collecting twofish (from pyjks->hsfs[hive])\n",
      "Collecting pycryptodomex (from pyjks->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/12/49071b981f96720ccf26732de16b0fea7b13b676e4fe0643d8cdb69e199e/pycryptodomex-3.10.1-cp35-abi3-manylinux1_x86_64.whl\n",
      "Collecting future (from pyhopshive[thrift]; extra == \"hive\"->hsfs[hive])\n",
      "Collecting thrift>=0.10.0; extra == \"thrift\" (from pyhopshive[thrift]; extra == \"hive\"->hsfs[hive])\n",
      "Collecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->sqlalchemy->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/0f/8c/715c54e9e34c0c4820f616a913a7de3337d0cd79074dd1bed4dd840f16ae/zipp-3.4.1-py3-none-any.whl\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\" (from importlib-metadata; python_version < \"3.8\"->sqlalchemy->hsfs[hive])\n",
      "  Using cached https://files.pythonhosted.org/packages/2e/35/6c4fff5ab443b57116cb1aad46421fb719bed2825664e8fe77d66d99bcbc/typing_extensions-3.10.0.0-py3-none-any.whl\n",
      "Installing collected packages: avro, greenlet, zipp, typing-extensions, importlib-metadata, sqlalchemy, urllib3, six, python-dateutil, jmespath, botocore, s3transfer, boto3, orderedmultidict, furl, pyhumps, mock, pytz, numpy, pandas, chardet, idna, certifi, requests, pyasn1, pyasn1-modules, javaobj-py3, twofish, pycryptodomex, pyjks, PyMySQL, future, thrift, pyhopshive, hsfs\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall hsfs -y\n",
    "!pip3 install hsfs[python] --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a341472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "# TODO: replace the values below: [UUID], [project-name], [api-key]\n",
    "connection = hsfs.connection(host=\"[UUID].cloud.hopsworks.ai\",   # UUID is from Step 1, above\n",
    "    project=\"[project-name]\",                                    # project-name is from Step 2, above\n",
    "    engine=\"python\",\n",
    "    api_key_value=\"[api-key]\")                                   # the API key comes from Step 3, above\n",
    "\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073756a",
   "metadata": {},
   "source": [
    "## Show the first 5 rows in the Demo Feature Group\n",
    "\n",
    "First run the \"Feature Store Tour\" in Hopsworks to create the demo Feature Store project.\n",
    "\n",
    "A feature group is a set of related `features`. A feature is a data point that helps make predictions. A feature data value (or point) is often either a number (scalar, vector, etc) or a boolean or enum or string (categorical value).  If you are a data engineer, think of features in feature groups as columns in a database. If you are a data scientist, think of features in feature groups as columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0923c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_features = fs.get_feature_group(\"teams_features\",version=1)\n",
    "teams_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41acd71c",
   "metadata": {},
   "source": [
    "## Ingest some features into the Feature Store as a Feature Group\n",
    "The date we will ingest looks as follows:\n",
    "\n",
    " * first_name : string (categorical value)\n",
    " * last_name : string (categorical value)\n",
    " * country : string (categorical value)\n",
    " \n",
    " We want to use these features later to predict the country a first_name,last_name pair come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcdf145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    name_country_fg = fs.get_feature_group(name=\"name_country_fg\",version=1)\n",
    "    print(\"name_country_fg found in feature store\")\n",
    "except Exception as e: \n",
    "    url = \"https://repo.hops.works/dev/jdowling/data_cleaned_train.csv\"\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "    name_country_fg = fs.create_feature_group(name=\"name_country_fg\",\n",
    "                                    version=1,\n",
    "                                    primary_key=['first_name', 'last_name'],\n",
    "                                    description=\"Name - Country prediction\",\n",
    "#                                    validation_type=\"STRICT\",\n",
    "                                    time_travel_format=\"HUDI\",\n",
    "                                    online_enabled=True,                                        \n",
    "                                    statistics_config=True)\n",
    "    print(\"Created name_country_fg in the feature store\")\n",
    "    name_country_fg.save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56797b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name: {}\".format(name_country_fg.name))\n",
    "print(\"Description: {}\".format(name_country_fg.description))\n",
    "print(\"Features:\")\n",
    "features = name_country_fg.features\n",
    "for feature in features:\n",
    "    print(\"{:<60} \\t Primary: {} \\t Partition: {}\".format(feature.name, feature.primary, feature.partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbc354",
   "metadata": {},
   "source": [
    "## Feature Data Validation\n",
    "\n",
    "Garbage in, garbage out.\n",
    "\n",
    "Let's check for garbage in. If you ingest names from more than 195 countries, it's garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a05935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.rule import Rule\n",
    "rules = connection.get_rules()\n",
    "[print(rule.to_dict()) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_countries = fs.create_expectation(\"countries\",\n",
    "                                          description=\"min and max number of countries\",\n",
    "                                          features=[\"country\"], \n",
    "                                          rules=[Rule(name=\"HAS_NUMBER_OF_DISTINCT_VALUES\", level=\"ERROR\", min=1), \n",
    "                                                 Rule(name=\"HAS_NUMBER_OF_DISTINCT_VALUES\", level=\"ERROR\", max=195)])\n",
    "expectation_countries.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_country_fg.attach_expectation(expectation_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ea892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Dataframe and ingest its features into a feature group that you create here.  \n",
    "import pandas as pd \n",
    "columns = ['first_name', 'last_name', 'country']\n",
    "data = [['tom', 'johnson', 'UK'], ['penelope', 'charles', 'UK'], ['harry', 'windsor', \"USA\"]]   \n",
    "df = pd.DataFrame(data, columns=columns) \n",
    "name_country_fg.insert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = name_country_fg.get_expectations()\n",
    "[print(exp.description) for exp in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_validations = name_country_fg.get_validations()\n",
    "[print(validation.to_dict()) for validation in fg_validations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e0ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            first_name  \\\n",
      "0    VRE1CIFZ3TVAPUTXD3SZBRT5F8TORU1G4QRU4ASWVA3NOQ...   \n",
      "1    WGBN6225ML0E1EVBVEC9QK24YF5M7H5X48UEMJDFUP9MZI...   \n",
      "2    VWC1JGD3RCM0RLE7Q4R48AW1PLTFAV4MNRYMUVTOIQRYJR...   \n",
      "3    MZXVS9DIDHG4LQCYQQR705PYS9DY959ZQ0E71JG8MTGHJU...   \n",
      "4    91ZY2Y6K2FJFFEB745UBUG6099ISSGEYLY1JDTEKTRRM7M...   \n",
      "..                                                 ...   \n",
      "195  JK91GNOUIXLXVZ2UVKUKBEEREPDT9RX6LSSBDYGYR1VIE4...   \n",
      "196  MYENROCDXE1O0REIRG6YE17F6OVXZZVLQYHLXP39CUGW79...   \n",
      "197  84RV7XECJWAXPERR6VIPPB6K1LN01LIG5IRP00T2ENSGLZ...   \n",
      "198  4Y9VAHZHG3ERJ5ZJSY5K0ROOAIVU6YSQR66RP1WPKVMWHN...   \n",
      "199  DZ4LKITDNFWDRB5X1KFBASDL74XA5VSPMF7CGQGZUIUTV5...   \n",
      "\n",
      "                                             last_name  \\\n",
      "0    PGAJWUP5RVF4RZCG6SN9DQ88ST5H629H24SP1B4RTJ4AF9...   \n",
      "1    WAXG3OMOS15MQAH1L81Y7573VW5ITNLC8AT91CGGZK8M96...   \n",
      "2    RKJTQPPW65EKT3CL0ZQS32ITRAJVGI4EK3795AROT78BXX...   \n",
      "3    DAL2W7CKMZ0W3DQW7WYDT5O8JM8GDD7K90OEIM034ABEHS...   \n",
      "4    HU8S30A87QRDPXZMI7GW9POKKPW5I9U7U1DW7VOL1G837S...   \n",
      "..                                                 ...   \n",
      "195  8P1Y3A730QDTCWDT9UM8SE5FA5VT4DB348OSZ8AMDCTQSW...   \n",
      "196  X90JDCMJ1PBIYI1VM4DYJDV1FTHNEFYSW14118UROR6TAO...   \n",
      "197  7FEZW65T58CEZXSXIFHJ3B3ECZNIAZQ6L7O8VUG40GQPSM...   \n",
      "198  8N4Q93539TX6PRGLN4PXET7C37BTW74XOJTJLAOZEY3W6S...   \n",
      "199  4O1Q952PG5MSW3R05T3XUFOPEWGFBYEN1SOKS0PWNJRV8M...   \n",
      "\n",
      "                                               country  \n",
      "0    64EWMXYWH4P1SWWE1AOEUMT358KT03JHMF1J1CGXJQLFNN...  \n",
      "1    GC44G3HCSZZFQWTNNM6PSXE084BG8T4D3SC1G42DIOAVPS...  \n",
      "2    2ZES14RFMY3GQD36DC2SE69PDY34E5M7D4RCT0JPCU5V7E...  \n",
      "3    JNVS6PY6ERUKF7O4TRQF6C14UWPBGENWE126I9QTPUN23Q...  \n",
      "4    B57MZJFLM4JDP4N5IQAK7NFUOTISMX2JHGZOGFLVAN17YJ...  \n",
      "..                                                 ...  \n",
      "195  320PMLAKU3AKVXE8SXHRY47LKL9J07RMBNXBMMFWEMGCP6...  \n",
      "196  JJOAIDSSH11TWZ62YF6ZKQNTSTMFWA1MHHOB07OEQ24OUJ...  \n",
      "197  O85XT445TURV0EFLX0H2K2IQ2GNYSZPC0Y978LQIF8QDTK...  \n",
      "198  H1NE72DQMRN3N6P4DIM4P2XXVQ5NZUPLBO5XCRCKSTDYTF...  \n",
      "199  Y4WJO62ALCUE37U5XLTS9NKGXX4W41L6THP9A2CPRPQTLT...  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "def id_generator(size=1500, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "num_rows = 600\n",
    "data = np.array([id_generator() for i in range(num_rows)]).reshape(200,3)\n",
    "df2 = pd.DataFrame(data, columns=columns)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_country_fg.insert(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_validations = name_country_fg.get_validations()\n",
    "[print(validation.to_dict()) for validation in fg_validations]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
