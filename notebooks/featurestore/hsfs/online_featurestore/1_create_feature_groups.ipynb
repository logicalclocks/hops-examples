{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f77013",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"1. Create empty feature groups for Online Feature Store\"\n",
    "date: 2021-04-25\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a78852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>22</td><td>application_1648323495760_0034</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1648323495760_0034/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_1648323495760_0034_01_000001/streamFg__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3abe80",
   "metadata": {},
   "source": [
    "# Create empty feature groups \n",
    "In this demo example we are expecting to recieve data from Kafka topic, read using spark streaming, do streamig aggregations and ingest aggregated data to feature groups. Thus we will create empy feature groups where we will ingest streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c731d",
   "metadata": {},
   "source": [
    "### Define schema for feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b229a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_schema = StructType([StructField('tid', StringType(), True),\n",
    "                          StructField('datetime', StringType(), True),\n",
    "                          StructField('cc_num', LongType(), True),\n",
    "                          StructField('amount', DoubleType(), True)])\n",
    "\n",
    "schema_10m = StructType([StructField('cc_num', LongType(), True),\n",
    "                         StructField('num_trans_per_10m', LongType(), True),\n",
    "                         StructField('avg_amt_per_10m', DoubleType(), True),\n",
    "                         StructField('stdev_amt_per_10m', DoubleType(), True)])\n",
    "\n",
    "schema_1h = StructType([StructField('cc_num', LongType(), True),\n",
    "                         StructField('num_trans_per_1h', LongType(), True),\n",
    "                         StructField('avg_amt_per_1h', DoubleType(), True),\n",
    "                         StructField('stdev_amt_per_1h', DoubleType(), True)])\n",
    "\n",
    "schema_12h = StructType([StructField('cc_num', LongType(), True),\n",
    "                         StructField('num_trans_per_12h', LongType(), True),\n",
    "                         StructField('avg_amt_per_12h', DoubleType(), True),\n",
    "                         StructField('stdev_amt_per_12h', DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077702fb",
   "metadata": {},
   "source": [
    "### Create empty spark dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdd1017",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_card_df = sqlContext.createDataFrame(sc.emptyRDD(), card_schema)\n",
    "empty_10m_agg_df = sqlContext.createDataFrame(sc.emptyRDD(), schema_10m)\n",
    "empty_1h_agg_df = sqlContext.createDataFrame(sc.emptyRDD(), schema_1h)\n",
    "empty_12h_agg_df = sqlContext.createDataFrame(sc.emptyRDD(), schema_12h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7692b",
   "metadata": {},
   "source": [
    "### Establish a connection with your Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9c5324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3cce17",
   "metadata": {},
   "source": [
    "### Create feature group metadata objects and save empty spark dataframes to materialise them in hopsworks feature store.\n",
    "\n",
    "Now We will create each feature group and enable online feature store. Since we are plannig to use these feature groups durring online model serving primary key(s) are required to retrieve feature vector from online feature store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3a615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_transactions = fs.create_feature_group(\"card_transactions\", \n",
    "                                             version = 1,\n",
    "                                             statistics_config=False, \n",
    "                                             primary_key=[\"tid\"],\n",
    "                                             stream=True)\n",
    "\n",
    "card_transactions.save(empty_card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "904638e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_transactions_10m_agg = fs.create_feature_group(\"card_transactions_10m_agg\", \n",
    "                                              version = 1,\n",
    "                                              statistics_config=False, \n",
    "                                              primary_key=[\"cc_num\"],\n",
    "                                              stream=True)\n",
    "\n",
    "card_transactions_10m_agg.save(empty_10m_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9df7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_transactions_1h_agg = fs.create_feature_group(\"card_transactions_1h_agg\", \n",
    "                                              version = 1,\n",
    "                                              statistics_config=False, \n",
    "                                              primary_key=[\"cc_num\"],\n",
    "                                              stream=True)\n",
    "\n",
    "card_transactions_1h_agg.save(empty_1h_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c617f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_transactions_12h_agg = fs.create_feature_group(\"card_transactions_12h_agg\", \n",
    "                                              version = 1,\n",
    "                                              statistics_config=False, \n",
    "                                              primary_key=[\"cc_num\"],\n",
    "                                              stream=True)\n",
    "\n",
    "card_transactions_12h_agg.save(empty_12h_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05dd21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}