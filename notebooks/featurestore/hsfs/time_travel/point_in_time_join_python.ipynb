{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0558929e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Point-in-time joins with PySpark\"\n",
    "date: 2021-09-09\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a0f62",
   "metadata": {},
   "source": [
    "# Point-in-Time (PIT) joins in Hopsworks Feature Store\n",
    "\n",
    "In order to create a training dataset, data scientists usually have to generate information about the future by putting themselves back in time, or travelling back in time.\n",
    "\n",
    "Let's take the case of churn prediction: We want to predict which of our users will churn within the next few months. To train such a model we need to construct a training dataset containing rows for each of our users, one column indicating whether a user churned and X additional columns with features about the user, such as his historical purchases or interactions with the product.\n",
    "\n",
    "Since we don't know the future yet (it's what we want to predict), to generate such prediction targets, we have to go back in time and determine which users churned in the last couple of months.\n",
    "\n",
    "### Snapshot based time travel\n",
    "\n",
    "If you have looked at our other time travel notebooks, the most simple solution to this problem would be to choose a single cutoff time for all our users. Using the time travel capabilities of the Feature Store with Apache Hudi make it easy enough to fetch data from a single point in time. This can work if features aren't frequently updated, however, when prediction target events of different users lie far apart and features are updated frequently you might leave a lot of information unused, hurting the accuracy of your model.\n",
    "\n",
    "### Individual point-in-time correct cutoffs\n",
    "\n",
    "A better approach includes using all information up to the time the prediction target event happened. So that means we still go back, let's say a maximum 6 months, in time, however, we want to remember the prediction target event time stamp on a user level (row level) and find the latest values of our prediction features before this point in time. The following diagrams illustrates this approach for one user:\n",
    "\n",
    "![image.png](../images/pit-join.png)\n",
    "\n",
    "A problem often occurring at this stage is the possibility of leaking information from the future (light red signals) into the training dataset. This means using feature signals which happened after the prediction target event has to be strictly avoided. What we want to retrieve from the feature store are the green, and the green only, feature signals.\n",
    "\n",
    "In this case it is not as simple as performing a time travel query with a single time stamp. This challenge is solved by a point-in-time correct join instead.\n",
    "**Point-in-time joins** prevent feature leakage by recreating the state of the world at a single point in time for every entity or primary key value (user in our case).\n",
    "\n",
    "**Hopsworks Feature Store** abstracts this complexity away by simply telling it where to find the relevant event time stamps for feature groups. We will go through the process in the rest of the notebook.\n",
    "\n",
    "## Event-time enabled Feature Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7350180",
   "metadata": {},
   "source": [
    "For the Feature Store to be able to perform a PIT join, we need to tell it where to find the event time stamp within each feature group. Event-time is a timestamp indicating the instant in time when an event happened at the source of the event, so this is *not* an ingestion time stamp or the like, but instead should originate from your source systems.\n",
    "\n",
    "To \"event-time enable\" a feature group, you set the `event_time` argument at feature group creation. We are using simple Integers to indicate the timestamps, for better readability.\n",
    "\n",
    "### For simplicity we will create three feature groups\n",
    "* `marketing` with customer id, complaints in the last 7 days, outbound activities in the last 7 days and coupons received in the last 14 days; \n",
    "* `contract` with customer id, contract_type;\n",
    "* `churn` which will contain labels wether loan was cancelled `1` or rejected `0` with a timestamp when the contract was created or cancelled, note that this feature group has a composite primary key of customer id and a contract id, referring to the id in the contract feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfa4f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>15</td><td>application_1630405039893_0017</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1630405039893_0017/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_1630405039893_0017_01_000001/demo_fs_meb10000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0ed622",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_schema = StructType([\n",
    "  StructField(\"customer_id\", IntegerType(), True),\n",
    "  StructField(\"ts\", LongType(), True),\n",
    "  StructField(\"complaints_7d\", IntegerType(), True),\n",
    "  StructField(\"outbound_7d\", IntegerType(), True),\n",
    "  StructField(\"coupon_14d\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "contract_schema = StructType([\n",
    "  StructField(\"contract_id\", IntegerType(), True),\n",
    "  StructField(\"ts\", LongType(), True),\n",
    "  StructField(\"contract_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "churn_schema =  StructType([\n",
    "  StructField(\"customer_id\", IntegerType(), True),\n",
    "  StructField(\"contract_id\", IntegerType(), True),\n",
    "  StructField(\"ts\", LongType(), True),\n",
    "  StructField(\"contract_churn\", IntegerType(), True)       \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcae41",
   "metadata": {},
   "source": [
    "We will first load the Churn Feature Group with the initial contracts. We can assume there is a job running inserting new rows for every new contract into this feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e318a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_contracts = [\n",
    "    Row(1, 100, 10010, 0),\n",
    "    Row(2, 101, 10017, 0),\n",
    "    Row(3, 102, 10035, 0),\n",
    "    Row(4, 103, 10023, 0),\n",
    "    Row(5, 104, 10546, 0),\n",
    "    Row(6, 105, 10213, 0),\n",
    "    Row(7, 106, 10056, 0),\n",
    "    Row(8, 107, 10012, 0)\n",
    "]\n",
    "\n",
    "new_contracts_df = spark.createDataFrame(new_contracts, churn_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bc6c1",
   "metadata": {},
   "source": [
    "At the same time some contracts will be cancelled and inserted into the feature group over time. We will perform this insertion with a bit of time difference, so that we can later demonstrate the capabilities of PIT joins together with time-travel queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59836af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "churned_contracts = [\n",
    "    Row(1, 100, 10356, 1),\n",
    "    Row(5, 104, 10692, 1),\n",
    "    Row(6, 105, 10375, 1),\n",
    "    Row(8, 107, 10023, 1)\n",
    "]\n",
    "\n",
    "churned_contracts_df = spark.createDataFrame(churned_contracts, churn_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939894c1",
   "metadata": {},
   "source": [
    "Now let's create some mock data for the secondary feature groups, containing the actual explanatory features used to predict the churn.\n",
    "\n",
    "The contract feature group is a feature group that gets new contracts appended with information about the contract itself only, such as the type of contract. Hence all timestamps are the same as the inital contract creation in the churn feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2509cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = [\n",
    "    Row(100, 10010, \"Long-term\"),\n",
    "    Row(101, 10017, \"Short-term\"),\n",
    "    Row(102, 10035, \"Trial\"),\n",
    "    Row(103, 10023, \"Short-term\"),\n",
    "    Row(104, 10546, \"Long-term\"),\n",
    "    Row(105, 10213, \"Trial\"),\n",
    "    Row(106, 10056, \"Long-term\"),\n",
    "    Row(107, 10012, \"Short-term\")\n",
    "]\n",
    "\n",
    "contracts_df = spark.createDataFrame(contracts, contract_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba147b2",
   "metadata": {},
   "source": [
    "The marketing activities feature group contains features related to outbound and inbound contacts to the customer. You can imagine these to be computed by a streaming application, updating the features every time new events arrive.\n",
    "\n",
    "In the point in time join we want to get the latest of these updates just before or at the same time as the prediction event happened. Contracts can be in the training dataset twice, once when they haven't churned yet and then when they churned. The rows which should be picked for either of the target events are marked with a comment.\n",
    "\n",
    "At the same time, always the latest state of the customer's marketing profile should be available in the online feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a49be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing = [\n",
    "    Row(1, 10010, 0, 0, 1), # this one\n",
    "    Row(1, 10174, 3, 0, 4),\n",
    "    Row(1, 10257, 7, 0, 3),\n",
    "    Row(1, 10352, 3, 0, 5), # this one\n",
    "    Row(1, 10753, 0, 0, 0),\n",
    "    Row(1, 10826, 0, 0, 1), # online feature store\n",
    "    \n",
    "    Row(2, 10017, 0, 1, 1), # this one\n",
    "    Row(2, 10021, 0, 1, 1),\n",
    "    Row(2, 10034, 0, 1, 2), # online feature store\n",
    "    \n",
    "    Row(3, 10035, 1, 3, 0), # this one\n",
    "    Row(3, 10275, 1, 2, 0),\n",
    "    \n",
    "    Row(5, 10546, 0, 1, 0), # this one\n",
    "    Row(5, 10598, 2, 2, 1), # this one\n",
    "    Row(5, 13567, 0, 1, 0),    \n",
    "    Row(5, 16245, 0, 1, 0), # online feature store\n",
    "    \n",
    "    Row(6, 10213, 0, 0, 1), # this one\n",
    "    Row(6, 10234, 0, 5, 0), # this one\n",
    "    Row(6, 10436, 0, 0, 1), # online feature store\n",
    "    \n",
    "    Row(7, 10056, 0, 0, 0), # this one\n",
    "    Row(7, 10056, 0, 1, 0),\n",
    "    Row(7, 10056, 0, 2, 1),\n",
    "    Row(7, 10056, 0, 3, 0), # online feature store\n",
    "    \n",
    "    Row(8, 10012, 0, 0, 1), # this one\n",
    "    Row(8, 10023, 10, 0, 1), # this one\n",
    "    Row(8, 10033, 0, 0, 1), # online feature store\n",
    "]\n",
    "\n",
    "marketing_df = spark.createDataFrame(marketing, marketing_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b79d7",
   "metadata": {},
   "source": [
    "### Create the feature groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae4d5e",
   "metadata": {},
   "source": [
    "We are now ready to create our three feature groups. Note the additional argument we are passing, in order to tell the Feature Store which column should be used as `event_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2467a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_fg = fs.create_feature_group(\"marketing\",\n",
    "                                       version=1,\n",
    "                                       description=\"Features about inbound/outbound communication with customers\",\n",
    "                                       online_enabled=True,\n",
    "                                       statistics_config=False,\n",
    "                                       primary_key=[\"customer_id\"],\n",
    "                                       event_time=\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de59f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_fg.save(marketing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad82e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_fg = fs.create_feature_group(\"contracts\",\n",
    "                                       version=1,\n",
    "                                       description=\"Contract information features\",\n",
    "                                       online_enabled=True,\n",
    "                                       statistics_config=False,\n",
    "                                       primary_key=[\"contract_id\"],\n",
    "                                       event_time=\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb23a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_fg.save(contracts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547bbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_fg = fs.create_feature_group(\"churn\",\n",
    "                                       version=1,\n",
    "                                       description=\"Customer/contract information about activity of contract\",\n",
    "                                       online_enabled=True,\n",
    "                                       statistics_config=False,\n",
    "                                       primary_key=[\"customer_id\", \"contract_id\"],\n",
    "                                       event_time=\"ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab33ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_fg.save(new_contracts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e1874",
   "metadata": {},
   "source": [
    "We insert the churned contracts, in a separate upsert step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ab9ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StorageWarning: The statistics are not enabled of feature group `churn`, with version `1`. No statistics computed."
     ]
    }
   ],
   "source": [
    "churn_fg.insert(churned_contracts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde28926",
   "metadata": {},
   "source": [
    "### Constructing a Point-in-Time Join\n",
    "\n",
    "The Feature Store HSFS API comes with a `Query` abstraction. Operations such as `.join` or `.select` on a feature group return such `Query` objects. Queries are solely based on metadata up until they are saved as training datasets or read into Dataframes.\n",
    "\n",
    "There are **two requirements** to construct a point in time join:\n",
    "\n",
    "1. All feature groups have to be event time enabled. If there is no event timestamp available, you can fall back on creating an ingestion timestamp in your feature engineering pipeline.\n",
    "2. The label (feature) group is the Feature Group that contains the column that is defined as the \"label\" column when creating the training dataset. The **left-most feature group has to be the label group**, meaning this is the feature group of which the timestamps will be used as reference timestamp against which the explaining features are joined (dark red dots in the diagram above). In our case this is the churn feature group.\n",
    "\n",
    "So let's construct the join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f12e0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = churn_fg.select_all().join(contracts_fg.select([\"contract_type\"]), on=[\"contract_id\"]) \\\n",
    "                             .join(marketing_fg.select([\"complaints_7d\", \"outbound_7d\", \"coupon_14d\"]), on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6979580",
   "metadata": {},
   "source": [
    "You can print and look at the constructed SQL join query. However, there is no need to go into this complexity, as it will all be handled by the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed12141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`customer_id`, `fg2`.`contract_id`, `fg2`.`ts`, `fg2`.`contract_churn`, `fg0`.`contract_type`, RANK() OVER (PARTITION BY `fg2`.`contract_id`, `fg2`.`ts` ORDER BY `fg0`.`ts` DESC) pit_rank_hopsworks\n",
      "FROM `fg2` `fg2`\n",
      "INNER JOIN `fg0` `fg0` ON `fg2`.`contract_id` = `fg0`.`contract_id` AND `fg2`.`ts` >= `fg0`.`ts`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`customer_id`, `fg2`.`contract_id`, `fg2`.`ts`, `fg2`.`contract_churn`, `fg1`.`complaints_7d`, `fg1`.`outbound_7d`, `fg1`.`coupon_14d`, RANK() OVER (PARTITION BY `fg2`.`customer_id`, `fg2`.`ts` ORDER BY `fg1`.`ts` DESC) pit_rank_hopsworks\n",
      "FROM `fg2` `fg2`\n",
      "INNER JOIN `fg1` `fg1` ON `fg2`.`customer_id` = `fg1`.`customer_id` AND `fg2`.`ts` >= `fg1`.`ts`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`customer_id`, `right_fg0`.`contract_id`, `right_fg0`.`ts`, `right_fg0`.`contract_churn`, `right_fg0`.`contract_type`, `right_fg1`.`complaints_7d`, `right_fg1`.`outbound_7d`, `right_fg1`.`coupon_14d`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`customer_id` = `right_fg1`.`customer_id` AND `right_fg0`.`contract_id` = `right_fg1`.`contract_id` AND `right_fg0`.`ts` = `right_fg1`.`ts`)"
     ]
    }
   ],
   "source": [
    "print(query.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27473bc0",
   "metadata": {},
   "source": [
    "As explained above, the Query itself does not perform any join yet until you call an action such as `.read` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d505230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|customer_id|contract_id|   ts|contract_churn|contract_type|complaints_7d|outbound_7d|coupon_14d|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|          6|        105|10375|             1|        Trial|            0|          5|         0|\n",
      "|          5|        104|10692|             1|    Long-term|            2|          2|         1|\n",
      "|          5|        104|10546|             0|    Long-term|            0|          1|         0|\n",
      "|          2|        101|10017|             0|   Short-term|            0|          1|         1|\n",
      "|          7|        106|10056|             0|    Long-term|            0|          0|         0|\n",
      "|          8|        107|10023|             1|   Short-term|           10|          0|         1|\n",
      "|          1|        100|10356|             1|    Long-term|            3|          0|         5|\n",
      "|          1|        100|10010|             0|    Long-term|            0|          0|         1|\n",
      "|          6|        105|10213|             0|        Trial|            0|          0|         1|\n",
      "|          8|        107|10012|             0|   Short-term|            0|          0|         1|\n",
      "|          3|        102|10035|             0|        Trial|            1|          3|         0|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "query.read().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81421ec4",
   "metadata": {},
   "source": [
    "#### Filters on point-in-time queries\n",
    "\n",
    "As with any other query, it is possible to apply filters to it.\n",
    "For example, if you are planning to train only a model for a certain contract type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "794926b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|customer_id|contract_id|   ts|contract_churn|contract_type|complaints_7d|outbound_7d|coupon_14d|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|          5|        104|10692|             1|    Long-term|            2|          2|         1|\n",
      "|          5|        104|10546|             0|    Long-term|            0|          1|         0|\n",
      "|          7|        106|10056|             0|    Long-term|            0|          0|         0|\n",
      "|          1|        100|10356|             1|    Long-term|            3|          0|         5|\n",
      "|          1|        100|10010|             0|    Long-term|            0|          0|         1|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "churn_fg.select_all().join(\n",
    "                        contracts_fg.select([\"contract_type\"])\n",
    "                                    .filter(contracts_fg.contract_type == \"Long-term\")\n",
    "                        , on=[\"contract_id\"]) \\\n",
    "                     .join(marketing_fg.select([\"complaints_7d\", \"outbound_7d\", \"coupon_14d\"]), on=[\"customer_id\"]).read().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3bec0",
   "metadata": {},
   "source": [
    "#### Combining time-travel and point-in-time join in one query\n",
    "\n",
    "We performed a separate upsert on our churn feature group in order to be able to demonstrate time-travel, so let's look at the commit information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e38b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1631088747000: {'committedOn': '20210908081227', 'rowsUpdated': 0, 'rowsInserted': 4, 'rowsDeleted': 0}, 1631088722000: {'committedOn': '20210908081202', 'rowsUpdated': 0, 'rowsInserted': 8, 'rowsDeleted': 0}}"
     ]
    }
   ],
   "source": [
    "commit_details = churn_fg.commit_details()\n",
    "print(commit_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dabc4",
   "metadata": {},
   "source": [
    "As you can see, there are two commits: (1) the initial commit, which inserted the eight contracts. And (2) the upsert commit with the additinal four churned contracts.\n",
    "\n",
    "Now we would like to query the state of the Feature Store at the time before the churned contracts were ingested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36145d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'20210908081202'"
     ]
    }
   ],
   "source": [
    "# getting the correct timestamp, we sort by the commit time and get the string representation of the commit time\n",
    "committed_on = commit_details[sorted(commit_details)[0]][\"committedOn\"]\n",
    "committed_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4a4c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1631088686000: {'committedOn': '20210908081126', 'rowsUpdated': 0, 'rowsInserted': 8, 'rowsDeleted': 0}}"
     ]
    }
   ],
   "source": [
    "contracts_fg.commit_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ae3097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------------+\n",
      "|customer_id|contract_id|   ts|contract_churn|\n",
      "+-----------+-----------+-----+--------------+\n",
      "|          2|        101|10017|             0|\n",
      "|          5|        104|10546|             0|\n",
      "|          8|        107|10012|             0|\n",
      "|          3|        102|10035|             0|\n",
      "|          6|        105|10213|             0|\n",
      "|          7|        106|10056|             0|\n",
      "|          4|        103|10023|             0|\n",
      "|          1|        100|10010|             0|\n",
      "+-----------+-----------+-----+--------------+"
     ]
    }
   ],
   "source": [
    "churn_fg.select_all().as_of(committed_on).read().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9127dd",
   "metadata": {},
   "source": [
    "And combining this with our point-in-time join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b66ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|customer_id|contract_id|   ts|contract_churn|contract_type|complaints_7d|outbound_7d|coupon_14d|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+\n",
      "|          5|        104|10546|             0|    Long-term|            0|          1|         0|\n",
      "|          2|        101|10017|             0|   Short-term|            0|          1|         1|\n",
      "|          7|        106|10056|             0|    Long-term|            0|          0|         0|\n",
      "|          1|        100|10010|             0|    Long-term|            0|          0|         1|\n",
      "|          6|        105|10213|             0|        Trial|            0|          0|         1|\n",
      "|          8|        107|10012|             0|   Short-term|            0|          0|         1|\n",
      "|          3|        102|10035|             0|        Trial|            1|          3|         0|\n",
      "+-----------+-----------+-----+--------------+-------------+-------------+-----------+----------+"
     ]
    }
   ],
   "source": [
    "query.as_of(committed_on).read().show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790ca45",
   "metadata": {},
   "source": [
    "### Creating the training dataset\n",
    "\n",
    "Creating the training dataset is now as simple as initializing the metadata with `.create_training_dataset()` and subsequently persisting it and materializing the query with `.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02cdffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.create_training_dataset(\"churn_model\",\n",
    "                                version=1,\n",
    "                                data_format=\"csv\",\n",
    "                                label=[\"contract_churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "390d4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.save(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25138e1c",
   "metadata": {},
   "source": [
    "#### Querying the online feature store\n",
    "\n",
    "We can reproduce the query and get the latest feature vector for each contract from the online feature store now.\n",
    "\n",
    "**NOTE:**\n",
    "- Any applied filter at the time of creating the training dataset is not applied during serving time. It is the responsibility of the application performing the lookup, to only provide contract ids which belong for example to a certain category. The reason for that is that we always want to get back a vector, applying a filter, might lead to no results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "939e0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100, 10356, 'Long-term', 0, 0, 1]"
     ]
    }
   ],
   "source": [
    "td.get_serving_vector({\"customer_id\": 1, \"contract_id\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fabf89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 101, 10017, 'Short-term', 0, 1, 2]"
     ]
    }
   ],
   "source": [
    "td.get_serving_vector({\"customer_id\": 2, \"contract_id\": 101})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167baa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
