{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"2. Create Training Data from Features \"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSFS `Feature Views` and `Training Datasets`\n",
    "\n",
    "`Feature Views` is the third building block of the Hopsworks Feature Store. Feature Views store metadata of our dataset.\n",
    "\n",
    "`Training datasets` is the fourth building block of the Hopsworks Feature Store. \n",
    "\n",
    "Training datasets can be saved in a ML framework friendly format (eg. TfRecords, CSV, Numpy) and then be fed to a machine learning model for training.\n",
    "\n",
    "Training datasets can also be stored on external storage systems like Amazon S3 or GCS to be read by external model training platforms.\n",
    "\n",
    "As with the previous notebooks, the first step is to establish a connection with the Hopsworks feature store and get the feature store handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>31</td><td>application_1652868601260_0056</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1652868601260_0056/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-31-0.eu-north-1.compute.internal:8044/node/containerlogs/container_e01_1652868601260_0056_01_000001/hsfs_basics__maksym00\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `Feature View` from a query\n",
    "\n",
    "In the previous notebook ([feature_exploration](./feature_exploration.ipynb)) we walked through how to explore and query the Hopsworks feature store using HSFS. We can use the queries produced in the previous notebook to create a `Feature Views`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_fg = fs.get_feature_group(\n",
    "    name = 'sales_fg',\n",
    "    version = 1\n",
    ")\n",
    "\n",
    "exogenous_fg = fs.get_feature_group(\n",
    "    name = 'exogenous_fg',\n",
    "    version = 1\n",
    ")\n",
    "\n",
    "query = sales_fg.select_all()\\\n",
    "        .join(exogenous_fg.select(['fuel_price', 'unemployment', 'cpi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create Feature View we can use `FeatureStore.create_feature_view()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.create_feature_view(\n",
    "    name = 'exodenous_sale',\n",
    "    version = 1,\n",
    "    query = query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_view.FeatureView object at 0x7f194fd99b20>"
     ]
    }
   ],
   "source": [
    "feature_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now `Feature View` is saved in Hopsworks and we can retrieve it using `FeatureStore.get_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\n",
    "    name = 'exodenous_sale',\n",
    "    version = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "feature_view.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `FeatureView.preview_feature_vector()` returns a sample of assembled serving vector from online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 34, datetime.date(2010, 2, 5), 19443.48, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.835, '7.808', '210.3399684']"
     ]
    }
   ],
   "source": [
    "feature_view.preview_feature_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To get subset of data use `FeatureView.get_batch_data()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch = feature_view.get_batch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(df_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "|fuel_price|unemployment|        cpi|\n",
      "+----------+------------+-----------+\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "|     2.625|       8.106|211.3501429|\n",
      "+----------+------------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_batch.select(['fuel_price', 'unemployment', 'cpi']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create training dataset we use `FeatureView.create_training_dataset()` method.\n",
    "\n",
    "It will inherit the name of FeatureView.\n",
    "\n",
    "The feature store currently supports the following data formats for\n",
    "training datasets: **tfrecord, csv, tsv, parquet, avro, orc**.\n",
    "\n",
    "We can choose necessary format using **data_format** parameter.\n",
    "\n",
    "\n",
    "Also we can specify split ratio using **splits** parameter.\n",
    "\n",
    "**train_split** - specify which split will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = feature_view.create_training_dataset(\n",
    "    version = 1,\n",
    "    description = 'trial_dataset',\n",
    "    data_format = 'csv',\n",
    "    splits = {'train': 80, 'validation': 20},\n",
    "    train_split = \"train\",\n",
    "    write_options = {'wait_for_job': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None)"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to load dataset from Hopsworks we can use `FeatureView.get_training_dataset_splits()` method.\n",
    "\n",
    "By specifying **splits** parameter we can choose what split of training dataset to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "|store|dept|      date|weekly_sales|is_holiday|sales_last_month_store_dep|\n",
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "|   15|   1|2010-02-05|    12239.38|     false|                       0.0|\n",
      "|   15|  10|2010-02-05|     9444.31|     false|                       0.0|\n",
      "|   15|  11|2010-02-05|    13259.31|     false|                       0.0|\n",
      "|   15|  12|2010-02-05|     2360.06|     false|                       0.0|\n",
      "|   15|  13|2010-02-05|    19437.35|     false|                       0.0|\n",
      "+-----+----+----------+------------+----------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "VersionWarning: No version provided for creating training dataset, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "td_version, df = feature_view.get_training_dataset_splits(\n",
    "    splits = {},\n",
    "    start_time = None,\n",
    "    end_time = None,\n",
    "    version = 2\n",
    ")\n",
    "\n",
    "df.select(['store', 'dept', 'date', 'weekly_sales', 'is_holiday', 'sales_last_month_store_dep']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}