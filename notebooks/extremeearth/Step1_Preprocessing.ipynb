{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 0: Preprocessing the data\n",
    "This notebook will perform the following operations:\n",
    "- take the 'train.json' as input file\n",
    "- do some preprocessing and data engineering\n",
    "- save the preprocessed dataset in a Hopsworks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "from hops import featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train_preprocessed_all.json\n",
      "train_preprocessed_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train_preprocessed.json\n",
      "test_preprocessed_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/test_preprocessed.json"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'eodata'\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER,'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "train_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed.json')\n",
    "test_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'test_preprocessed.json')\n",
    "\n",
    "print(\"train_ds_path:\", train_ds_path)\n",
    "print(\"train_preprocessed_all_ds_path:\", train_preprocessed_all_ds_path)\n",
    "print(\"train_preprocessed_ds_path:\", train_preprocessed_ds_path)\n",
    "print(\"test_preprocessed_ds_path:\", test_preprocessed_ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the raw data to pandas dataframe\n",
    "raw_train_df = pd.read_json(train_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ... is_iceberg\n",
      "0     dfd5f913  ...          0\n",
      "1     e25388fd  ...          0\n",
      "2     58b2aaa0  ...          1\n",
      "3     4cfc3a18  ...          0\n",
      "4     271f93f4  ...          0\n",
      "...        ...  ...        ...\n",
      "1599  04e11240  ...          0\n",
      "1600  c7d6f6f8  ...          0\n",
      "1601  bba1a0f1  ...          0\n",
      "1602  7f66bb44  ...          0\n",
      "1603  9d8f326c  ...          0\n",
      "\n",
      "[1604 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "raw_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new feature band_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for taking list average\n",
    "def list_avg(row):\n",
    "    return [sum(x)/2 for x in zip(row['band_1'], row['band_2'])]\n",
    "\n",
    "raw_train_df['band_avg'] = raw_train_df.apply(lambda row: list_avg(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ...                                           band_avg\n",
      "0     dfd5f913  ...  [-27.516239499999998, -28.346024, -29.84960749...\n",
      "1     e25388fd  ...  [-21.874347999999998, -21.4524295, -20.7830205...\n",
      "2     58b2aaa0  ...  [-24.737316, -24.348173, -22.762496, -21.28190...\n",
      "3     4cfc3a18  ...  [-25.172013999999997, -25.301306500000003, -25...\n",
      "4     271f93f4  ...  [-26.6069355, -26.712035999999998, -26.7120359...\n",
      "...        ...  ...                                                ...\n",
      "1599  04e11240  ...  [-29.4237985, -29.105365, -26.472991999999998,...\n",
      "1600  c7d6f6f8  ...  [-27.437631500000002, -27.400965, -27.76694599...\n",
      "1601  bba1a0f1  ...  [-21.723625, -23.7647725, -23.9906165, -22.930...\n",
      "1602  7f66bb44  ...  [-24.262994499999998, -23.944199, -24.2661145,...\n",
      "1603  9d8f326c  ...  [-22.1770305, -22.817203499999998, -23.9654685...\n",
      "\n",
      "[1604 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "raw_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path train_preprocessed_all.json to hdfs path hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train_preprocessed_all.json\n",
      "\n",
      "Finished copying"
     ]
    }
   ],
   "source": [
    "#save raw train df in dataset\n",
    "raw_train_df.to_json(path_or_buf='train_preprocessed_all.json', orient='records')\n",
    "hdfs.copy_to_hdfs(\"train_preprocessed_all.json\", DATA_FOLDER , overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}