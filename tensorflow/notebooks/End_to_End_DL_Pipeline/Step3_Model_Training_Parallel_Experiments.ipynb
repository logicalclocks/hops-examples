{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Training ResNet on The TinyImageNet dataset. Notebook (3/6) in the End-to-End Scalable Deep Learning Pipeline on Hops.\n",
    "\n",
    "ResNet is one of the state-of-the-art deep networks for computer vision. ResNet is based on the idea of residual learning. A residual network has so called \"shortcut connections\" parallel to the normal convolutional layers. Those shortcuts act like highways for gradients which allows to train extremely deep networks without suffering from vanishing or exploding gradients. Intuitively, by having the short-cut connections available during training, the network can learn which layers it doesn't need, since it can always fall-back on the shortcut (identity) connection.\n",
    "![sample_images_from_tfr.png](./../images/resnet_1.png)\n",
    "\n",
    "Despite the huge increase in the overall depth, a ResNet with 50 layers has roughly half the parameters in AlexNet, this is because ResNet relies only on small filters in the network. E.g a stack of several 3x3 filters uses the same number of parameters as a single 7x7 filter. \n",
    "\n",
    "ResNet come in different depths: Resnet18, Resnet34, Resnet50,Resnet101, Resnet152\n",
    "\n",
    "For the deeper variants, ResNet uses \"bottleneck building blocks\" to lower the training time for the deepest networks. \n",
    "\n",
    "![bottlenneck.png](./../images/bottleneck.png)\n",
    "\n",
    "ResNet uses batch notmalization right after each convolution and before the activation function. Typically ReLU is used as the activation function.\n",
    "\n",
    "\n",
    "This notebook will read the TFRecords that were written by notebook number 2 ([Notebook number two](./Step2_Image_PreProcessing.ipynb)) and feed them into ResNet for single-GPU training and distributed hyperparameter search using several GPUs. \n",
    "\n",
    "Specifically, this notebook reads TFRecords from:\n",
    "\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/tfrecords_clean\n",
    "\n",
    "And write hyperparameter results to: \n",
    "\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/hyperparams.txt\n",
    "\n",
    "![step3.png](./../images/step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports\n",
    "\n",
    "Tested with versions:\n",
    "\n",
    "- numpy: 1.14.5\n",
    "- hops: 2.6.4\n",
    "- pydoop: 2.0a3\n",
    "- tensorboard: 1.8.0\n",
    "- tensorflow: 1.8.0\n",
    "- tensorflow-gpu: 1.8.0\n",
    "- tfspark: 1.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pydoop.hdfs as py_hdfs\n",
    "from hops import hdfs\n",
    "import numpy as np\n",
    "from builtins import range\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "from tensorflow.contrib.data.python.ops import interleave_ops\n",
    "from tensorflow.contrib.data.python.ops import batching\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import shutil\n",
    "import logging\n",
    "import re\n",
    "from glob import glob\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = hdfs.project_path()\n",
    "DATASET_BASE_DIR = PROJECT_DIR + \"tiny-imagenet/tiny-imagenet-200/\"\n",
    "TRAIN_DIR = DATASET_BASE_DIR + \"train\"\n",
    "TEST_DIR = DATASET_BASE_DIR + \"test\"\n",
    "VAL_DIR = DATASET_BASE_DIR + \"val/images/\"\n",
    "ID_TO_CLASS_FILE = DATASET_BASE_DIR + \"/words.txt\"\n",
    "INPUT_DIR = DATASET_BASE_DIR + \"tfrecords_clean/\"\n",
    "LOG_DIR = DATASET_BASE_DIR + \"logs/\"\n",
    "VAL_LABELS_FILE = DATASET_BASE_DIR + \"val/val_annotations.txt\"\n",
    "FILE_PATTERN = \"*.JPEG\"\n",
    "SIZES_FILE = DATASET_BASE_DIR + \"sizes.txt\"\n",
    "HYPERPARAMS_FILE = DATASET_BASE_DIR + \"hyperparams.txt\"\n",
    "VALIDATION_SET_RESULTS_FILE = DATASET_BASE_DIR + \"single_gpu_validation_results.txt\"\n",
    "EXPORT_MODEL_DIR = DATASET_BASE_DIR + \"exported_model/\"\n",
    "DEFAULT_BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Building the Layers in ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerBuilder(object):\n",
    "    \"\"\"\n",
    "    Class for building ResNet layers, contains functions for building the various layers:\n",
    "    - conv layers\n",
    "    - max pooling layers\n",
    "    \"\"\"\n",
    "    def __init__(self, activation=None, data_format='channels_last',\n",
    "                 training=False, use_batch_norm=False, batch_norm_config=None,\n",
    "                 conv_initializer=None, adv_bn_init=False):\n",
    "        \"\"\"\n",
    "        Initialize the layer builder with configuration parameters\n",
    "        \n",
    "        :param activation: activation function to use\n",
    "        :param data_format: the format of the images, e.g channels first (3,64,64) or channels last (64,64,3)\n",
    "        :param training: boolean wether training or evaluation is performed\n",
    "        :param use_batch_norm: whether to add a batch normalization layer\n",
    "        :param batch_norm_config: configuration for normalizing a batch, e.g how to scale/decay how much variance etc.\n",
    "        :param conv_initializer: optional initializer for conv layers (tf.contrib.layers.variance_scaling_initializer)\n",
    "        :param adv_bn_init: init gamma of the last BN of each ResMod at 0\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.data_format = data_format\n",
    "        self.training = training\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.batch_norm_config = batch_norm_config\n",
    "        self.conv_initializer = conv_initializer\n",
    "        self.adv_bn_init = adv_bn_init\n",
    "        # If no batch_normalization configuration provided, use a default one\n",
    "        if self.batch_norm_config is None:\n",
    "            self.batch_norm_config = {\n",
    "                'decay': 0.9,\n",
    "                'epsilon': 1e-4,\n",
    "                'scale': True,\n",
    "                'zero_debias_moving_mean': False,\n",
    "            }\n",
    "\n",
    "    def _conv2d(self, inputs, activation, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This function returns an operation for performing a 2D convolution operation over a given input.\n",
    "        \n",
    "        :param inputs: the tensor input data\n",
    "        :param activation: activation function to apply after the convolution operation, \n",
    "        if none then it uses linear activation\n",
    "        \"\"\"\n",
    "        x = tf.layers.conv2d(\n",
    "            inputs, data_format=self.data_format,\n",
    "            use_bias=not self.use_batch_norm,\n",
    "            kernel_initializer=self.conv_initializer,\n",
    "            activation=None if self.use_batch_norm else activation,\n",
    "            *args, **kwargs)\n",
    "        # Apply normalization on top of the convolution\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "            x = activation(x) if activation is not None else x\n",
    "        return x\n",
    "\n",
    "    def conv2d_linear_last_bn(self, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This function performs convolution with a linear activation \n",
    "        and batch normalization\n",
    "        \"\"\"\n",
    "        x = tf.layers.conv2d(\n",
    "            inputs, data_format=self.data_format,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=self.conv_initializer,\n",
    "            activation=None, *args, **kwargs)\n",
    "        param_initializers = {\n",
    "            'moving_mean': tf.zeros_initializer(),\n",
    "            'moving_variance': tf.ones_initializer(),\n",
    "            'beta': tf.zeros_initializer(),\n",
    "        }\n",
    "        if self.adv_bn_init:\n",
    "            param_initializers['gamma'] = tf.zeros_initializer()\n",
    "        else:\n",
    "            param_initializers['gamma'] = tf.ones_initializer()\n",
    "        x = self.batch_norm(x, param_initializers=param_initializers)\n",
    "        return x\n",
    "\n",
    "    def conv2d_linear(self, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        linear convolution without an activation function\n",
    "        \"\"\"\n",
    "        return self._conv2d(inputs, None, *args, **kwargs)\n",
    "\n",
    "    def conv2d(self, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        generic convolution with activation function\n",
    "        \"\"\"\n",
    "        return self._conv2d(inputs, self.activation, *args, **kwargs)\n",
    "\n",
    "    def pad2d(self, inputs, begin, end=None):\n",
    "        if end is None:\n",
    "            end = begin\n",
    "        try:\n",
    "            _ = begin[1]\n",
    "        except TypeError:\n",
    "            begin = [begin, begin]\n",
    "        try:\n",
    "            _ = end[1]\n",
    "        except TypeError:\n",
    "            end = [end, end]\n",
    "        if self.data_format == 'channels_last':\n",
    "            padding = [[0, 0], [begin[0], end[0]], [begin[1], end[1]], [0, 0]]\n",
    "        else:\n",
    "            padding = [[0, 0], [0, 0], [begin[0], end[0]], [begin[1], end[1]]]\n",
    "        return tf.pad(inputs, padding)\n",
    "\n",
    "    def max_pooling2d(self, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        max pooling layer over 2D inputs (images) (selects the max signal for each part it is slided over)\n",
    "        \"\"\"\n",
    "        return tf.layers.max_pooling2d(\n",
    "            inputs, data_format=self.data_format, *args, **kwargs)\n",
    "\n",
    "    def average_pooling2d(self, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        average pooling layer over 2D inputs (images) (averages the signals for each part it is slided over)\n",
    "        \"\"\"\n",
    "        return tf.layers.average_pooling2d(\n",
    "            inputs, data_format=self.data_format, *args, **kwargs)\n",
    "\n",
    "    def dense_linear(self, inputs, units, **kwargs):\n",
    "        return tf.layers.dense(inputs, units, activation=None)\n",
    "\n",
    "    def dense(self, inputs, units, **kwargs):\n",
    "        \"\"\"\n",
    "        This layer implements the operation: \n",
    "        outputs = activation(inputs.kernel + bias) \n",
    "        Where activation is the activation function passed as the activation argument (if not None), \n",
    "        kernel is a weights matrix created by the layer, \n",
    "        and bias is a bias vector created by the layer (only if use_bias is True).\n",
    "        I.e just a simple activation of sum of logits + bias, no convolution\n",
    "        \"\"\"\n",
    "        return tf.layers.dense(inputs, units, activation=self.activation)\n",
    "\n",
    "    def activate(self, inputs, activation=None):\n",
    "        \"\"\"\n",
    "        Applies an activation function (if any) over a set of inputs,\n",
    "        identity function if there is no activity function.\n",
    "        \"\"\"\n",
    "        activation = activation or self.activation\n",
    "        return activation(inputs) if activation is not None else inputs\n",
    "\n",
    "    def batch_norm(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Adds a batch normalization layer.\n",
    "        The normalization is over all but the last dimension if \n",
    "        data_format is NHWC and all but the second dimension if data_format is NCHW.\n",
    "        \n",
    "        Normalizing well could get better performance and converge quickly. Most of\n",
    "        time we will subtract mean value to make input mean to be zero to prevent \n",
    "        weights change same directions so that converge slowly\n",
    "        \n",
    "        Batch normalization potentially helps in two ways: faster learning and higher \n",
    "        overall accuracy. The improved method also allows you to use a higher \n",
    "        learning rate, potentially providing another boost in speed. \n",
    "        Why does this work? Well, we know that normalization (shifting inputs \n",
    "        to zero-mean and unit variance) is often used as a pre-processing step to \n",
    "        make the data comparable across features. As the data flows through a\n",
    "        deep network, the weights and parameters adjust those values, sometimes \n",
    "        making the data too big or too small again - a problem the authors refer \n",
    "        to as \"internal covariate shift\". By normalizing the data in each mini-batch, \n",
    "        this problem is largely avoided.\n",
    "        \n",
    "        Batch Normalization is a technique to provide any layer in a Neural Network \n",
    "        with inputs that are zero mean/unit variance\n",
    "        \"\"\"\n",
    "        all_kwargs = dict(self.batch_norm_config)\n",
    "        all_kwargs.update(kwargs)\n",
    "        data_format = 'NHWC' if self.data_format == 'channels_last' else 'NCHW'\n",
    "        return tf.contrib.layers.batch_norm(\n",
    "            inputs, is_training=self.training, data_format=data_format,\n",
    "            fused=True, **all_kwargs)\n",
    "\n",
    "    def spatial_average2d(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs average pooling over specific spatial location\n",
    "        \"\"\"\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        if self.data_format == 'channels_last':\n",
    "            n, h, w, c = shape\n",
    "        else:\n",
    "            n, c, h, w = shape\n",
    "        n = -1 if n is None else n\n",
    "        x = tf.layers.average_pooling2d(inputs, (h, w), (1, 1),\n",
    "                                        data_format=self.data_format)\n",
    "        return tf.reshape(x, [n, c])\n",
    "\n",
    "    def flatten2d(self, inputs):\n",
    "        \"\"\"\n",
    "        Flattens 2d inputs \n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        if self.data_format != 'channel_last':\n",
    "            # Note: This ensures the output order matches that of NHWC networks\n",
    "            x = tf.transpose(x, [0, 2, 3, 1])\n",
    "        input_shape = x.get_shape().as_list()\n",
    "        num_inputs = 1\n",
    "        for dim in input_shape[1:]:\n",
    "            num_inputs *= dim\n",
    "        return tf.reshape(x, [-1, num_inputs], name='flatten')\n",
    "\n",
    "    def residual2d(self, inputs, network, units=None, scale=1.0, activate=False):\n",
    "        \"\"\"\n",
    "        The Residual connection \n",
    "        \"\"\"\n",
    "        outputs = network(inputs)\n",
    "        c_axis = -1 if self.data_format == 'channels_last' else 1\n",
    "        h_axis = 1 if self.data_format == 'channels_last' else 2\n",
    "        w_axis = h_axis + 1\n",
    "        ishape, oshape = [y.get_shape().as_list() for y in [inputs, outputs]]\n",
    "        ichans, ochans = ishape[c_axis], oshape[c_axis]\n",
    "        strides = ((ishape[h_axis] - 1) // oshape[h_axis] + 1,\n",
    "                   (ishape[w_axis] - 1) // oshape[w_axis] + 1)\n",
    "        with tf.name_scope('residual'):\n",
    "            if (ochans != ichans or strides[0] != 1 or strides[1] != 1):\n",
    "                inputs = self.conv2d_linear(inputs, units, 1, strides, 'SAME')\n",
    "            x = inputs + scale * outputs\n",
    "            if activate:\n",
    "                x = self.activate(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet BuildingBlocks (grouped layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_bottleneck_v1(builder, inputs, depth, depth_bottleneck, stride,\n",
    "                         basic=False):\n",
    "    \"\"\"\n",
    "    Bottleneck residual unit\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.get_shape().as_list()[1]\n",
    "    x = inputs\n",
    "    with tf.name_scope('resnet_v1'):\n",
    "        if depth == num_inputs:\n",
    "            if stride == 1:\n",
    "                shortcut = x\n",
    "            else:\n",
    "                shortcut = builder.max_pooling2d(x, 1, stride)\n",
    "        else:\n",
    "            shortcut = builder.conv2d_linear(x, depth, 1, stride, 'SAME')\n",
    "        if basic:\n",
    "            x = builder.pad2d(x, 1)\n",
    "            x = builder.conv2d(x, depth_bottleneck, 3, stride, 'VALID')\n",
    "            x = builder.conv2d_linear(x, depth, 3, 1, 'SAME')\n",
    "        else:\n",
    "            x = builder.conv2d(x, depth_bottleneck, 1, 1, 'SAME')\n",
    "            x = builder.conv2d(x, depth_bottleneck, 3, stride, 'SAME')\n",
    "            # x = builder.conv2d_linear(x, depth,            1, 1,      'SAME')\n",
    "            x = builder.conv2d_linear_last_bn(x, depth, 1, 1, 'SAME')\n",
    "        x = tf.nn.relu(x + shortcut)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the Building Blocks and All of the Layers Together Into A Complete Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_resnet_v1_impl(builder, inputs, layer_counts, basic=False):\n",
    "    \"\"\"\n",
    "    Build the complete resnet from a input operation and a list of layers\n",
    "    \"\"\"\n",
    "    x = inputs\n",
    "    x = builder.pad2d(x, 3)\n",
    "    x = builder.conv2d(x, 64, 7, 2, 'VALID')\n",
    "    x = builder.max_pooling2d(x, 3, 2, 'SAME')\n",
    "    for i in range(layer_counts[0]):\n",
    "        x = resnet_bottleneck_v1(builder, x, 256, 64, 1, basic)\n",
    "    for i in range(layer_counts[1]):\n",
    "        x = resnet_bottleneck_v1(builder, x, 512, 128, 2 if i == 0 else 1, basic)\n",
    "    for i in range(layer_counts[2]):\n",
    "        x = resnet_bottleneck_v1(builder, x, 1024, 256, 2 if i == 0 else 1, basic)\n",
    "    for i in range(layer_counts[3]):\n",
    "        x = resnet_bottleneck_v1(builder, x, 2048, 512, 2 if i == 0 else 1, basic)\n",
    "    return builder.spatial_average2d(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_resnet_v1(inputs, nlayer, data_format='channels_last',\n",
    "                        training=False, conv_initializer=None, adv_bn_init=False):\n",
    "    \"\"\"Deep Residual Networks family of models\n",
    "    https://arxiv.org/abs/1512.03385\n",
    "    Infer the complete ResNet architecture based on parameters, e.g resnet18,34,50,101, or 152\n",
    "    and then build the corresponding network and return\n",
    "    \"\"\"\n",
    "    builder = LayerBuilder(tf.nn.relu, data_format, training, use_batch_norm=True,\n",
    "                           conv_initializer=conv_initializer, adv_bn_init=adv_bn_init)\n",
    "    if nlayer == 18:\n",
    "        return inference_resnet_v1_impl(builder, inputs, [2, 2, 2, 2], basic=True)\n",
    "    elif nlayer == 34:\n",
    "        return inference_resnet_v1_impl(builder, inputs, [3, 4, 6, 3], basic=True)\n",
    "    elif nlayer == 50:\n",
    "        return inference_resnet_v1_impl(builder, inputs, [3, 4, 6, 3])\n",
    "    elif nlayer == 101:\n",
    "        return inference_resnet_v1_impl(builder, inputs, [3, 4, 23, 3])\n",
    "    elif nlayer == 152:\n",
    "        return inference_resnet_v1_impl(builder, inputs, [3, 8, 36, 3])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid nlayer (%i); must be one of: 18,34,50,101,152\" %\n",
    "                         nlayer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_func(model_name):\n",
    "    \"\"\"\n",
    "    Get a function representing the model from the model_name (resnet18,34,50,101,152) and return it\n",
    "    \"\"\"\n",
    "    if model_name.startswith('resnet'):\n",
    "        nlayer = int(model_name[len('resnet'):])\n",
    "        return lambda images, *args, **kwargs: \\\n",
    "            inference_resnet_v1(images, nlayer, *args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type: %s\" % model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse TFRecords from HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfr(example_proto):\n",
    "    \"\"\"\n",
    "    Parses an example protocol buffer (TFRecord) into a dict of\n",
    "    feature names and tensors\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'label_one_hot': tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        'image_raw': tf.FixedLenFeature((), tf.string, default_value=\"\")\n",
    "    }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return parsed_features[\"image_raw\"], parsed_features[\"label_one_hot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bytes(image, label):\n",
    "    \"\"\"\n",
    "    Decode the bytes that was serialized in the TFRecords in HopsFS to tensors so that we can apply \n",
    "    image preprocessing\n",
    "    \"\"\"\n",
    "    image_tensor = tf.decode_raw(image, tf.uint8),\n",
    "    label_tensor = tf.decode_raw(label, tf.uint8),\n",
    "    label_tensor = tf.cast(label_tensor, tf.int32)\n",
    "    image_tensor = tf.reshape(image_tensor, [64,64,3]) #dimension information was lost when serializing to disk\n",
    "    return image_tensor,label_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(filenames,batch_size, training=False):\n",
    "    \"\"\"\n",
    "    Creates a dataset of examples to use for training or inference in Tensorflow, reads TFRecords from HopsFS\n",
    "    \"\"\"\n",
    "    num_readers = os.cpu_count()\n",
    "    \n",
    "    # Train Dataset is already shuffled and augmented\n",
    "    ds = tf.data.TFRecordDataset(filenames,\n",
    "        compression_type=None,    \n",
    "        buffer_size=100240, \n",
    "        num_parallel_reads=num_readers) # Parallel read from HopsFS\n",
    "    \n",
    "    # Parse the binary TFR format into dataset\n",
    "    ds = ds.map(parse_tfr)\n",
    "    # Decode the bytestrings into tensors\n",
    "    ds = ds.map(decode_bytes)\n",
    "    ds = ds.batch(batch_size)\n",
    "    if(training):\n",
    "        ds = ds.repeat() # So that we can iterate infinite times, let the model decide when to stop based on num_epochs\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(tensors):\n",
    "    \"\"\"\n",
    "    Stages the given tensors in a StagingArea for asynchronous put/get.\n",
    "    \"\"\"\n",
    "    # Creates the staging area\n",
    "    stage_area = data_flow_ops.StagingArea(\n",
    "        dtypes=[tensor.dtype for tensor in tensors],\n",
    "        shapes=[tensor.get_shape() for tensor in tensors])\n",
    "    # Operation for inserting a batch of tensors into the staging area\n",
    "    put_op = stage_area.put(tensors)\n",
    "    # Operation for extracting the first tensor from the staging area\n",
    "    get_tensors = stage_area.get()\n",
    "    # Create a tensorflow collection of all tensors inserted to the staging area (a general purpose key/data storage)\n",
    "    tf.add_to_collection('STAGING_AREA_PUTS', put_op)\n",
    "    # Return the two operations for interacting with the staging area\n",
    "    return put_op, get_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring the Execution Using Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefillStagingAreasHook(tf.train.SessionRunHook):\n",
    "    \"\"\"\n",
    "    A hook for prefilling the staging area\n",
    "    \"\"\"\n",
    "    def after_create_session(self, session, coord):\n",
    "        \"\"\"\n",
    "        This function is called right after the TF Session is created\n",
    "        and execution is about to start. It will prefill the staging area\n",
    "        with the next batch\n",
    "        \"\"\"\n",
    "        enqueue_ops = tf.get_collection('STAGING_AREA_PUTS')\n",
    "        for i in range(len(enqueue_ops)):\n",
    "            session.run(enqueue_ops[:i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSessionRunHook(tf.train.SessionRunHook):\n",
    "    \"\"\"\n",
    "    A hook for logging during the execution\n",
    "    \"\"\"\n",
    "    def __init__(self, global_batch_size, num_records, display_every=10):\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.num_records = num_records\n",
    "        self.display_every = display_every\n",
    "\n",
    "    def after_create_session(self, session, coord):\n",
    "        \"\"\"\n",
    "        This function is called right after the TF Session is created.\n",
    "        Initializes counters\n",
    "        \"\"\"\n",
    "        self.elapsed_secs = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def before_run(self, run_context):\n",
    "        \"\"\"\n",
    "        This function is called right before each call to sess.run()\n",
    "        it returns a RunArgs object that will be added as arguments\n",
    "        to sess.run(). In this case we parameterize the run function\n",
    "        with the global step and some logging information.\n",
    "        \n",
    "        :param run_context: contains information about the upcoming run() method call \n",
    "        \"\"\"\n",
    "        self.t0 = time.time()\n",
    "        return tf.train.SessionRunArgs(\n",
    "            fetches=[tf.train.get_global_step(),\n",
    "                     'loss:0', 'total_loss:0', 'learning_rate:0', \"top1acc:0\", \"top5acc:0\"])\n",
    "\n",
    "    def after_run(self, run_context, run_values):\n",
    "        \"\"\"\n",
    "        This function is called after each call to sess.run().\n",
    "        It will collect some statistics about the run() and log it.\n",
    "        \n",
    "        :param run_context: contains information about the upcoming run() method call\n",
    "        :param run_values: contains results of requested tensors/ops by before_run()\n",
    "        \"\"\"\n",
    "        self.elapsed_secs += time.time() - self.t0\n",
    "        self.count += 1\n",
    "        global_step, loss, total_loss, lr, top1acc, top5acc = run_values.results\n",
    "        if global_step == 1 or global_step % self.display_every == 0:\n",
    "            dt = self.elapsed_secs / self.count\n",
    "            img_per_sec = self.global_batch_size / dt\n",
    "            epoch = global_step * self.global_batch_size / self.num_records\n",
    "            print('Training Stats: | Step: {} | Epoch: {} | Speed(img/sec): {} | Loss: {} | TotalLoss: {} | LR: {} | Top1Acc: {} | Top5Acc: {} |'.format(global_step, epoch, img_per_sec, loss, total_loss, lr, top1acc[0], top5acc[0]))\n",
    "            self.elapsed_secs = 0.\n",
    "            self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training with Reduced Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fp32_trainvar_getter(getter, name, shape=None, dtype=None,\n",
    "                          trainable=True, regularizer=None,\n",
    "                          *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Cast variables to fp16 (half the precision) if running with mixed precision.\n",
    "    \"\"\"\n",
    "    storage_dtype = tf.float32 if trainable else dtype\n",
    "    variable = getter(name, shape, dtype=storage_dtype,\n",
    "                      trainable=trainable,\n",
    "                      regularizer=regularizer if trainable and 'BatchNorm' not in name else None,\n",
    "                      *args, **kwargs)\n",
    "    if trainable and dtype != tf.float32:\n",
    "        cast_name = name + '/fp16_cast'\n",
    "        try:\n",
    "            cast_variable = tf.get_default_graph().get_tensor_by_name(\n",
    "                cast_name + ':0')\n",
    "        except KeyError:\n",
    "            cast_variable = tf.cast(variable, dtype, name=cast_name)\n",
    "        cast_variable._ref = variable._ref\n",
    "        variable = cast_variable\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp32_trainable_vars(name='fp32_vars', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A varible scope with custom variable getter to convert fp16 trainable\n",
    "    variables with fp32 storage followed by fp16 cast.\n",
    "    \"\"\"\n",
    "    return tf.variable_scope(\n",
    "        name, custom_getter=_fp32_trainvar_getter, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecisionOptimizer(tf.train.Optimizer):\n",
    "    \"\"\"\n",
    "    An optimizer that updates trainable variables in fp32.\n",
    "    Reduced precision training can save memory capacity, memory bandwidth, memory power, \n",
    "    and arithmetic power by using smaller numbers. FP16 works with little effort: 2x\n",
    "    gain in memory, 4x in multiply power. With care, one can use: 8b for convolutions, \n",
    "    4b for fully-connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer,\n",
    "                 scale=None,\n",
    "                 name=\"MixedPrecisionOptimizer\",\n",
    "                 use_locking=False):\n",
    "        super(MixedPrecisionOptimizer, self).__init__(\n",
    "            name=name, use_locking=use_locking)\n",
    "        self._optimizer = optimizer\n",
    "        self._scale = float(scale) if scale is not None else 1.0\n",
    "\n",
    "    def compute_gradients(self, loss, var_list=None, *args, **kwargs):\n",
    "        if var_list is None:\n",
    "            var_list = (\n",
    "                    tf.trainable_variables() +\n",
    "                    tf.get_collection(tf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n",
    "\n",
    "        replaced_list = var_list\n",
    "\n",
    "        if self._scale != 1.0:\n",
    "            loss = tf.scalar_mul(self._scale, loss)\n",
    "\n",
    "        gradvar = self._optimizer.compute_gradients(loss, replaced_list, *args, **kwargs)\n",
    "\n",
    "        final_gradvar = []\n",
    "        for orig_var, (grad, var) in zip(var_list, gradvar):\n",
    "            if var is not orig_var:\n",
    "                grad = tf.cast(grad, orig_var.dtype)\n",
    "            if self._scale != 1.0:\n",
    "                grad = tf.scalar_mul(1. / self._scale, grad)\n",
    "            final_gradvar.append((grad, orig_var))\n",
    "\n",
    "        return final_gradvar\n",
    "    \n",
    "    def apply_gradients(self, *args, **kwargs):\n",
    "        return self._optimizer.apply_gradients(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get New Learning Rate After Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(lr, steps, lr_steps, warmup_it, decay_steps, global_step, lr_decay_mode):\n",
    "    \"\"\" Gets the new learning rate after decay\"\"\"\n",
    "    if lr_decay_mode == 'steps':\n",
    "        learning_rate = tf.train.piecewise_constant(global_step,\n",
    "                                                    steps, lr_steps)\n",
    "    elif lr_decay_mode == 'poly':\n",
    "        learning_rate = tf.train.polynomial_decay(lr,\n",
    "                                                  global_step - warmup_it,\n",
    "                                                  decay_steps=decay_steps - warmup_it,\n",
    "                                                  end_learning_rate=0.00001,\n",
    "                                                  power=2,\n",
    "                                                  cycle=False)\n",
    "    else:\n",
    "        raise ValueError('Invalid type of lr_decay_mode')\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_decay(warmup_lr, global_step, warmup_steps, warmup_end_lr):\n",
    "    \"\"\"\n",
    "    Decay learning rate during warmup iterations\n",
    "    \"\"\"\n",
    "    from tensorflow.python.ops import math_ops\n",
    "    p = tf.cast(global_step, tf.float32) / tf.cast(warmup_steps, tf.float32)\n",
    "    diff = math_ops.subtract(warmup_end_lr, warmup_lr)\n",
    "    res = math_ops.add(warmup_lr, math_ops.multiply(diff, p))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_function(features, labels, mode, params):\n",
    "    \"\"\"\n",
    "    Performs the Training/Evaluation by Running the Model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Estimator Mode: {}\".format(tf.estimator.ModeKeys.PREDICT))\n",
    "    \n",
    "    # Extract parameters from dict\n",
    "    lr = params['lr']\n",
    "    lr_steps = params['lr_steps']\n",
    "    steps = params['steps']\n",
    "    lr_decay_mode = params['lr_decay_mode']\n",
    "    decay_steps = params['decay_steps']\n",
    "    model_name = params['model']\n",
    "    num_classes = params['n_classes']\n",
    "    model_dtype = get_with_default(params, 'dtype', tf.float32)\n",
    "    model_format = get_with_default(params, 'format', 'channels_last')\n",
    "    device = get_with_default(params, 'device', '/gpu:0')\n",
    "    model_func = get_model_func(model_name)\n",
    "    inputs = features  # TODO: Should be using feature columns?\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    momentum = params['mom']\n",
    "    weight_decay = params['wdecay']\n",
    "    warmup_lr = params['warmup_lr']\n",
    "    warmup_it = params['warmup_it']\n",
    "    loss_scale = params['loss_scale']\n",
    "    adv_bn_init = params['adv_bn_init']\n",
    "    conv_init = params['conv_init']\n",
    "\n",
    "    # Stage batch in staging area\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        with tf.device('/cpu:0'):\n",
    "            preload_op, (inputs, labels) = stage([inputs, labels])\n",
    "            image = tf.reshape(features[:10], [-1, 64,64,3])\n",
    "            tf.summary.image('image', image)\n",
    "            \n",
    "    with tf.device(None):\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            gpucopy_op, (inputs, labels) = stage([inputs, labels])\n",
    "        \n",
    "        # Normalize images \n",
    "        inputs = tf.cast(inputs, model_dtype)\n",
    "        imagenet_mean = np.array([121, 115, 100], dtype=np.float32)\n",
    "        imagenet_std = np.array([70, 68, 71], dtype=np.float32)\n",
    "        inputs = tf.subtract(inputs, imagenet_mean)\n",
    "        inputs = tf.multiply(inputs, 1. / imagenet_std)\n",
    "        \n",
    "        if model_format == 'channels_first':\n",
    "            inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "        \n",
    "        #Compute Logits    \n",
    "        with fp32_trainable_vars(\n",
    "                regularizer=tf.contrib.layers.l2_regularizer(weight_decay)):\n",
    "            # Get reference to top layer of the entire network from the model func\n",
    "            top_layer = model_func(\n",
    "                inputs, data_format=model_format, training=is_training,\n",
    "                conv_initializer=conv_init, adv_bn_init=adv_bn_init)\n",
    "            # Compute the output logits\n",
    "            logits = tf.layers.dense(top_layer, num_classes,\n",
    "                                     kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        # Get prediction by taking max of logits\n",
    "        predicted_classes = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        logits = tf.cast(logits, tf.float32)\n",
    "        # If performing prediction, we dont need to do optimization or compute the loss, we can return just\n",
    "        # the logits and the predictions \n",
    "        # (EstimatorSpec is a nobject that defines the model that the estimator will run)\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            with tf.device(None):\n",
    "                probabilities = tf.nn.softmax(logits) #Get probabilities by using softmax (sum to 1)\n",
    "                predictions = {\n",
    "                    'class_ids': predicted_classes[:, None],\n",
    "                    'probabilities': probabilities,\n",
    "                    'logits': logits\n",
    "                }\n",
    "                return tf.estimator.EstimatorSpec(mode, predictions=predictions,export_outputs={\n",
    "                    'predict': tf.estimator.export.PredictOutput(predictions)})\n",
    "        # If training, compute loss\n",
    "        loss = tf.losses.softmax_cross_entropy(\n",
    "            logits=logits, onehot_labels=labels)\n",
    "        loss = tf.identity(loss, name='loss')  # For access by logger (TODO: Better way to access it?)\n",
    "        \n",
    "        # accuracies\n",
    "        with tf.device(None):  # Allow fallback to CPU if no GPU support for these ops\n",
    "            top1acc = tf.metrics.accuracy(\n",
    "                        labels=tf.argmax(labels,axis=1), predictions=predicted_classes)\n",
    "            top5acc = tf.metrics.mean(\n",
    "                        tf.cast(tf.nn.in_top_k(logits, tf.argmax(labels, axis=1), 5), tf.float32))\n",
    "            top1acc = tf.identity(top1acc, name='top1acc') # for access by the logger\n",
    "            top5acc = tf.identity(top5acc, name='top5acc') # for access  by the logger\n",
    "            \n",
    "        # If performing evaluation, compute accuracies and return the EstimatorSpec with the operations\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            with tf.device(None):\n",
    "                top1acc = tf.metrics.accuracy(\n",
    "                            labels=tf.argmax(labels,axis=1), predictions=predicted_classes)\n",
    "                top5acc = tf.metrics.mean(\n",
    "                            tf.cast(tf.nn.in_top_k(logits, tf.argmax(labels, axis=1), 5), tf.float32))\n",
    "                metrics = {'val-top1acc': top1acc, 'val-top5acc': top5acc}\n",
    "            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        assert (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        total_loss = tf.add_n([loss] + reg_losses, name='total_loss')\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        global_step = tf.train.get_global_step()\n",
    "        #global_step = tf.identity(global_step, name='global_step')  # For access by logger (TODO: Better way to access it?)\n",
    "        \n",
    "        # Decay learning rate\n",
    "        with tf.device('/cpu:0'):  # Allow fallback to CPU if no GPU support for these ops\n",
    "            learning_rate = tf.cond(global_step < warmup_it,\n",
    "                                    lambda: warmup_decay(warmup_lr, global_step, warmup_it,\n",
    "                                                         lr),\n",
    "                                    lambda: get_lr(lr, steps, lr_steps, warmup_it, decay_steps, global_step,\n",
    "                                                   lr_decay_mode))\n",
    "            learning_rate = tf.identity(learning_rate, 'learning_rate')\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "        \n",
    "        # Define the optimizer for training (performing backprop)\n",
    "        # Momentum is a trick to power-through local minimas during training\n",
    "        # Momentum-based acceleration schemes increase the speed of learning and damp oscillations \n",
    "        # in directions of high curvature\n",
    "        opt = tf.train.MomentumOptimizer(\n",
    "            learning_rate, momentum, use_nesterov=True)\n",
    "        # Wrap the MomentumOptimizer in a MixedPrecision one\n",
    "        opt = MixedPrecisionOptimizer(opt, scale=loss_scale)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) or []\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            gate_gradients = (tf.train.Optimizer.GATE_NONE)\n",
    "            # Training step\n",
    "            train_op = opt.minimize(\n",
    "                total_loss, global_step=tf.train.get_global_step(),\n",
    "                gate_gradients=gate_gradients)\n",
    "        train_op = tf.group(preload_op, gpucopy_op, train_op)  # , update_ops)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=total_loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_load_ckpts(log_dir,local_logdir=False):\n",
    "    \"\"\"\n",
    "    Collects all ckpts from the log dir and stores them in a sorted list that is returned\n",
    "    \"\"\"\n",
    "    ckpts = []\n",
    "    print(\"looking for checkpoints in : {}\".format(log_dir))\n",
    "    # Use pydoop to read logs\n",
    "    if not local_logdir:\n",
    "        for f in py_hdfs.ls(log_dir):\n",
    "            log_dir = log_dir.replace(\"//Logs\", \"/Logs\") #hack to fix in case hops-py-util path includes double slashes\n",
    "            if log_dir.endswith(\"/\"):\n",
    "                f1 = f.replace(log_dir, \"\")\n",
    "            else:\n",
    "                f1 = f.replace(log_dir + \"/\", \"\")\n",
    "            m = re.match(r'model.ckpt-([0-9]+).index', f1)\n",
    "            if m is None:\n",
    "                continue\n",
    "            ckpts.append({'step': int(m.group(1)),\n",
    "                          'path': f.replace(\".index\",\"\"),\n",
    "                          'mtime': py_hdfs.stat(f).st_mtime,\n",
    "                          })\n",
    "    # Use os module to read locally\n",
    "    else:\n",
    "        for f in os.listdir(log_dir):\n",
    "            m = re.match(r'model.ckpt-([0-9]+).index', f)\n",
    "            if m is None:\n",
    "                continue\n",
    "            fullpath = os.path.join(log_dir, f)\n",
    "            ckpts.append({'step': int(m.group(1)),\n",
    "                          'path': os.path.splitext(fullpath)[0],\n",
    "                          'mtime': os.stat(fullpath).st_mtime,\n",
    "                          })\n",
    "    ckpts.sort(key=itemgetter('step'))\n",
    "    return ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_and_size():\n",
    "    \"\"\"\n",
    "    A function for obtaining all the resting TFRecord files to be parsed.\n",
    "    \"\"\"\n",
    "    # Convert regular expression file pattern into a list of files (tfrecords files)\n",
    "    train_tfr_file = tf.gfile.Glob(INPUT_DIR + \"train.tfrecords\")\n",
    "    val_tfr_file = tf.gfile.Glob(INPUT_DIR + \"val.tfrecords\")\n",
    "    test_tfr_file = tf.gfile.Glob(INPUT_DIR + \"test.tfrecords\")\n",
    "    \n",
    "    # Read sizes.txt that to get the size of the datasets (used for correct shuffling)\n",
    "    with py_hdfs.open(SIZES_FILE, 'r') as f:\n",
    "        file_lines = f.read().decode(\"utf-8\").split(\"\\n\")    \n",
    "    train_size = int(file_lines[0].split(\",\")[1])\n",
    "    val_size = int(file_lines[1].split(\",\")[1])\n",
    "    test_size = int(file_lines[2].split(\",\")[1])\n",
    "    \n",
    "    return train_tfr_file, val_tfr_file, test_tfr_file, train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_with_default(obj, key, default_value):\n",
    "    \"\"\" \n",
    "    Get dict value if exists othewise return default\n",
    "    \"\"\"\n",
    "    return obj[key] if key in obj and obj[key] is not None else default_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model\n",
    "\n",
    "To prepare a trained Estimator for serving, we must export it in the standard SavedModel format.\n",
    "\n",
    "During training, an input_fn() ingests data and prepares it for use by the model. At serving time, similarly, a serving_input_receiver_fn() accepts inference requests and prepares them for the model. This function has the following purposes:\n",
    "\n",
    "- To add placeholders to the graph that the serving system will feed with inference requests.\n",
    "- To add any additional ops needed to convert data from the input format into the feature Tensors expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"\n",
    "     At serving time, a serving_input_receiver_fn() accepts inference requests \n",
    "     and prepares them for the model. This function has the following purposes:\n",
    "     To add placeholders to the graph that the serving system will feed with inference requests.\n",
    "     To add any additional ops needed to convert data from the input format into the feature \n",
    "     Tensors expected by the model.\n",
    "    \"\"\"\n",
    "    features = tf.placeholder(dtype=tf.uint8, shape=[DEFAULT_BATCH_SIZE, 64, 64, 3], name='input_tensor')\n",
    "    # Has to be a TensorServingInputReceiver rather than a ServingInputReceiver since our model function\n",
    "    # takes raw tensors rather than a dict of features as input\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features=features, receiver_tensors=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_estimator(estimator, export_dir_base):\n",
    "    \"\"\"\n",
    "    Exports the trained estimator to be used for serving\n",
    "    \"\"\"\n",
    "    # This method builds a new graph by first calling the serving_input_receiver_fn \n",
    "    # to obtain feature Tensors, and then calling this Estimator's model_fn to generate \n",
    "    # the model graph based on those features. It restores the given checkpoint \n",
    "    # (or, lacking that, the most recent checkpoint) into this graph in a fresh session. \n",
    "    # Finally it creates a timestamped export directory below the given export_dir_base, \n",
    "    # and writes a SavedModel into it containing a single MetaGraphDef saved from this session.\n",
    "    estimator.export_savedmodel(export_dir_base=export_dir_base, serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Orchestrating the Experiment (Create Estimator, Train, Eval, Log, Checkpoint Etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=\"resnet18\",lr=0.01, mom=0.90, wdecay=0.0001, lr_decay_mode=\"poly\"):\n",
    "    \"\"\"\n",
    "    Pipeline entrypoint, this function will set up the network, train it, evaluate it, and store the results.\n",
    "    \n",
    "    Hyperparameters for distributed hyperparameter tuning:\n",
    "    \n",
    "    :param model: Name of model to run: resnet[18,34,50,101,152]\n",
    "    :param lr: Starting learning rate\n",
    "    :param mom: momentum\n",
    "    :param wdecay: weight decay\n",
    "    :param lr_decay_mode: Takes either `steps` (decay by a factor at specified steps) or `poly`(polynomial_decay with degree 2)\n",
    "    \"\"\"\n",
    "    import hops\n",
    "    print(\"hopsversion: {}\".format(hops.__version__))\n",
    "    data_dir = INPUT_DIR # Path to dataset in TFRecordsFormat \n",
    "    batch_size=100 # Size of each minibatch per GPU\n",
    "    num_epochs=50 # Number of epochs to run\n",
    "    log_dir=LOG_DIR # Directory in which to write training summaries and checkpoints.\n",
    "    log_name=\"tinyimagenet_resnet.log\" # Name of the log\n",
    "    display_every=500 # How often (in iterations) to print out running information.\n",
    "    evaluate=False # Evaluate the top-1 and top-5 accuracy of the latest checkpointed model.\n",
    "    evaluate_interval=1 # Evaluate accuracy per eval_interval number of epochs\n",
    "    fp16=False # Train using float16 (half) precision instead of float32.\n",
    "    warmup_lr=.001 # Warmup starting from this learning rate\n",
    "    warmup_epochs=0 # Number of epochs to warmup with given lr\n",
    "    lr_decay_factor=0.1 # learning rate decayed by this factor at each step. Used when lr_decay_mode is steps. Needs to be given with lr_decay_steps\n",
    "    lr_decay_steps='10,20,40' #  epoch numbers at which lr is decayed by lr_decay_factor. Used when lr_decay_mode is steps\n",
    "    loss_scale=1024 # loss scale\n",
    "    num_gpus=1 # Specify total number of GPUS used to train a checkpointed model during eval. Used only to calculate epoch number to print during evaluation\n",
    "    save_checkpoints_steps=23000 # after how many steps to save the TF checkpoint\n",
    "    save_summary_steps=0 # after how many steps to save the summary for tensorboard\n",
    "    adv_bn_init=False # init gamme of the last BN of each ResMod at 0.\n",
    "    adv_conv_init=False\n",
    "    local_logdir = False\n",
    "    \n",
    "    \n",
    "    # Set OS Environmentvariables for GPUs\n",
    "    gpu_thread_count = 2\n",
    "    os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "    os.environ['TF_GPU_THREAD_COUNT'] = str(gpu_thread_count)\n",
    "    os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "    \n",
    "    # Write checkpoints and logs so that Hops Tensorboard can access them\n",
    "    from hops import tensorboard\n",
    "    log_dir = tensorboard.logdir()\n",
    "    \n",
    "    # Get files and sizes for Train/Val/Test TFRecords, we do not have lables for test_tfr_file\n",
    "    train_tfr_file, val_tfr_file, test_tfr_file, train_size, val_size, test_size = get_filenames_and_size()\n",
    "    \n",
    "    #training_samples_per_rank = num_training_samples\n",
    "    #TinyImageNet have images downsampled from 224x224 to 64x64\n",
    "    height, width = 64, 64\n",
    "    global_batch_size = batch_size\n",
    "\n",
    "    # Get the number of training steps: num_examples*epochs(complete pass over dataset)/Batch_size (how many examples in each step)\n",
    "    nstep = train_size * num_epochs // global_batch_size\n",
    "    decay_steps = nstep\n",
    "    nstep_per_epoch = train_size // global_batch_size\n",
    "\n",
    "    # If decay mode is steps, then the learning rate is decayed by a static factor every x number of training steps\n",
    "    if lr_decay_mode == 'steps':\n",
    "        steps = [int(x) * nstep_per_epoch for x in lr_decay_steps.split(',')]\n",
    "        lr_steps = [lr]\n",
    "        for i in range(len(lr_decay_steps.split(','))):\n",
    "            lr_steps.append(lr * pow(lr_decay_factor, i + 1))\n",
    "    # Else we use polynomial decay of the learningrate (learning rate decayed by a polynomial rate)\n",
    "    else:\n",
    "        steps = []\n",
    "        lr_steps = []\n",
    "\n",
    "    if not save_checkpoints_steps:\n",
    "        # default to save one checkpoint per epoch\n",
    "        save_checkpoints_steps = nstep_per_epoch\n",
    "    if not save_summary_steps:\n",
    "        # default to save one checkpoint per epoch\n",
    "        save_summary_steps = nstep_per_epoch\n",
    "\n",
    "    # How many warmup iterations\n",
    "    warmup_it = nstep_per_epoch * warmup_epochs\n",
    "\n",
    "    print('PY' + str(sys.version) + 'TF' + str(tf.__version__))\n",
    "    \n",
    "    # Create a ProtoBuf message with GPU configuration to send out to all\n",
    "    # nodes/gpus\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.force_gpu_compatible = True  # Force pinned memory\n",
    "    config.intra_op_parallelism_threads = 1  # Avoid pool of Eigen threads\n",
    "    config.inter_op_parallelism_threads = 5 # Allow to use thread parallelism to deal with independent tasks\n",
    "    config.allow_soft_placement = True # Allow fallback to CPU if GPU is missing\n",
    "    \n",
    "    do_checkpoint = True\n",
    "    \n",
    "    # Creating the ResNet classifier using the Estimator API\n",
    "    # We are using a custom estimator rather than a pre-made one, \n",
    "    # which means we need to supply a model_function (cnn_model_function)\n",
    "    save_summary_steps=save_summary_steps if do_checkpoint else None\n",
    "    save_checkpoints_steps=save_checkpoints_steps if do_checkpoint else None\n",
    "    print(\"Creating Classifier with model: {}, steps: {}, lr_steps: {}, lr_decay_mode: {}, save_summary_steps: {}, save_checkpoints_steps: {}\".format(model, steps, lr_steps, lr_decay_mode,save_summary_steps, \n",
    "                                          save_checkpoints_steps))\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=cnn_model_function,\n",
    "        model_dir=log_dir, #logdir from hops, update\n",
    "        params={\n",
    "            'model': model,\n",
    "            'decay_steps': decay_steps,\n",
    "            'n_classes': 200,\n",
    "            'dtype': tf.float16 if fp16 else tf.float32,\n",
    "            'format': 'channels_last',\n",
    "            'device': '/gpu:0', #remove maybe?\n",
    "            'lr': lr,\n",
    "            'mom': mom,\n",
    "            'wdecay': wdecay,\n",
    "            'steps': steps,\n",
    "            'lr_steps': lr_steps,\n",
    "            'lr_decay_mode': lr_decay_mode,\n",
    "            'warmup_it': warmup_it,\n",
    "            'warmup_lr': warmup_lr,\n",
    "            'loss_scale': loss_scale,\n",
    "            'adv_bn_init': adv_bn_init,\n",
    "            'conv_init': tf.variance_scaling_initializer() if adv_conv_init else None,\n",
    "        },\n",
    "        config=tf.estimator.RunConfig(\n",
    "            session_config=config,\n",
    "            save_summary_steps=save_summary_steps,\n",
    "            save_checkpoints_steps=save_checkpoints_steps,\n",
    "            keep_checkpoint_max=None))\n",
    "\n",
    "    # Start training if we are not running only evaluation\n",
    "    if not evaluate:\n",
    "        \n",
    "        # Estimator API hides a lot of details about the model, which\n",
    "        # makes the code cleaner but you have less control over the \n",
    "        # session and execution of the model.\n",
    "        # To be able to track execution while it is running, we use \n",
    "        # hooks. PrefillStagingAreasHook is a hook for pre-fetching\n",
    "        # tensors before starting execution on the GPU\n",
    "        training_hooks = [PrefillStagingAreasHook()]\n",
    "        # LogHook used for logging during execution\n",
    "        training_hooks.append(\n",
    "            LogSessionRunHook(global_batch_size,\n",
    "                              train_size,\n",
    "                              display_every))\n",
    "        try:\n",
    "            # Starts the training of the classifier, feeding in data from\n",
    "            # make_dataset functin\n",
    "            start_time = time.time()\n",
    "            print(\"Start training, batch-size: {}, num-examples: {}, num-epochs: {}, num-steps: {}, num-step-per-epoch: {}, lr: {}, mom: {}, wdecay: {}\".format(batch_size, train_size, num_epochs, nstep, nstep_per_epoch, lr, mom, wdecay))\n",
    "            classifier.train(\n",
    "                input_fn=lambda: make_dataset(\n",
    "                    train_tfr_file,\n",
    "                    batch_size, training=True), #batch_size\n",
    "                steps=nstep, #nsteps\n",
    "                hooks=training_hooks)\n",
    "            with tf.device(None):  # Allow fallback to CPU if no GPU support for these ops\n",
    "                    export_estimator(classifier, EXPORT_MODEL_DIR)\n",
    "            print(\"Finished in {}\".format(time.time() - start_time))\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "            \n",
    "    final_accuracy_top1 = 0.0\n",
    "    # Evaluation after training \n",
    "    if True:\n",
    "        print(\"Evaluating\")\n",
    "        print(\"Validation dataset size: {}\".format(val_size))\n",
    "        barrier = tf.constant(0, dtype=tf.float32)\n",
    "        tf.Session(config=config).run(barrier)\n",
    "        time.sleep(5)  # a little extra margin...\n",
    "        validation_step_top1_top5_loss = []\n",
    "        validation_step_top1_top5_loss.append(\"step,top1acc,top5acc,loss\")\n",
    "        # Read the stored checkpoints and collect the accuracies for evaluation\n",
    "        try:\n",
    "            ckpts = sort_and_load_ckpts(log_dir)\n",
    "            print(\"Number of model checkpoints to evaluate on the validation dataset: {}\".format(len(ckpts)))\n",
    "            for i, c in enumerate(ckpts):\n",
    "                if i < len(ckpts) - 1:\n",
    "                    if (not evaluate_interval) or (i % evaluate_interval != 0):\n",
    "                        continue\n",
    "                # Evaluate the trained classifier based on each checkpoint by using the validation dataset, \n",
    "                # the validation dataset is read from HopsFS using make_dataset\n",
    "                # the model weights are read from HopsFS\n",
    "                print(\"running evaluation on the validation set (size: {}) with the checkpoint: {}\".format(val_size, c[\"path\"]))\n",
    "                eval_result = classifier.evaluate(\n",
    "                    input_fn=lambda: make_dataset(\n",
    "                        val_tfr_file,\n",
    "                        batch_size),\n",
    "                    checkpoint_path=c['path'])\n",
    "                print(\"eval_result on validation set: {}\".format(eval_result))\n",
    "                # save the validation results in the checkpoint\n",
    "                c['epoch'] = (c['step'] * num_gpus) / (nstep_per_epoch)\n",
    "                c['top1'] = eval_result['val-top1acc']\n",
    "                c['top5'] = eval_result['val-top5acc']\n",
    "                c['loss'] = eval_result['loss']\n",
    "            barrier = tf.constant(0, dtype=tf.float32)\n",
    "            \n",
    "            for i, c in enumerate(ckpts):\n",
    "                tf.Session(config=config).run(barrier)\n",
    "                if 'top1' not in c:\n",
    "                    continue\n",
    "                # print the epoch, top1, top5, loss, mtime over time, going through all checkpoints\n",
    "                print('Validation Dataset Evaluation: | Step: {:5d} | Epoch: {:5.1f} | Top1Acc: {:5.3f} | Top5Acc: {:6.2f} | Loss: {:6.2f} | Time(h): {:10.3f}'.format(\n",
    "                                 c['step'],\n",
    "                                 c['epoch'],\n",
    "                                 c['top1'],\n",
    "                                 c['top5'],\n",
    "                                 c['loss'],\n",
    "                                 c['mtime']))\n",
    "                validation_step_top1_top5_loss.append(\"{},{},{},{}\".format(c[\"step\"], c[\"top1\"], c[\"top5\"], c[\"loss\"]))\n",
    "                final_accuracy_top1 = c[\"top1\"]\n",
    "            print(\"Finished evaluation on validation set\")\n",
    "            validation_results_str = \"\\n\".join(validation_step_top1_top5_loss)\n",
    "            py_hdfs.dump(validation_results_str, VALIDATION_SET_RESULTS_FILE)\n",
    "            return final_accuracy_top1\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Keyboard interrupt\")\n",
    "        return final_accuracy_top1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiments and Hyperparameter Search Using Hops Library\n",
    "\n",
    "For these type of experiments we only run a few epochs to see trends among hyperparameters, not to achieve the best possible accuracy (that we will do in the next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "args_dict = {\"model\" : [\"resnet18\",\"resnet34\"],\"lr\" : [0.001, 0.1], \n",
    "             \"lr_decay_mode\" : [\"poly\", \"steps\"],\n",
    "            \"wdecay\":[0.00001, 0.001], \"mom\":[0.80, 1]} #hyperparameters to tune\n",
    "# evolutionary search through the space of hyperparameters using genetic algorithms\n",
    "logdir, best_param_dict = experiment.evolutionary_search(spark, main, args_dict, generations=20, population=15, direction=\"max\", cleanup_generations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evo_search_accuracy.png](./../images/evo_search_accuracy.png)\n",
    "![evo_search_hist.png](./../images/evo_search_hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Best HyperParameters to HopsFS\n",
    "The next notebook will read the best parameters and use those for training on a larger amount of epochs using a larger amount of GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_str = \"\"\n",
    "for k,v in best_param_dict.items():\n",
    "    hyperparams_str = hyperparams_str + str(k) + \",\" + str(v) + \"\\n\"\n",
    "py_hdfs.dump(hyperparams_str, HYPERPARAMS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run single training experiment with a set of hyperparameters using a single GPU\n",
    "Running 150 epochs (6hours on single GPU) with the following parameters yields Top1Acc: 0.95 | Top5Acc: 0.97 on the training set:\n",
    "\n",
    "- model: resnet18\n",
    "- lr: 0.01\n",
    "- batch_size: 100\n",
    "- mom: 0.90\n",
    "- wdecay : 0.0001\n",
    "- warmup_lr: 0.001 \n",
    "- warmup_epochs: 0\n",
    "- lr_decay_factor: 0.1\n",
    "- lr_decay_steps: '30,60,80' \n",
    "- lr_decay_mode: \"poly\"\n",
    "- loss_scale=1024\n",
    "- num_gpus=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "experiment.launch(spark, main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot accuracy:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/single_gpu_validation_results.txt\",sep=\",\")\n",
    "steps = df[\"step\"].values\n",
    "top1accs = df[\"top1acc\"].values\n",
    "top5accs = df[\"top5acc\"].values\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-4, 4, 150)\n",
    "ax.plot(steps, top1accs, linewidth=2, alpha=0.6, label=\"top1-accuracy\")\n",
    "ax.plot(steps, top5accs, linewidth=2, alpha=0.6, label=\"top5-accuracy\")\n",
    "ax.set_title(\"Validation set accuracy\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n",
    "fig.savefig('validation_set_accuracy.png')\n",
    "plt.show()\n",
    "```\n",
    "![validation_set_accuracy.png](./../images/validation_set_accuracy.png)\n",
    "\n",
    "Plot loss:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/single_gpu_validation_results.txt\",sep=\",\")\n",
    "steps = df[\"step\"].values\n",
    "loss = df[\"loss\"].values\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-4, 4, 150)\n",
    "ax.plot(steps, loss, linewidth=2, alpha=0.6, label=\"training loss\")\n",
    "ax.set_title(\"Training loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "fig.savefig('validation_set_loss.png')\n",
    "plt.show()\n",
    "```\n",
    "![validation_set_loss.png](./../images/validation_set_loss.png)\n",
    "\n",
    "We can see that the model is overfitting as its validation accuracy is not improving despite training loss going down. The top-accuracy on the leaderboard for this dataset is 0.732. To improve the model accuracy we can add more regularization (e.g dropout) to reduce the amount of overfitting. We can also do more data augmentation and we can re-shape the 2% of the images in the training dataset that we have excluded due to being mis-shaped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring The Jobs\n",
    "\n",
    "### Open the Job-UIs panel\n",
    "![tensorboard1.png](./../images/tensorboard1.png)\n",
    "\n",
    "### Open the SparkUI (Spark is used to distribute the hyperparameter tuning)\n",
    "![tensorboard2.png](./../images/tensorboard2.png)\n",
    "\n",
    "### Open the Job in the SparkUI\n",
    "![tensorboard3.png](./../images/tensorboard3.png)\n",
    "\n",
    "### Links to Logs for Each Experiment\n",
    "![tensorboard4.png](./../images/tensorboard4.png)\n",
    "\n",
    "### Example Log for an Experiment\n",
    "![tensorboard5.png](./../images/tensorboard5.png)\n",
    "\n",
    "### Links to Tensorboards for Each Experiment\n",
    "![tensorboard6.png](./../images/tensorboard6.png)\n",
    "\n",
    "### Example Tensorboard for an Experiment\n",
    "![tensorboard7.png](./../images/tensorboard7.png)\n",
    "\n",
    "![tensorboard8.png](./../images/tensorboard8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
