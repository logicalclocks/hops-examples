{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started: Preprocess Mnist Dataset to CSV or TFRecords format\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to run preprocessing for the mnist dataset. After finishing this notebook, you will be ready to learn how to run Distributed Training using TensorFlowOnSpark.\n",
    "\n",
    "### In this notebook we are going to learn how to:\n",
    "- Download the Mnist dataset to your local machine\n",
    "- Upload Mnist dataset to your project in HopsWorks\n",
    "- Learn how to submit a .zip containing the Mnist dataset to be accessible by the notebook\n",
    "- Convert Mnist dataset to .csv or .tfrecords format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download mnist to your machine and create a .zip\n",
    "\n",
    "```bash\n",
    "mkdir ./mnist\n",
    "cd mnist\n",
    "curl -O \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
    "curl -O \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "curl -O \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\"\n",
    "curl -O \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
    "zip -r mnist.zip *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload the zip to project in HopsWorks\n",
    "\n",
    "First navigate to your Datasets view in HopsWorks\n",
    "\n",
    "![datasets.png](../images/datasets.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Then create a new dataset named *mnist* by clicking Create New DataSet and entering that name.\n",
    "\n",
    "![create_dataset.png](../images/create_dataset.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next step is to simply click on your newly created dataset and upload the *mnist.zip* that you created locally.\n",
    "It is now uploaded to your HopsWorks project and identifiable by a path in HDFS.\n",
    "\n",
    "![upload.png](../images/upload.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Path in HDFS where mnist.zip is stored\n",
    "\n",
    "All datasets contained in your project is stored in HDFS. Each project is contained in an HDFS root folder.\n",
    "Your root folder can be found by evaluating the following python code:\n",
    "\n",
    "```python\n",
    "from hops import hdfs\n",
    "project_path = \"/Projects/\" + hdfs.project_name()\n",
    "```\n",
    "\n",
    "\n",
    "### Approach 1. Get the path programmatically\n",
    "\n",
    "Simply append the name of the mnist dataset you created and point to the mnist.zip file contained inside\n",
    "\n",
    "```python\n",
    "from hops import hdfs\n",
    "mnist_zip_path = project_path + \"mnist/mnist.zip\n",
    "```\n",
    "\n",
    "### Approach 2. In the HopsWorks UI\n",
    "\n",
    "Navigate inside the mnist dataset and simply select mnist.zip and copy the path\n",
    "\n",
    "![upload.png](../images/dataset_path.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Restart Jupyter and upload the mnist.zip\n",
    "\n",
    "When the Jupyter notebook server is started, it is possible to supply dependencies that should be accessible by your program, this could be .jars, or archives which should be automatically unzipped like .zip or .tgz, or one or more .py files which may or may not be encased in a .zip or .egg file.\n",
    "\n",
    "To demonstrate this you will need to restart Jupyter and add the mnist.zip as an archive in the Jupyter configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The conversion python code\n",
    "\n",
    "This code defines the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>15</td><td>application_1512039930441_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0:8088/proxy/application_1512039930441_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0:8042/node/containerlogs/container_e01_1512039930441_0016_01_000001/demo_tensorflow_admin000__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from array import array\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "from hops import hdfs\n",
    "\n",
    "def toTFExample(image, label):\n",
    "    \"\"\"Serializes an image/label as a TFExample byte string\"\"\"\n",
    "    example = tf.train.Example(\n",
    "      features = tf.train.Features(\n",
    "        feature = {\n",
    "          'label': tf.train.Feature(int64_list=tf.train.Int64List(value=label.astype(\"int64\"))),\n",
    "          'image': tf.train.Feature(int64_list=tf.train.Int64List(value=image.astype(\"int64\")))\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def fromTFExample(bytestr):\n",
    "    \"\"\"Deserializes a TFExample from a byte string\"\"\"\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(bytestr)\n",
    "    return example\n",
    "\n",
    "def toCSV(vec):\n",
    "    \"\"\"Converts a vector/array into a CSV string\"\"\"\n",
    "    return ','.join([str(i) for i in vec])\n",
    "\n",
    "def fromCSV(s):\n",
    "    \"\"\"Converts a CSV string to a vector/array\"\"\"\n",
    "    return [float(x) for x in s.split(',') if len(s) > 0]\n",
    "\n",
    "def writeMNIST(sc, input_images, input_labels, output, format, num_partitions):\n",
    "    \"\"\"Writes MNIST image/label vectors into parallelized files on HDFS\"\"\"\n",
    "    # load MNIST gzip into memory\n",
    "    with open(input_images, 'rb') as f:\n",
    "        images = numpy.array(mnist.extract_images(f))\n",
    "\n",
    "    with open(input_labels, 'rb') as f:\n",
    "        if format == \"csv2\":\n",
    "            labels = numpy.array(mnist.extract_labels(f, one_hot=False))\n",
    "        else:\n",
    "            labels = numpy.array(mnist.extract_labels(f, one_hot=True))\n",
    "\n",
    "    shape = images.shape\n",
    "    print(\"images.shape: {0}\".format(shape))          # 60000 x 28 x 28\n",
    "    print(\"labels.shape: {0}\".format(labels.shape))   # 60000 x 10\n",
    "\n",
    "    # create RDDs of vectors\n",
    "    imageRDD = sc.parallelize(images.reshape(shape[0], shape[1] * shape[2]), num_partitions)\n",
    "    labelRDD = sc.parallelize(labels, num_partitions)\n",
    "\n",
    "    output_images = output + \"/images\"\n",
    "    output_labels = output + \"/labels\"\n",
    "\n",
    "    # save RDDs as specific format\n",
    "    if format == \"pickle\":\n",
    "        imageRDD.saveAsPickleFile(output_images)\n",
    "        labelRDD.saveAsPickleFile(output_labels)\n",
    "    elif format == \"csv\":\n",
    "        imageRDD.map(toCSV).saveAsTextFile(output_images)\n",
    "        labelRDD.map(toCSV).saveAsTextFile(output_labels)\n",
    "    elif format == \"csv2\":\n",
    "        imageRDD.map(toCSV).zip(labelRDD).map(lambda x: str(x[1]) + \"|\" + x[0]).saveAsTextFile(output)\n",
    "    else: # format == \"tfr\":\n",
    "        tfRDD = imageRDD.zip(labelRDD).map(lambda x: (bytearray(toTFExample(x[0], x[1])), None))\n",
    "        # requires: --jars tensorflow-hadoop-1.0-SNAPSHOT.jar\n",
    "        tfRDD.saveAsNewAPIHadoopFile(output, \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n",
    "                                    keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                                    valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "#  Note: this creates TFRecord files w/o requiring a custom Input/Output format\n",
    "#  else: # format == \"tfr\":\n",
    "#    def writeTFRecords(index, iter):\n",
    "#      output_path = \"{0}/part-{1:05d}\".format(output, index)\n",
    "#      writer = tf.python_io.TFRecordWriter(output_path)\n",
    "#      for example in iter:\n",
    "#        writer.write(example)\n",
    "#      return [output_path]\n",
    "#    tfRDD = imageRDD.zip(labelRDD).map(lambda x: toTFExample(x[0], x[1]))\n",
    "#    tfRDD.mapPartitionsWithIndex(writeTFRecords).collect()\n",
    "\n",
    "def readMNIST(sc, output, format):\n",
    "    \"\"\"Reads/verifies previously created output\"\"\"\n",
    "\n",
    "    output_images = output + \"/images\"\n",
    "    output_labels = output + \"/labels\"\n",
    "    imageRDD = None\n",
    "    labelRDD = None\n",
    "\n",
    "    if format == \"pickle\":\n",
    "        imageRDD = sc.pickleFile(output_images)\n",
    "        labelRDD = sc.pickleFile(output_labels)\n",
    "    elif format == \"csv\":\n",
    "        imageRDD = sc.textFile(output_images).map(fromCSV)\n",
    "        labelRDD = sc.textFile(output_labels).map(fromCSV)\n",
    "    else: # format.startswith(\"tf\"):\n",
    "        # requires: --jars tensorflow-hadoop-1.0-SNAPSHOT.jar if you want to convert to TFRecords\n",
    "        tfRDD = sc.newAPIHadoopFile(output, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n",
    "                              keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                              valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "        imageRDD = tfRDD.map(lambda x: fromTFExample(str(x[0])))\n",
    "\n",
    "    num_images = imageRDD.count()\n",
    "    num_labels = labelRDD.count() if labelRDD is not None else num_images\n",
    "    samples = imageRDD.take(10)\n",
    "    print(\"num_images: \", num_images)\n",
    "    print(\"num_labels: \", num_labels)\n",
    "    print(\"samples: \", samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings arguments and running conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-f\", \"--format\", help=\"output format\", choices=[\"csv\",\"csv2\",\"pickle\",\"tf\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"-n\", \"--num-partitions\", help=\"Number of output partitions\", type=int, default=10)\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"HDFS directory to save examples in parallelized format\", default=\"/Projects/\" + hdfs.project_name() + '/mnist')\n",
    "parser.add_argument(\"-r\", \"--read\", help=\"read previously saved examples\", action=\"store_true\")\n",
    "parser.add_argument(\"-v\", \"--verify\", help=\"verify saved examples after writing\", action=\"store_true\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(\"args:\",args)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "if not args.read:\n",
    "    # Note: these files are inside the mnist.zip file\n",
    "    writeMNIST(sc, \"mnist/train-images-idx3-ubyte.gz\", \"mnist/train-labels-idx1-ubyte.gz\", args.output + \"/train\", args.format, args.num_partitions)\n",
    "    writeMNIST(sc, \"mnist/t10k-images-idx3-ubyte.gz\", \"mnist/t10k-labels-idx1-ubyte.gz\", args.output + \"/test\", args.format, args.num_partitions)\n",
    "\n",
    "if args.read or args.verify:\n",
    "    readMNIST(sc, args.output + \"/train\", args.format)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
